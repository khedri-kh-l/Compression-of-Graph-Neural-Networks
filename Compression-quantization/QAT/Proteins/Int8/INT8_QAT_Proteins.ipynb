{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de7d5d",
   "metadata": {
    "id": "c5de7d5d"
   },
   "outputs": [],
   "source": [
    "\n",
    "################################################################################################\n",
    "# REACH INT8 acc of 76.0%\n",
    "# python main.py --int8 --ste_mom --lr 0.005 --wd 0.0002 --epochs 200\n",
    "################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31aa4c4",
   "metadata": {
    "id": "f31aa4c4"
   },
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "229ae5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor\n",
    "from torch.optim import Adam\n",
    "from torch.nn import Linear, Sequential, ReLU, Identity, BatchNorm1d as BN\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import DataLoader, DenseDataLoader as DenseLoader\n",
    "from torch_geometric.data import Batch\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import statistics as stat\n",
    "from tabulate import tabulate\n",
    "import statistics as stat\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e2cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Quantization\n",
    "from dq.quantization import IntegerQuantizer\n",
    "from dq.linear_quantized import LinearQuantized\n",
    "from dq.baseline_quant import GINConvQuant\n",
    "from dq.multi_quant import evaluate_prob_mask, GINConvMultiQuant\n",
    "from dq.transforms import ProbabilisticHighDegreeMask\n",
    "\n",
    "#loading dataset and training\n",
    "from dataset import get_dataset\n",
    "from train_eval import cross_validation_with_val_set\n",
    "from gin import GIN\n",
    "import utils as utils\n",
    "\n",
    "# output dir and tensorboard writer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Computing Energy and cpu usage \n",
    "import psutil\n",
    "import itertools\n",
    "import tracemalloc\n",
    "import gc\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363476a2",
   "metadata": {
    "id": "363476a2"
   },
   "source": [
    "## Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b0f0830",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b0f0830",
    "outputId": "c50e39c9-7435-4d02-bd1a-8cce2f4103c8"
   },
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model\",type=str,default='GIN')\n",
    "parser.add_argument(\"--epochs\", type=int, default=200)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "parser.add_argument(\"--num_layers\", type=int, default=5)\n",
    "parser.add_argument(\"--hidden\", type=int, default=64)\n",
    "parser.add_argument(\"--lr\", type=float, default=0.01)\n",
    "parser.add_argument(\"--wd\", type=float, default=4e-5)\n",
    "parser.add_argument(\"--noise\", type=float, default=1.0)\n",
    "parser.add_argument(\"--lr_decay_factor\", type=float, default=0.5)\n",
    "parser.add_argument(\"--lr_decay_step_size\", type=int, default=50)\n",
    "\n",
    "parser.add_argument(\"--path\", type=str, default=\"/datasets/\", help=\"where all datasets live\")\n",
    "parser.add_argument(\"--outdir\", type=str, default=\"D:/output/ProteinBINexps/INT8-DQ\")\n",
    "\n",
    "parser.add_argument(\"--DQ\", action=\"store_true\", help=\"enables DegreeQuant\")\n",
    "parser.add_argument(\"--low\", type=float, default=0.0)\n",
    "parser.add_argument(\"--change\", type=float, default=0.1)\n",
    "parser.add_argument(\"--sample_prop\", type=float, default=None)\n",
    "\n",
    "parser.add_argument(\"--result_folder\",type=str,default='result')\n",
    "# Path to checkpoint\n",
    "parser.add_argument(\"--check_folder\",type=str,default='checkpoint')\n",
    "# Path to dataset\n",
    "parser.add_argument(\"--path2dataset\",type=str,default='/')\n",
    "\n",
    "quant_mode = parser.add_mutually_exclusive_group(required=False)\n",
    "quant_mode.add_argument(\"--fp32\", action=\"store_true\", help=\"no quantization\")\n",
    "quant_mode.add_argument(\"--int8\", action=\"store_true\", help=\"INT8 quant\")\n",
    "quant_mode.add_argument(\"--int4\", action=\"store_true\", help=\"INT4 quant\")\n",
    "\n",
    "ste_mode = parser.add_mutually_exclusive_group(required=False)\n",
    "ste_mode.add_argument(\"--ste_abs\", action=\"store_true\", help=\"STE-ABS\")\n",
    "ste_mode.add_argument(\"--ste_mom\", action=\"store_true\", help=\"STE-MOM\")\n",
    "ste_mode.add_argument(\"--ste_per\", action=\"store_true\", help=\"STE-PER\")\n",
    "ste_mode.add_argument(\"--gc_abs\", action=\"store_true\", help=\"GC-ABS\")\n",
    "ste_mode.add_argument(\"--gc_mom\", action=\"store_true\", help=\"GC-MOM\")\n",
    "ste_mode.add_argument(\"--gc_per\", action=\"store_true\", help=\"GC-PER\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args(['--fp32', '--ste_abs'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c2260",
   "metadata": {
    "id": "be5c2260"
   },
   "source": [
    "### Generating the qConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afe08b7",
   "metadata": {},
   "source": [
    "- INT8=True $\\Rightarrow$ args.int8=True\n",
    "- DQ=False $\\Rightarrow$ args.DQ=False\n",
    "- gc-per=True $\\Rightarrow$ args.gc_per=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24bd71f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24bd71f4",
    "outputId": "620002e5-2539-4c15-f86f-425fd5705925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model='GIN', epochs=200, batch_size=128, num_layers=5, hidden=64, lr=0.01, wd=4e-05, noise=1.0, lr_decay_factor=0.5, lr_decay_step_size=50, path='/datasets/', outdir='D:/output/ProteinBINexps/INT8-DQ', DQ=False, low=0.0, change=0.1, sample_prop=None, result_folder='result', check_folder='checkpoint', path2dataset='/', fp32=False, int8=True, int4=False, ste_abs=False, ste_mom=True, ste_per=False, gc_abs=False, gc_mom=False, gc_per=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args.DQ=False\n",
    "args.fp32=False\n",
    "args.int8=True\n",
    "args.int4=False\n",
    "args.gc_per=False\n",
    "args.ste_abs=False\n",
    "args.ste_mom=True\n",
    "\n",
    "\n",
    "if args.fp32:\n",
    "    qypte = \"FP32\"\n",
    "elif args.int8:\n",
    "    qypte = \"INT8\"\n",
    "elif args.int4:\n",
    "    qypte = \"INT4\"\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "ste = False\n",
    "momentum = False\n",
    "percentile = None\n",
    "\n",
    "# ste quant\n",
    "if args.ste_abs:\n",
    "    ste = True\n",
    "elif args.ste_mom:\n",
    "    ste = True\n",
    "    momentum = True\n",
    "elif args.gc_abs:\n",
    "    pass\n",
    "elif args.gc_mom:\n",
    "    momentum = True\n",
    "elif args.ste_per:\n",
    "    ste = True\n",
    "    percentile = 0.01 if args.int4 else 0.001\n",
    "elif args.gc_per:\n",
    "    percentile = 0.01 if args.int4 else 0.001\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "DQ = None\n",
    "if args.DQ:\n",
    "    DQ = {\"prob_mask_low\": args.low, \"prob_mask_change\": args.change}\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a94f7",
   "metadata": {
    "id": "046a94f7"
   },
   "source": [
    "## Loading dataset and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e0c344",
   "metadata": {
    "id": "77e0c344"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NormalizedDegree(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, data):\n",
    "        deg = degree(data.edge_index[0], dtype=torch.float)\n",
    "        deg = (deg - self.mean) / self.std\n",
    "        data.x = deg.view(-1, 1)\n",
    "        return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9cb95ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path, name, sparse=True, cleaned=False, DQ=None):\n",
    "    dataset = TUDataset(path, name, cleaned=cleaned)\n",
    "    dataset.data.edge_attr = None\n",
    "\n",
    "    if dataset.data.x is None:\n",
    "        max_degree = 0\n",
    "        degs = []\n",
    "        for data in dataset:\n",
    "            degs += [degree(data.edge_index[0], dtype=torch.long)]\n",
    "            max_degree = max(max_degree, degs[-1].max().item())\n",
    "\n",
    "        if max_degree < 1000:\n",
    "            dataset.transform = T.OneHotDegree(max_degree)\n",
    "        else:\n",
    "            deg = torch.cat(degs, dim=0).to(torch.float)\n",
    "            mean, std = deg.mean().item(), deg.std().item()\n",
    "            dataset.transform = NormalizedDegree(mean, std)\n",
    "\n",
    "    \n",
    "    \n",
    "    if not sparse:\n",
    "        num_nodes = max_num_nodes = 0\n",
    "        for data in dataset:\n",
    "            num_nodes += data.num_nodes\n",
    "            max_num_nodes = max(data.num_nodes, max_num_nodes)\n",
    "\n",
    "        # Filter out a few really large graphs in order to apply DiffPool.\n",
    "        if name == \"PROTEINS\":\n",
    "            num_nodes = min(int(num_nodes / len(dataset) * 1.5), max_num_nodes)\n",
    "        else:\n",
    "            num_nodes = min(int(num_nodes / len(dataset) * 5), max_num_nodes)\n",
    "\n",
    "        indices = []\n",
    "        for i, data in enumerate(dataset):\n",
    "            if data.num_nodes <= num_nodes:\n",
    "                indices.append(i)\n",
    "        dataset = dataset[torch.tensor(indices)]\n",
    "\n",
    "        if dataset.transform is None:\n",
    "            dataset.transform = T.ToDense(num_nodes)\n",
    "        else:\n",
    "            dataset.transform = T.Compose([dataset.transform, T.ToDense(num_nodes)])\n",
    " \n",
    "    \n",
    "    \n",
    "    if DQ is not None:\n",
    "        print(f\"Generating ProbabilisticHighDegreeMask: {DQ}\")\n",
    "        dq_transform = ProbabilisticHighDegreeMask(\n",
    "            DQ[\"prob_mask_low\"], min(DQ[\"prob_mask_low\"] + DQ[\"prob_mask_change\"], 1.0)\n",
    "        )\n",
    "        if dataset.transform is None:\n",
    "            dataset.transform = dq_transform\n",
    "        else:\n",
    "            dataset.transform = T.Compose([dataset.transform, dq_transform])\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Ensure dataset.transform is not None\n",
    "    if dataset.transform is None:\n",
    "        dataset.transform = T.Compose([])  # Assign an empty transform\n",
    "   \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42726829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_name='PROTEINS'\n",
    "dataset = get_dataset(args.path, dataset_name, sparse=True, DQ=DQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b1dfa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc663bec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ba97505",
   "metadata": {
    "id": "1ba97505"
   },
   "source": [
    "###  Output dir and tensorboard writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43f396e8",
   "metadata": {
    "id": "43f396e8"
   },
   "outputs": [],
   "source": [
    "def append_date_and_time_to_string(string):\n",
    "    now = datetime.utcnow().strftime(\"%m_%d_%H_%M_%S\")\n",
    "\n",
    "    return Path(string) / now\n",
    "\n",
    "\n",
    "def set_outputdir_and_writer(\n",
    "    model_name,\n",
    "    outdir,\n",
    "    num_layers,\n",
    "    hidden,\n",
    "    lr,\n",
    "    quant_mode,\n",
    "    ste,\n",
    "    momentum,\n",
    "    percentile,\n",
    "    is_DQ,\n",
    "    w_decay,\n",
    "    low,\n",
    "    change,\n",
    "):\n",
    "\n",
    "    layers = \"layers_\" + str(num_layers)\n",
    "    hidden = \"hidden_\" + str(hidden)\n",
    "\n",
    "    ste_config = \"STE_\" if ste else \"GC_\"\n",
    "    if momentum:\n",
    "        ste_config += \"MOM\"\n",
    "    elif percentile is not None:\n",
    "        ste_config += \"PER\"\n",
    "    else:\n",
    "        ste_config += \"ABS\"\n",
    "\n",
    "    if is_DQ:\n",
    "        quant_mode += \"_DQ_low\" + str(low) + \"_chng\" + str(change)\n",
    "\n",
    "    dir = (\n",
    "        Path(outdir)\n",
    "        / model_name\n",
    "        / layers\n",
    "        / hidden\n",
    "        / str(quant_mode)\n",
    "        / ste_config\n",
    "        / str(\"lr_\" + str(lr))\n",
    "        / str(\"wd_\" + str(w_decay))\n",
    "    )\n",
    "\n",
    "    dir = append_date_and_time_to_string(dir)\n",
    "\n",
    "    writer = SummaryWriter(dir)\n",
    "    print(f\"Output dir:{dir}\")\n",
    "\n",
    "    return dir, writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa3fef6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa3fef6b",
    "outputId": "14b9a552-987b-4756-a1a5-770511bfe86f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dir:D:\\output\\ProteinBINexps\\INT8-DQ\\GIN\\layers_5\\hidden_64\\INT8\\STE_MOM\\lr_0.01\\wd_4e-05\\04_22_08_39_20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# output dir and tensorboard writer\n",
    "dir, writer = set_outputdir_and_writer(\n",
    "    \"GIN\",\n",
    "    args.outdir,\n",
    "    args.num_layers,\n",
    "    args.hidden,\n",
    "    args.lr,\n",
    "    qypte,\n",
    "    ste,\n",
    "    momentum,\n",
    "    percentile,\n",
    "    args.DQ,\n",
    "    args.wd,\n",
    "    args.low,\n",
    "    args.change,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c74def6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Setting path to save model and record the output\n",
    "\n",
    "path2result = args.result_folder+'/'+'_'+dataset_name\n",
    "path2check = args.check_folder+'/'+args.model+'_'+dataset_name\n",
    "if not os.path.exists(path2result):\n",
    "    os.makedirs(path2result)\n",
    "if not os.path.exists(path2check):\n",
    "    os.makedirs(path2check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2304182",
   "metadata": {
    "id": "c2304182"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fb06876",
   "metadata": {
    "id": "7fb06876"
   },
   "outputs": [],
   "source": [
    "model = GIN(\n",
    "    dataset,\n",
    "    num_layers=args.num_layers,\n",
    "    hidden=args.hidden,\n",
    "    dq=args.DQ,\n",
    "    qypte=qypte,\n",
    "    ste=ste,\n",
    "    momentum=momentum,\n",
    "    percentile=percentile,\n",
    "    sample_prop=args.sample_prop,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e4150",
   "metadata": {},
   "source": [
    "## Helpful Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e50aa906",
   "metadata": {
    "id": "e50aa906"
   },
   "outputs": [],
   "source": [
    "\n",
    "def k_fold(dataset, folds):\n",
    "    skf = StratifiedKFold(folds, shuffle=True, random_state=12345)\n",
    "\n",
    "    test_indices, train_indices = [], []\n",
    "    for _, idx in skf.split(torch.zeros(len(dataset)), dataset.data.y):\n",
    "        test_indices.append(torch.from_numpy(idx))\n",
    "\n",
    "    val_indices = [test_indices[i - 1] for i in range(folds)]\n",
    "\n",
    "    for i in range(folds):\n",
    "        train_mask = torch.ones(len(dataset), dtype=torch.bool)\n",
    "        train_mask[test_indices[i]] = 0\n",
    "        train_mask[val_indices[i]] = 0\n",
    "        train_indices.append(train_mask.nonzero().view(-1))\n",
    "\n",
    "    return train_indices, test_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51e83377",
   "metadata": {
    "id": "51e83377"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loader):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * num_graphs(data)\n",
    "        optimizer.step()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_acc(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(data).max(1)[1]\n",
    "        correct += pred.eq(data.y.view(-1)).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_loss(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "        loss += F.nll_loss(out, data.y.view(-1), reduction=\"sum\").item()\n",
    "    return loss / len(loader.dataset)\n",
    "\n",
    "def num_graphs(data):\n",
    "    if data.batch is not None:\n",
    "        return data.num_graphs\n",
    "    else:\n",
    "        return data.x.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7735ae",
   "metadata": {},
   "source": [
    "### Functions for Mmeasuring criterias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57f1e138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_parameters(model: nn.Module, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the total number of parameters of model\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    num_counted_elements = 0\n",
    "    for param in model.parameters():\n",
    "        if count_nonzero_only:\n",
    "            num_counted_elements += param.count_nonzero()\n",
    "        else:\n",
    "            num_counted_elements += param.numel()\n",
    "    return num_counted_elements\n",
    "\n",
    "# Function to get CPU usage\n",
    "def get_cpu_usage():\n",
    "    return psutil.cpu_percent(interval=1)\n",
    "\n",
    "\n",
    "\n",
    "# Function to approximate power consumption (Assume some average power usage per CPU percentage point)\n",
    "def estimate_power_usage(cpu_usage):\n",
    "    base_power_usage = 10  # Assumed base power usage in watts\n",
    "    power_per_percent = 0.5  # Assumed additional watts per CPU usage percent\n",
    "    return base_power_usage + (power_per_percent * cpu_usage)\n",
    "\n",
    "# The model size based on the number of parameters\n",
    "def calculate_model_size(model: nn.Module, \n",
    "                         qypte: str = 'fp32', \n",
    "                         include_metadata: bool = False,\n",
    "                         model_path: str = None) -> float:\n",
    "    \"\"\"\n",
    "    Calculate model size in KB/MB for different precisions.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        precision: 'fp32' (32-bit float) or 'int4' (4-bit integer)\n",
    "        include_metadata: Whether to include PyTorch metadata in size calculation\n",
    "        model_path: If provided, will check actual file size on disk\n",
    "        \n",
    "    Returns:\n",
    "        Size in KB (if include_metadata=False) or actual file size (if include_metadata=True)\n",
    "    \"\"\"\n",
    "    # Get total number of parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Calculate theoretical size\n",
    "    if qypte == 'FP32':\n",
    "        size_bits = total_params * 32\n",
    "    elif qypte== 'INT4':\n",
    "        size_bits = total_params * 4\n",
    "    elif qypte == 'INT8':\n",
    "        size_bits = total_params * 8    \n",
    "   \n",
    "    \n",
    "    size_bytes = size_bits / 8\n",
    "    size_kb = size_bytes / 1024\n",
    "    \n",
    "    # If checking actual file size\n",
    "    if include_metadata and model_path:\n",
    "        if not os.path.exists(model_path):\n",
    "            # Save model to temporary file if path doesn't exist\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        actual_size_kb = os.path.getsize(model_path) / 1024\n",
    "        return actual_size_kb\n",
    "    \n",
    "    return size_kb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab391bb5",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5b18d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_with_val_set(\n",
    "    dataset,\n",
    "    model,\n",
    "    folds,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    lr_decay_factor,\n",
    "    lr_decay_step_size,\n",
    "    weight_decay,\n",
    "    qypte='FP32',\n",
    "    use_tqdm=True,\n",
    "    writer=None,\n",
    "    logger=None,\n",
    "):\n",
    "\n",
    "        \n",
    "        val_losses, accs, durations = [], [], []\n",
    "        quant_model_accuracy=[]\n",
    "        quant_model_loss=[]\n",
    "        t_quant_model=[]\n",
    "        Num_parm_quant_model=[]\n",
    "        quant_model_size=[]\n",
    "        quant_energy_consumption=[]\n",
    "        quant_cpu_usage=[]\n",
    "        quant_memory_usage=[]\n",
    "        max_acc=0.4\n",
    "        \n",
    "       \n",
    "        # Initialize a dictionary to store all results per iteration\n",
    "        Eva_iter = {\n",
    "            \"val losses per iter\": [],\n",
    "            \"durations per iter\": [],\n",
    "            \"quant model accuracy per iter\": [],\n",
    "            \"time inference of quant model per iter\": [],\n",
    "            \"number parmameters of quant model per iter\": [],  # Store the best accuracy for each fold\n",
    "            \"size of quant model per iter\": [],\n",
    "            \"energy consumption of quant model per iter\": [],\n",
    "            \"cpu usage of quant model per iter\": [],\n",
    "            \"total memory usage of quant model per iter\": [],\n",
    "            \"final_metrics\": {}  # Store final metrics (mean, std, etc.)\n",
    "        }\n",
    "        \n",
    "        for fold, (train_idx, test_idx, val_idx) in enumerate(zip(*k_fold(dataset, folds))):\n",
    "            train_dataset = dataset[train_idx.tolist()]\n",
    "            test_dataset = dataset[test_idx.tolist()]\n",
    "            val_dataset = dataset[val_idx.tolist()]\n",
    "            if \"adj\" in train_dataset[0]:\n",
    "                    train_loader = DenseLoader(train_dataset, batch_size, shuffle=True)\n",
    "                    val_loader = DenseLoader(val_dataset, batch_size, shuffle=False)\n",
    "                    test_loader = DenseLoader(test_dataset, batch_size, shuffle=False)\n",
    "            else:\n",
    "                    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "                    val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "                    test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "            model.to(device).reset_parameters()\n",
    "            optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            t_start = time.perf_counter()\n",
    "\n",
    "            #if use_tqdm:\n",
    "                #t = tqdm(total=epochs, desc=\"Fold #\" + str(fold))\n",
    "            Eva_fold= OrderedDict() #It is a dictionary to arrange output of this fold\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                    train_loss = train(model, optimizer, train_loader)\n",
    "                    val_loss = eval_loss(model, val_loader)\n",
    "                    val_losses.append(val_loss)\n",
    "                    \n",
    "                    accs.append(eval_acc(model, test_loader))\n",
    "                    eval_info = {\n",
    "                        \"fold\": fold,\n",
    "                        \"epoch\": epoch,\n",
    "                        \"train_loss\": train_loss,\n",
    "                        \"val_loss\": val_losses[-1],\n",
    "                        \"test_acc\": accs[-1],\n",
    "                    }\n",
    "                    acc_test=accs[-1]  \n",
    "                \n",
    "                    if logger is not None:\n",
    "                        logger(eval_info)\n",
    "\n",
    "                    if writer is not None:\n",
    "                        writer.add_scalar(f\"Fold{fold}/Train_Loss\", train_loss, epoch)\n",
    "                        writer.add_scalar(f\"Fold{fold}/Val_Loss\", val_loss, epoch)\n",
    "                        writer.add_scalar(\n",
    "                           f\"Fold{fold}/Lr\", optimizer.param_groups[0][\"lr\"], epoch\n",
    "                        )\n",
    "\n",
    "                    if epoch % lr_decay_step_size == 0:\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group[\"lr\"] = lr_decay_factor * param_group[\"lr\"]\n",
    "\n",
    "                    if epoch % 30 == 0:\n",
    "                        print(f\"Eval Epoch: {epoch} |Val_loss:{val_loss:.03f}| Train_Loss: {train_loss:.3f} | Acc_Val: {val_losses[-1]:.3f}|Fold: {fold}\")\n",
    "                   \n",
    "\n",
    "                \n",
    "                    if(acc_test>max_acc):\n",
    "                        path =  path2check+'/'+args.model+'_'+dataset_name+'_'+'quantized.pth.tar'\n",
    "                        #path = dir+'/'+args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'quantized.pth.tar'\n",
    "                        max_acc = acc_test\n",
    "                        torch.save({'state_dict': model.state_dict(), 'best_accu': acc_test}, path)\n",
    "  \n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.synchronize()\n",
    "                    t_end = time.perf_counter()\n",
    "                    durations.append(t_end - t_start)\n",
    "                    \n",
    "            # Start monitoring CPU and memory usage, model size, number of parametes, time inference and  power consumption\n",
    "            \n",
    "            \n",
    "            quant_model_path= path2check+'/'+args.model+'_'+dataset_name+'_'+'quantized.pth.tar'\n",
    "            state = torch.load(quant_model_path)\n",
    "            dict=state['state_dict']\n",
    "            recover_model = lambda: model.load_state_dict(state['state_dict'])\n",
    "            \n",
    "            gc.collect()\n",
    "            time.sleep(5)  # Add a 5-second delay to stabilize the initial state\n",
    "            tracemalloc.start()  # Start tracking memory allocations\n",
    "            snapshot_before = tracemalloc.take_snapshot()#take a snapshot of the current memory state before starting the measurement.\n",
    "\n",
    "            t0 = time.perf_counter()\n",
    "            initial_cpu_usage = get_cpu_usage()\n",
    "            power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "\n",
    "\n",
    "            fold_quant_model_accuracy= eval_acc(model, test_loader)\n",
    "\n",
    "            fold_quant_cpu_usage = get_cpu_usage()\n",
    "            t1 = time.perf_counter()\n",
    "            fold_t_quant_model=t1-t0\n",
    "\n",
    "            snapshot_after = tracemalloc.take_snapshot()\n",
    "            tracemalloc.stop()\n",
    "            top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "            folde_quant_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "            fold_quant_energy_consumption = power_usage * fold_t_quant_model\n",
    "              #fold_quant_model_size = os.path.getsize(main_model_path)\n",
    "            fold_quant_model_size =calculate_model_size(model, qypte )\n",
    "            fold_num_parm_quant_model=get_num_parameters(model, count_nonzero_only=True)\n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5) \n",
    "            #Update Eva dictionary\n",
    "            Eva_fold.update({'quant model accuracy per fold': fold_quant_model_accuracy,\n",
    "                        'time inference of quant model per fold':fold_t_quant_model,\n",
    "                        'number parmameters of quant model per fold': fold_num_parm_quant_model,\n",
    "                        'size of quant model per fold': fold_quant_model_size, \n",
    "                        'energy consumption of quant model per fold':fold_quant_energy_consumption,\n",
    "                        'total memory usage of quant model per fold':folde_quant_total_memory_diff,\n",
    "                        'cpu usage of quant model per fold':fold_quant_cpu_usage\n",
    "                       })\n",
    "            \n",
    "            gc.collect()\n",
    "            time.sleep(5) \n",
    "   \n",
    "\n",
    "            quant_model_accuracy.append(Eva_fold['quant model accuracy per fold'])\n",
    "            t_quant_model.append(Eva_fold['time inference of quant model per fold'])\n",
    "            Num_parm_quant_model.append(int(Eva_fold['number parmameters of quant model per fold']))\n",
    "            quant_model_size.append(int(Eva_fold['size of quant model per fold']))\n",
    "            quant_energy_consumption.append(Eva_fold['energy consumption of quant model per fold'])\n",
    "            quant_cpu_usage.append(Eva_fold['cpu usage of quant model per fold'])\n",
    "            quant_memory_usage.append(Eva_fold['total memory usage of quant model per fold'])\n",
    "\n",
    "           \n",
    "\n",
    "     \n",
    "     \n",
    "        Eva_iter[\"quant model accuracy per iter\"]= stat.mean(quant_model_accuracy)\n",
    "        Eva_iter[\"time inference of quant model per iter\"]= stat.mean(t_quant_model)\n",
    "        Eva_iter[\"number parmameters of quant model per iter\"]=  stat.mean(Num_parm_quant_model)\n",
    "        Eva_iter[\"size of quant model per iter\"]= stat.mean(quant_model_size)\n",
    "        Eva_iter[\"energy consumption of quant model per iter\"]= stat.mean(quant_energy_consumption)\n",
    "        Eva_iter[\"cpu usage of quant model per iter\"]= stat.mean(quant_cpu_usage)\n",
    "        Eva_iter[\"total memory usage of quant model per iter\"]= stat.mean(quant_memory_usage)\n",
    "    \n",
    "    \n",
    "        loss, acc, duration = tensor(val_losses), tensor(accs), tensor(durations)\n",
    "        loss, acc = loss.view(folds, epochs), acc.view(folds, epochs)\n",
    "        loss, argmin = loss.min(dim=1)\n",
    "        acc = acc[torch.arange(folds, dtype=torch.long), argmin]\n",
    "\n",
    "        Eva_iter[\"val losses per iter\"]= loss.mean().item()\n",
    "        Eva_iter[\"durations per iter\"]= duration.mean().item()\n",
    "\n",
    "        \n",
    "      \n",
    "        \n",
    "        return Eva_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54392ac8",
   "metadata": {},
   "source": [
    "### Manual Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b19b675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are all list of criteria for measurements. \n",
    "# We collect all desired datas of each list across iterations. \n",
    "# Then, we compute average and std of each list.\n",
    "\n",
    "\n",
    "\n",
    "#quant model\n",
    "Quant_val_loss=[]\n",
    "Quant_duration=[]\n",
    "Quant_model_accuracy=[]\n",
    "T_quant_model=[]\n",
    "Num_parm_quant_model=[]\n",
    "Quant_model_size=[]\n",
    "Quant_Energy_Consumption=[]\n",
    "Quant_Cpu_Usage=[]\n",
    "Quant_Memory_Usage=[]\n",
    "\n",
    "\n",
    "# Here is the dictionary to record the list of all measurements\n",
    "Eva_measure={'quant validation loss':Quant_val_loss,\n",
    "             'quant duration':Quant_duration,\n",
    "            'quant model accuracy': Quant_model_accuracy,\n",
    "            'time inference of quant model':T_quant_model,\n",
    "            'number parmameters of quant model':Num_parm_quant_model,\n",
    "            'quant model size':Quant_model_size,\n",
    "            'energy consumption of quant model':Quant_Energy_Consumption,\n",
    "            'cpu usage of quant model':Quant_Cpu_Usage,\n",
    "            'memory usage of quant model':Quant_Memory_Usage}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdd5e63c",
   "metadata": {
    "id": "bdd5e63c"
   },
   "outputs": [],
   "source": [
    "iterations=1\n",
    "epochs=10\n",
    "folds=10\n",
    "batch_size=args.batch_size\n",
    "lr=args.lr\n",
    "lr_decay_factor=args.lr_decay_factor\n",
    "lr_decay_step_size=args.lr_decay_step_size\n",
    "weight_decay=args.wd\n",
    "writer=writer\n",
    "logger=None\n",
    "use_tqdm=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89269adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************\n",
      "The iteration is :1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "#### load the quantized  model\n",
    "\n",
    "for i in range(iterations):\n",
    "    print('********************************************')\n",
    "    print(f'The iteration is :{i+1} ')\n",
    "   \n",
    " \n",
    "\n",
    "    \n",
    "    Eva_iter=cross_validation_with_val_set(\n",
    "                                            dataset,\n",
    "                                            model,\n",
    "                                            folds,\n",
    "                                            epochs,\n",
    "                                            batch_size,\n",
    "                                            lr,\n",
    "                                            lr_decay_factor,\n",
    "                                            lr_decay_step_size,\n",
    "                                            weight_decay,\n",
    "                                            qypte,\n",
    "                                            use_tqdm=True,\n",
    "                                            writer=None,\n",
    "                                            logger=None,)\n",
    "\n",
    "\n",
    " \n",
    "    Quant_val_loss.append(Eva_iter[\"val losses per iter\"])\n",
    "    Quant_duration.append(Eva_iter[\"durations per iter\"])\n",
    "    Quant_model_accuracy.append(Eva_iter[\"quant model accuracy per iter\"])\n",
    "    T_quant_model.append(Eva_iter[\"time inference of quant model per iter\"])\n",
    "    Num_parm_quant_model.append(Eva_iter[\"number parmameters of quant model per iter\"])\n",
    "    Quant_model_size.append(Eva_iter[\"size of quant model per iter\"])\n",
    "    Quant_Energy_Consumption.append(Eva_iter[\"energy consumption of quant model per iter\"])\n",
    "    Quant_Cpu_Usage.append( Eva_iter[\"cpu usage of quant model per iter\"])\n",
    "    Quant_Memory_Usage.append(Eva_iter[\"total memory usage of quant model per iter\"])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d72c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a dictionary to save all measurements. Aftre measuring, we can compute mean and std of each item.\n",
    "from collections import OrderedDict \n",
    "Eva_final = OrderedDict()\n",
    "\n",
    "\n",
    "\n",
    "quant_model_val_loss_mean =stat.mean(Quant_val_loss)\n",
    "quant_model_val_loss_std = stat.stdev(Quant_val_loss)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of quant loss validation':float(format(quant_model_val_loss_mean, '.3f'))})\n",
    "Eva_final.update({'Std of quant loss validation':float(format(quant_model_val_loss_std, '.3f'))})    \n",
    "\n",
    "quant_model_duration_mean =stat.mean(Quant_duration)\n",
    "quant_model_duration_std = stat.stdev(Quant_duration)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of quant model duration':float(format(quant_model_duration_mean , '.3f'))})\n",
    "Eva_final.update({'Std of quant model duration':float(format(quant_model_duration_std, '.3f'))})                                         \n",
    "                                     \n",
    "\n",
    "quant_model_accuracy_mean =stat.mean(Quant_model_accuracy)\n",
    "quant_model_accuracy_std = stat.stdev(Quant_model_accuracy)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of quant model accuracy':float(format(quant_model_accuracy_mean, '.3f'))})\n",
    "Eva_final.update({'Std of quant model accuracy':float(format(quant_model_accuracy_std, '.3f'))})\n",
    "                 \n",
    "\n",
    "t_quant_model_mean = stat.mean(T_quant_model)\n",
    "t_quant_model_std =stat.stdev(T_quant_model)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of time inference of quant model':float(format(t_quant_model_mean, '.3f'))})\n",
    "Eva_final.update({'Std of time inference of quant model':float(format(t_quant_model_std, '.3f'))})\n",
    "\n",
    "num_parm_quant_model_mean = stat.mean(Num_parm_quant_model)\n",
    "num_parm_quant_model_std = stat.stdev(Num_parm_quant_model)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of number parmameters of quant model':num_parm_quant_model_mean})\n",
    "Eva_final.update({'Std of number parmameters of quant model':num_parm_quant_model_std})\n",
    "\n",
    "quant_model_size_mean =stat.mean( Quant_model_size)\n",
    "quant_model_size_std = stat.stdev(Quant_model_size)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of quant model size':quant_model_size_mean})\n",
    "Eva_final.update({'Std of quant_model_size':quant_model_size_std })\n",
    "\n",
    "quant_energy_consumption_mean = stat.mean(Quant_Energy_Consumption)\n",
    "quant_energy_consumption_std = stat.stdev(Quant_Energy_Consumption)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of energy consumption of quant model':quant_energy_consumption_mean })\n",
    "Eva_final.update({'Std of energy consumption of quant model':quant_energy_consumption_std})\n",
    "\n",
    "\n",
    "quant_cpu_usage_mean = stat.mean(Quant_Cpu_Usage)\n",
    "quant_cpu_usage_std = stat.stdev(Quant_Cpu_Usage)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of cpu usage of quant model':quant_cpu_usage_mean})\n",
    "Eva_final.update({'Std of cpu usage of quant model':quant_cpu_usage_std})\n",
    "\n",
    "quant_memory_usage_mean = stat.mean(Quant_Memory_Usage)\n",
    "quant_memory_usage_std = stat.stdev(Quant_Memory_Usage)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of memory usage of quant model':quant_memory_usage_mean})\n",
    "Eva_final.update({'Std of memory usage of quant model':quant_memory_usage_std})\n",
    "\n",
    "#################################\n",
    "\n",
    "\n",
    "# Determing Quantization Method \n",
    "if args.DQ == True:\n",
    "    dq='DQ'\n",
    "else:\n",
    "    dq='QAT'\n",
    "print(f\"All measurement about {dq} Quantization process of type:{ qypte} on modes:{args.DQ}  \")   \n",
    "Eva_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17686b00",
   "metadata": {},
   "source": [
    "### Recording the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b966281c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a773ca6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef660ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5984613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96088ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2823fa84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294a0eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aec4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "295b5a26",
   "metadata": {
    "id": "295b5a26"
   },
   "source": [
    "### Quantization Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c64d2d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c64d2d8",
    "outputId": "03c0b50b-0829-4159-fa74-4830a7851779"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "Fold #0:   0%|                                                                                   | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold #0:  50%|████████████████████▌                    | 1/2 [00:07<00:07,  7.98s/it, Train_Loss=0.701, Val_Loss=0.885]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold #0: 100%|█████████████████████████████████████████| 2/2 [00:16<00:00,  8.27s/it, Train_Loss=0.699, Val_Loss=0.694]\n",
      "Fold #0: 100%|█████████████████████████████████████████| 2/2 [00:16<00:00,  8.25s/it, Train_Loss=0.699, Val_Loss=0.694]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold #1:   0%|                                                 | 0/2 [00:08<?, ?it/s, Train_Loss=0.705, Val_Loss=0.690]\u001b[A\n",
      "Fold #1:  50%|████████████████████▌                    | 1/2 [00:08<00:08,  8.52s/it, Train_Loss=0.705, Val_Loss=0.690]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold #1:  50%|████████████████████▌                    | 1/2 [00:17<00:08,  8.52s/it, Train_Loss=0.687, Val_Loss=0.674]\u001b[A\n",
      "Fold #1: 100%|█████████████████████████████████████████| 2/2 [00:17<00:00,  8.93s/it, Train_Loss=0.687, Val_Loss=0.674]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold #2:  50%|████████████████████▌                    | 1/2 [00:08<00:08,  8.90s/it, Train_Loss=0.691, Val_Loss=0.654]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold #2: 100%|█████████████████████████████████████████| 2/2 [00:18<00:00,  9.04s/it, Train_Loss=0.663, Val_Loss=0.711]\n",
      "Fold #2: 100%|█████████████████████████████████████████| 2/2 [00:18<00:00,  9.05s/it, Train_Loss=0.663, Val_Loss=0.711]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold #3:   0%|                                                 | 0/2 [00:09<?, ?it/s, Train_Loss=0.698, Val_Loss=0.682]\u001b[A\n",
      "Fold #3:  50%|████████████████████▌                    | 1/2 [00:09<00:09,  9.77s/it, Train_Loss=0.698, Val_Loss=0.682]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold #3:  50%|████████████████████▌                    | 1/2 [00:18<00:09,  9.77s/it, Train_Loss=0.684, Val_Loss=0.705]\u001b[A\n",
      "Fold #3: 100%|█████████████████████████████████████████| 2/2 [00:18<00:00,  9.15s/it, Train_Loss=0.684, Val_Loss=0.705]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold #4:  50%|████████████████████▌                    | 1/2 [00:09<00:09,  9.19s/it, Train_Loss=0.699, Val_Loss=0.880]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold #4: 100%|█████████████████████████████████████████| 2/2 [00:19<00:00, 10.01s/it, Train_Loss=0.672, Val_Loss=0.840]\n",
      "Fold #4: 100%|█████████████████████████████████████████| 2/2 [00:19<00:00,  9.91s/it, Train_Loss=0.672, Val_Loss=0.840]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold #5:   0%|                                                 | 0/2 [00:23<?, ?it/s, Train_Loss=0.678, Val_Loss=0.660]\u001b[A\n",
      "Fold #5:  50%|████████████████████▌                    | 1/2 [00:23<00:23, 23.25s/it, Train_Loss=0.678, Val_Loss=0.660]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold #5:  50%|████████████████████▌                    | 1/2 [00:39<00:23, 23.25s/it, Train_Loss=0.663, Val_Loss=0.737]\u001b[A\n",
      "Fold #5: 100%|█████████████████████████████████████████| 2/2 [00:39<00:00, 19.74s/it, Train_Loss=0.663, Val_Loss=0.737]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold #6:  50%|████████████████████▌                    | 1/2 [00:09<00:09,  9.17s/it, Train_Loss=0.683, Val_Loss=0.774]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold #6: 100%|█████████████████████████████████████████| 2/2 [00:16<00:00,  8.27s/it, Train_Loss=0.679, Val_Loss=0.787]\n",
      "Fold #6: 100%|█████████████████████████████████████████| 2/2 [00:16<00:00,  8.43s/it, Train_Loss=0.679, Val_Loss=0.787]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold #7:   0%|                                                 | 0/2 [00:07<?, ?it/s, Train_Loss=0.694, Val_Loss=0.728]\u001b[A\n",
      "Fold #7:  50%|████████████████████▌                    | 1/2 [00:07<00:07,  7.98s/it, Train_Loss=0.694, Val_Loss=0.728]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold #7:  50%|████████████████████▌                    | 1/2 [00:14<00:07,  7.98s/it, Train_Loss=0.680, Val_Loss=0.654]\u001b[A\n",
      "Fold #7: 100%|█████████████████████████████████████████| 2/2 [00:14<00:00,  7.37s/it, Train_Loss=0.680, Val_Loss=0.654]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold #8:  50%|████████████████████▌                    | 1/2 [00:06<00:06,  6.70s/it, Train_Loss=0.692, Val_Loss=0.683]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold #8: 100%|█████████████████████████████████████████| 2/2 [00:13<00:00,  6.62s/it, Train_Loss=0.679, Val_Loss=0.698]\n",
      "Fold #8: 100%|█████████████████████████████████████████| 2/2 [00:13<00:00,  6.65s/it, Train_Loss=0.679, Val_Loss=0.698]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold #9:   0%|                                                 | 0/2 [00:08<?, ?it/s, Train_Loss=0.703, Val_Loss=0.693]\u001b[A\n",
      "Fold #9:  50%|████████████████████▌                    | 1/2 [00:08<00:08,  8.02s/it, Train_Loss=0.703, Val_Loss=0.693]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n",
      "mask:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold #9:  50%|████████████████████▌                    | 1/2 [00:16<00:08,  8.02s/it, Train_Loss=0.694, Val_Loss=0.679]\u001b[A\n",
      "Fold #9: 100%|█████████████████████████████████████████| 2/2 [00:16<00:00,  8.28s/it, Train_Loss=0.694, Val_Loss=0.679]\u001b[A"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, test_idx, val_idx) in enumerate(zip(*k_fold(dataset, folds))):\n",
    "    train_dataset = dataset[train_idx.tolist()]\n",
    "    test_dataset = dataset[test_idx.tolist()]\n",
    "    val_dataset = dataset[val_idx.tolist()]\n",
    "    if \"adj\" in train_dataset[0]:\n",
    "            train_loader = DenseLoader(train_dataset, batch_size, shuffle=True)\n",
    "            val_loader = DenseLoader(val_dataset, batch_size, shuffle=False)\n",
    "            test_loader = DenseLoader(test_dataset, batch_size, shuffle=False)\n",
    "    else:\n",
    "            train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "    model.to(device).reset_parameters()\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    t_start = time.perf_counter()\n",
    "\n",
    "    if use_tqdm:\n",
    "        t = tqdm(total=epochs, desc=\"Fold #\" + str(fold))\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "            train_loss = train(model, optimizer, train_loader)\n",
    "            val_loss = eval_loss(model, val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "            accs.append(eval_acc(model, test_loader))\n",
    "            eval_info = {\n",
    "                \"fold\": fold,\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_losses[-1],\n",
    "                \"test_acc\": accs[-1],\n",
    "            }\n",
    "\n",
    "            if logger is not None:\n",
    "                logger(eval_info)\n",
    "\n",
    "            if writer is not None:\n",
    "                writer.add_scalar(f\"Fold{fold}/Train_Loss\", train_loss, epoch)\n",
    "                writer.add_scalar(f\"Fold{fold}/Val_Loss\", val_loss, epoch)\n",
    "                writer.add_scalar(\n",
    "                    f\"Fold{fold}/Lr\", optimizer.param_groups[0][\"lr\"], epoch\n",
    "                )\n",
    "\n",
    "            if epoch % lr_decay_step_size == 0:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group[\"lr\"] = lr_decay_factor * param_group[\"lr\"]\n",
    "\n",
    "            if use_tqdm:\n",
    "                t.set_postfix(\n",
    "                    {\n",
    "                        \"Train_Loss\": \"{:05.3f}\".format(train_loss),\n",
    "                        \"Val_Loss\": \"{:05.3f}\".format(val_loss),\n",
    "                    }\n",
    "                )\n",
    "                t.update(1)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "                t_end = time.perf_counter()\n",
    "                durations.append(t_end - t_start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7adf00",
   "metadata": {
    "id": "1f7adf00"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3a74525",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3a74525",
    "outputId": "c100f059-8120-4d8e-aef4-73a61daaf773"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6453, Test Accuracy: 0.605 ± 0.024, Duration: 30.982\n"
     ]
    }
   ],
   "source": [
    "loss, acc, duration = tensor(val_losses), tensor(accs), tensor(durations)\n",
    "loss, acc = loss.view(folds, epochs), acc.view(folds, epochs)\n",
    "loss, argmin = loss.min(dim=1)\n",
    "acc = acc[torch.arange(folds, dtype=torch.long), argmin]\n",
    "\n",
    "loss_mean = loss.mean().item()\n",
    "acc_mean = acc.mean().item()\n",
    "acc_std = acc.std().item()\n",
    "duration_mean = duration.mean().item()\n",
    "print(\n",
    "    \"Val Loss: {:.4f}, Test Accuracy: {:.3f} ± {:.3f}, Duration: {:.3f}\".format(\n",
    "        loss_mean, acc_mean, acc_std, duration_mean\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0a8635e",
   "metadata": {
    "id": "e0a8635e"
   },
   "outputs": [],
   "source": [
    "if writer is not None:\n",
    "    writer.add_scalar(f\"Final/Test_Acc\", acc_mean, epoch)\n",
    "    writer.add_scalar(f\"Final/Test_Acc_Std\", acc_std, epoch)\n",
    "    writer.add_scalar(f\"Final/Test_Loss\", loss_mean, epoch)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76118f69",
   "metadata": {
    "id": "76118f69"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0e4ce",
   "metadata": {
    "id": "c6b0e4ce"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31849ce0",
   "metadata": {
    "id": "31849ce0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
