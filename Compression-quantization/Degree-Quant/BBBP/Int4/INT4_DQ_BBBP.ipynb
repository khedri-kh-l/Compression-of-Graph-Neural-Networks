{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4033d93",
   "metadata": {
    "id": "a4033d93"
   },
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1b8ff90",
   "metadata": {
    "id": "c1b8ff90"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor\n",
    "from torch.optim import Adam\n",
    "from torch.nn import Linear, Sequential, ReLU, Identity, BatchNorm1d as BN\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import DataLoader, DenseDataLoader as DenseLoader\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import statistics as stat\n",
    "from tabulate import tabulate\n",
    "import statistics as stat\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "017e602d",
   "metadata": {
    "id": "017e602d"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Quantization\n",
    "from dq.quantization import IntegerQuantizer\n",
    "from dq.linear_quantized import LinearQuantized\n",
    "from dq.baseline_quant import GINConvQuant\n",
    "from dq.multi_quant import evaluate_prob_mask, GINConvMultiQuant\n",
    "from dq.transforms import ProbabilisticHighDegreeMask\n",
    "\n",
    "#loading dataset and training\n",
    "from dataset import get_dataset\n",
    "from train_eval import cross_validation_with_val_set\n",
    "from gin import GIN\n",
    "import utils as utils\n",
    "\n",
    "# output dir and tensorboard writer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Computing Energy and cpu usage \n",
    "import psutil\n",
    "import itertools\n",
    "import tracemalloc\n",
    "import gc\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c38cbf8",
   "metadata": {
    "id": "6c38cbf8"
   },
   "source": [
    "## Setting Arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b0f0830",
   "metadata": {
    "id": "5b0f0830",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model\",type=str,default='GIN')\n",
    "parser.add_argument(\"--epochs\", type=int, default=200)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "parser.add_argument(\"--num_layers\", type=int, default=5)\n",
    "parser.add_argument(\"--hidden\", type=int, default=64)\n",
    "parser.add_argument(\"--lr\", type=float, default=0.01)\n",
    "parser.add_argument(\"--wd\", type=float, default=4e-5)\n",
    "parser.add_argument(\"--noise\", type=float, default=1.0)\n",
    "parser.add_argument(\"--lr_decay_factor\", type=float, default=0.5)\n",
    "parser.add_argument(\"--lr_decay_step_size\", type=int, default=50)\n",
    "\n",
    "parser.add_argument(\"--path\", type=str, default=\"/datasets/\", help=\"where all datasets live\")\n",
    "parser.add_argument(\"--outdir\", type=str, default=\"D:/output/BBBPBINexps/INT8-DQ\")\n",
    "\n",
    "parser.add_argument(\"--DQ\", action=\"store_true\", help=\"enables DegreeQuant\")\n",
    "parser.add_argument(\"--low\", type=float, default=0.0)\n",
    "parser.add_argument(\"--change\", type=float, default=0.1)\n",
    "parser.add_argument(\"--sample_prop\", type=float, default=None)\n",
    "\n",
    "parser.add_argument(\"--result_folder\",type=str,default='result')\n",
    "# Path to checkpoint\n",
    "parser.add_argument(\"--check_folder\",type=str,default='checkpoint')\n",
    "# Path to dataset\n",
    "parser.add_argument(\"--path2dataset\",type=str,default='/')\n",
    "\n",
    "quant_mode = parser.add_mutually_exclusive_group(required=False)\n",
    "quant_mode.add_argument(\"--fp32\", action=\"store_true\", help=\"no quantization\")\n",
    "quant_mode.add_argument(\"--int8\", action=\"store_true\", help=\"INT8 quant\")\n",
    "quant_mode.add_argument(\"--int4\", action=\"store_true\", help=\"INT4 quant\")\n",
    "\n",
    "ste_mode = parser.add_mutually_exclusive_group(required=False)\n",
    "ste_mode.add_argument(\"--ste_abs\", action=\"store_true\", help=\"STE-ABS\")\n",
    "ste_mode.add_argument(\"--ste_mom\", action=\"store_true\", help=\"STE-MOM\")\n",
    "ste_mode.add_argument(\"--ste_per\", action=\"store_true\", help=\"STE-PER\")\n",
    "ste_mode.add_argument(\"--gc_abs\", action=\"store_true\", help=\"GC-ABS\")\n",
    "ste_mode.add_argument(\"--gc_mom\", action=\"store_true\", help=\"GC-MOM\")\n",
    "ste_mode.add_argument(\"--gc_per\", action=\"store_true\", help=\"GC-PER\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args(['--fp32', '--ste_abs'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b248ec",
   "metadata": {
    "id": "97b248ec"
   },
   "source": [
    "### Generating the qConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1361abd3",
   "metadata": {},
   "source": [
    "- INT4=True $\\Rightarrow$ args.int4=True\n",
    "- DQ=True $\\Rightarrow$ args.DQ=True\n",
    "- gc-per=True $\\Rightarrow$ args.gc_per=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24bd71f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24bd71f4",
    "outputId": "7c23602b-16af-4ad7-9f29-d1606b2ea527",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model='GIN', epochs=200, batch_size=128, num_layers=5, hidden=64, lr=0.01, wd=4e-05, noise=1.0, lr_decay_factor=0.5, lr_decay_step_size=50, path='/datasets/', outdir='D:/output/BBBPBINexps/INT8-DQ', DQ=True, low=0.0, change=0.1, sample_prop=None, result_folder='result', check_folder='checkpoint', path2dataset='/', fp32=False, int8=False, int4=True, ste_abs=False, ste_mom=False, ste_per=True, gc_abs=False, gc_mom=False, gc_per=False)\n"
     ]
    }
   ],
   "source": [
    "args.DQ=True\n",
    "args.fp32=False\n",
    "args.int4=True\n",
    "args.int8=False\n",
    "\n",
    "\n",
    "args.ste_abs=False\n",
    "args.ste_mom=False\n",
    "args.ste_per=True\n",
    "args.gc_abs=False\n",
    "args.gc_mom=False\n",
    "args.gc_per=False\n",
    "\n",
    "\n",
    "if args.fp32:\n",
    "    qypte = \"FP32\"\n",
    "elif args.int8:\n",
    "    qypte = \"INT8\"\n",
    "elif args.int4:\n",
    "    qypte = \"INT4\"\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "ste = False\n",
    "momentum = False\n",
    "percentile = None\n",
    "\n",
    "# ste quant\n",
    "if args.ste_abs:\n",
    "    ste = True\n",
    "elif args.ste_mom:\n",
    "    ste = True\n",
    "    momentum = True\n",
    "elif args.gc_abs:\n",
    "    pass\n",
    "elif args.gc_mom:\n",
    "    momentum = True\n",
    "elif args.ste_per:\n",
    "    ste = True\n",
    "    percentile = 0.01 if args.int4 else 0.001\n",
    "elif args.gc_per:\n",
    "    percentile = 0.01 if args.int4 else 0.001\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "if args.DQ:\n",
    "    DQ = {\"prob_mask_low\": args.low, \"prob_mask_change\": args.change}\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb8cf11",
   "metadata": {
    "id": "0bb8cf11"
   },
   "source": [
    "## Loading dataset and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e33aef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch.utils.data import random_split, Subset\n",
    "from torch_geometric.data import Data, InMemoryDataset, DataLoader\n",
    "\n",
    "def load_MolecueNet(dataset_dir, dataset_name, task=None):\n",
    "    \"\"\" Attention the multi-task problems not solved yet \"\"\"\n",
    "    molecule_net_dataset_names = {name.lower(): name for name in MoleculeNet.names.keys()}\n",
    "    dataset = MoleculeNet(root=dataset_dir, name=molecule_net_dataset_names[dataset_name.lower()])\n",
    "    dataset.data.x = dataset.data.x.float()\n",
    "    if task is None:\n",
    "        dataset.data.y = dataset.data.y.squeeze().long()\n",
    "    else:\n",
    "        dataset.data.y = dataset.data.y[task].long()\n",
    "    dataset.node_type_dict = None\n",
    "    dataset.node_color = None\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40168d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "class ProbabilisticHighDegreeMask:\n",
    "    def __init__(self, low_quantise_prob, high_quantise_prob, per_graph=True):\n",
    "        self.low_prob = low_quantise_prob\n",
    "        self.high_prob = high_quantise_prob\n",
    "        self.per_graph = per_graph\n",
    "\n",
    "    def _process_graph(self, graph):\n",
    "        if graph.num_nodes == 0 or graph.edge_index.size(1) == 0:  # Check for empty graphs\n",
    "            graph.prob_mask = torch.zeros(graph.num_nodes, dtype=torch.float)\n",
    "            return graph\n",
    "\n",
    "        n = graph.num_nodes\n",
    "        indegree = degree(graph.edge_index[1], n, dtype=torch.long)\n",
    "        counts = torch.bincount(indegree)\n",
    "\n",
    "        step_size = (self.high_prob - self.low_prob) / n\n",
    "        indegree_ps = counts * step_size\n",
    "        indegree_ps = torch.cumsum(indegree_ps, dim=0)\n",
    "        indegree_ps += self.low_prob\n",
    "        graph.prob_mask = indegree_ps[indegree]\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if self.per_graph and isinstance(data, Batch):\n",
    "            graphs = data.to_data_list()\n",
    "            processed = []\n",
    "            for g in graphs:\n",
    "                g = self._process_graph(g)\n",
    "                processed.append(g)\n",
    "            return Batch.from_data_list(processed)\n",
    "        else:\n",
    "            return self._process_graph(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96ed7636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import Compose\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "def get_dataset(path, name, sparse=True, cleaned=False, DQ=None):\n",
    "    # Load the dataset\n",
    "    molecule_net_dataset_names = {name.lower(): name for name in MoleculeNet.names.keys()}\n",
    "    \n",
    "    # Load the dataset with the composed transform\n",
    "    dataset = MoleculeNet(\n",
    "        root=path,\n",
    "        name=molecule_net_dataset_names[name.lower()],\n",
    "        #transform=transform\n",
    "    )\n",
    "\n",
    "   # Remove graphs with zero nodes\n",
    "    filtered_data = []\n",
    "    filtered_labels = []\n",
    "    for data in dataset:\n",
    "\n",
    "        if data.edge_index.numpy().size > 0 and data.num_nodes > 0:  # Keep only graphs with at least one node\n",
    "            filtered_data.append(data)\n",
    "            filtered_labels.append(data.y)\n",
    "    \n",
    "    # Replace the dataset with the filtered data\n",
    "    dataset._data_list = filtered_data\n",
    "    dataset._indices = range(len(filtered_data))  # Update indices\n",
    "    dataset.data.y = torch.stack(filtered_labels)  # Update labels\n",
    "    \n",
    "    print(dataset.num_classes)\n",
    "    if dataset.data.x is None:\n",
    "        max_degree = 0\n",
    "        degs = []\n",
    "        for data in dataset:\n",
    "            degs += [degree(data.edge_index[0], dtype=torch.long)]\n",
    "            max_degree = max(max_degree, degs[-1].max().item())\n",
    "\n",
    "        if max_degree < 1000:\n",
    "            dataset.transform = T.OneHotDegree(max_degree)\n",
    "        else:\n",
    "            deg = torch.cat(degs, dim=0).to(torch.float)\n",
    "            mean, std = deg.mean().item(), deg.std().item()\n",
    "            dataset.transform = NormalizedDegree(mean, std)\n",
    "\n",
    "    if not sparse:\n",
    "        num_nodes = max_num_nodes = 0\n",
    "        for data in dataset:\n",
    "            num_nodes += data.num_nodes\n",
    "            max_num_nodes = max(data.num_nodes, max_num_nodes)\n",
    "\n",
    "        # Filter out a few really large graphs in order to apply DiffPool.\n",
    "        if name == \"BBBP\":\n",
    "            num_nodes = min(int(num_nodes / len(dataset) * 1.5), max_num_nodes)\n",
    "        else:\n",
    "            num_nodes = min(int(num_nodes / len(dataset) * 5), max_num_nodes)\n",
    "\n",
    "        indices = []\n",
    "        for i, data in enumerate(dataset):\n",
    "            if data.num_nodes <= num_nodes:\n",
    "                indices.append(i)\n",
    "        dataset = dataset[torch.tensor(indices)]\n",
    "\n",
    "        if dataset.transform is None:\n",
    "            dataset.transform = T.ToDense(num_nodes)\n",
    "        else:\n",
    "            dataset.transform = T.Compose([dataset.transform, T.ToDense(num_nodes)])\n",
    "    \n",
    " \n",
    "    \n",
    "    # If there are existing transforms, add them\n",
    "    if DQ is not None:\n",
    "        print(f\"Generating ProbabilisticHighDegreeMask: {DQ}\")\n",
    "        dq_transform = ProbabilisticHighDegreeMask(\n",
    "            DQ[\"prob_mask_low\"], min(DQ[\"prob_mask_low\"] + DQ[\"prob_mask_change\"], 1.0)\n",
    "        )\n",
    "\n",
    "    if dataset.transform is None:\n",
    "        dataset.transform = dq_transform\n",
    "    else:\n",
    "        dataset.transform = T.Compose([dataset.transform, dq_transform])\n",
    "\n",
    "    return dataset      \n",
    "    #if dataset.transform is None:\n",
    "        #print(f\"dataset.transform:{dataset.transform}\")\n",
    "  \n",
    "        #dataset.transform = T.Compose([dataset.transform, dq_transform])\n",
    "    \n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce4f5122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Generating ProbabilisticHighDegreeMask: {'prob_mask_low': 0.0, 'prob_mask_change': 0.1}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "dataset_name='BBBP'\n",
    "dataset = get_dataset(args.path, dataset_name, sparse=True, DQ=DQ)\n",
    "print(dataset.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08148c9",
   "metadata": {
    "id": "f08148c9"
   },
   "source": [
    "###  Output dir and tensorboard writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43f396e8",
   "metadata": {
    "id": "43f396e8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def append_date_and_time_to_string(string):\n",
    "    now = datetime.utcnow().strftime(\"%m_%d_%H_%M_%S\")\n",
    "\n",
    "    return Path(string) / now\n",
    "\n",
    "\n",
    "def set_outputdir_and_writer(\n",
    "    model_name,\n",
    "    outdir,\n",
    "    num_layers,\n",
    "    hidden,\n",
    "    lr,\n",
    "    quant_mode,\n",
    "    ste,\n",
    "    momentum,\n",
    "    percentile,\n",
    "    is_DQ,\n",
    "    w_decay,\n",
    "    low,\n",
    "    change,\n",
    "):\n",
    "\n",
    "    layers = \"layers_\" + str(num_layers)\n",
    "    hidden = \"hidden_\" + str(hidden)\n",
    "\n",
    "    ste_config = \"STE_\" if ste else \"GC_\"\n",
    "    if momentum:\n",
    "        ste_config += \"MOM\"\n",
    "    elif percentile is not None:\n",
    "        ste_config += \"PER\"\n",
    "    else:\n",
    "        ste_config += \"ABS\"\n",
    "\n",
    "    if is_DQ:\n",
    "        quant_mode += \"_DQ_low\" + str(low) + \"_chng\" + str(change)\n",
    "\n",
    "    dir = (\n",
    "        Path(outdir)\n",
    "        / model_name)\n",
    "\n",
    "    dir = append_date_and_time_to_string(dir)\n",
    "\n",
    "    writer = SummaryWriter(dir)\n",
    "    print(f\"Output dir:{dir}\")\n",
    "\n",
    "    return dir, writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80486b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "#model = args.model\n",
    "#dataset_name = args.dataset_name\n",
    "#num_layers = args.num_layers\n",
    "#hidden_units=args.hidden_units\n",
    "#bit=args.bit\n",
    "#max_epoch = args.max_epoch\n",
    "#resume = args.resume\n",
    "path2result = args.result_folder+'/'+'_'+dataset_name\n",
    "path2check = args.check_folder+'/'+args.model+'_'+dataset_name\n",
    "if not os.path.exists(path2result):\n",
    "    os.makedirs(path2result)\n",
    "if not os.path.exists(path2check):\n",
    "    os.makedirs(path2check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43389e20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43389e20",
    "outputId": "a9767e16-e3d5-477c-8d01-3fd66118c5d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dir:D:\\output\\BBBPBINexps\\INT8-DQ\\GIN\\layers_5\\hidden_64\\INT4_DQ_low0.0_chng0.1\\STE_PER\\lr_0.01\\wd_4e-05\\06_22_11_47_38\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# output dir and tensorboard writer\n",
    "dir, writer = utils.set_outputdir_and_writer(\n",
    "    \"GIN\",\n",
    "    args.outdir,\n",
    "    args.num_layers,\n",
    "    args.hidden,\n",
    "    args.lr,\n",
    "    qypte,\n",
    "    ste,\n",
    "    momentum,\n",
    "    percentile,\n",
    "    args.DQ,\n",
    "    args.wd,\n",
    "    args.low,\n",
    "    args.change,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807bacfe",
   "metadata": {},
   "source": [
    "## qGIN Model with Quatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac185aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_quantizer(qypte, ste, momentum, percentile, signed, sample_prop):\n",
    "    if qypte == \"FP32\":\n",
    "        return Identity\n",
    "    else:\n",
    "        return lambda: IntegerQuantizer(\n",
    "            4 if qypte == \"INT4\" else 8,\n",
    "            signed=signed,\n",
    "            use_ste=ste,\n",
    "            use_momentum=momentum,\n",
    "            percentile=percentile,\n",
    "            sample=sample_prop,\n",
    "        )\n",
    "\n",
    "\n",
    "def make_quantizers(qypte, dq, sign_input, ste, momentum, percentile, sample_prop):\n",
    "    if dq:\n",
    "        # GIN doesn't apply DQ to the LinearQuantize layers so we keep the \n",
    "        # default inputs, weights, features keys.\n",
    "        # See NOTE in the multi_quant.py file\n",
    "        layer_quantizers = {\n",
    "            \"inputs\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, sign_input, sample_prop\n",
    "            ),\n",
    "            \"weights\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "            \"features\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "        }\n",
    "        mp_quantizers = {\n",
    "            \"message_low\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "            \"message_high\": create_quantizer(\n",
    "                \"FP32\", ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "            \"update_low\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "            \"update_high\": create_quantizer(\n",
    "                \"FP32\", ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "            \"aggregate_low\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "            \"aggregate_high\": create_quantizer(\n",
    "                \"FP32\", ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "        }\n",
    "    else:\n",
    "        layer_quantizers = {\n",
    "            \"inputs\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, sign_input, sample_prop\n",
    "            ),\n",
    "            \"weights\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "            \"features\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "        }\n",
    "        mp_quantizers = {\n",
    "            \"message\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "            \"update_q\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "            \"aggregate\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "        }\n",
    "    return layer_quantizers, mp_quantizers\n",
    "\n",
    "\n",
    "class ResettableSequential(Sequential):\n",
    "    def reset_parameters(self):\n",
    "        for child in self.children():\n",
    "            if hasattr(child, \"reset_parameters\"):\n",
    "                child.reset_parameters()\n",
    "\n",
    "\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        num_layers,\n",
    "        hidden,\n",
    "        dq,\n",
    "        qypte,\n",
    "        ste,\n",
    "        momentum,\n",
    "        percentile,\n",
    "        sample_prop,\n",
    "    ):\n",
    "        super(GIN, self).__init__()\n",
    "\n",
    "        self.is_dq = dq\n",
    "        gin_layer = GINConvMultiQuant if dq else GINConvQuant \n",
    "\n",
    "        lq, mq = make_quantizers(\n",
    "            qypte,\n",
    "            dq,\n",
    "            False,\n",
    "            ste=ste,\n",
    "            momentum=momentum,\n",
    "            percentile=percentile,\n",
    "            sample_prop=sample_prop,\n",
    "        )\n",
    "        lq_signed, _ = make_quantizers(\n",
    "            qypte,\n",
    "            dq,\n",
    "            True,\n",
    "            ste=ste,\n",
    "            momentum=momentum,\n",
    "            percentile=percentile,\n",
    "            sample_prop=sample_prop,\n",
    "        )\n",
    "\n",
    "        # NOTE: see comment in multi_quant.py on the use of \n",
    "        # \"mask-aware\" MLPs.\n",
    "        self.conv1 = gin_layer(\n",
    "            ResettableSequential(\n",
    "                Linear(dataset.num_features, hidden),\n",
    "                ReLU(),\n",
    "                LinearQuantized(hidden, hidden, layer_quantizers=lq),\n",
    "                ReLU(),\n",
    "                BN(hidden),\n",
    "            ),\n",
    "            train_eps=True,\n",
    "            mp_quantizers=mq,\n",
    "        )\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(\n",
    "                gin_layer(\n",
    "                    ResettableSequential(\n",
    "                        LinearQuantized(hidden, hidden, layer_quantizers=lq_signed),\n",
    "                        ReLU(),\n",
    "                        LinearQuantized(hidden, hidden, layer_quantizers=lq),\n",
    "                        ReLU(),\n",
    "                        BN(hidden),\n",
    "                    ),\n",
    "                    train_eps=True,\n",
    "                    mp_quantizers=mq,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.lin1 = LinearQuantized(hidden, hidden, layer_quantizers=lq_signed)\n",
    "        self.lin2 = LinearQuantized(hidden, dataset.num_classes, layer_quantizers=lq)\n",
    "        \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        # NOTE: It is possible to use the same mask consistently or generate a \n",
    "        # new mask per layer. For other experiments we used a per-layer mask\n",
    "        # We did not observe major differences but we expect the impact will\n",
    "        # be layer and dataset dependent. Extensive experiments assessing the\n",
    "        # difference were not run, however, due to the high cost.\n",
    "         \n",
    "        #if hasattr(data, \"prob_mask\") and data.prob_mask is not None:\n",
    "            #mask = evaluate_prob_mask(data)\n",
    "        #else:\n",
    "            #mask = None\n",
    "            \n",
    "            \n",
    "                \n",
    "         \n",
    "        mask = evaluate_prob_mask(data)\n",
    "        #mask = mask.float()  # Convert to FloatTensor if needed\n",
    "        #print(mask)\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = x.float()  # Convert to FloatTensor\n",
    "\n",
    "        x = self.conv1(x, edge_index, mask)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, mask)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        # NOTE: the linear layers from here do not contribute significantly to run-time\n",
    "        # Therefore you probably don't want to quantize these as it will likely have \n",
    "        # an impact on performance.\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # NOTE: This is a quantized final layer. You probably don't want to be\n",
    "        # this aggressive in practice.\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    \n",
    "model = GIN(\n",
    "    dataset,\n",
    "    num_layers=args.num_layers,\n",
    "    hidden=args.hidden,\n",
    "    dq=args.DQ,\n",
    "    qypte=qypte,\n",
    "    ste=ste,\n",
    "    momentum=momentum,\n",
    "    percentile=None,\n",
    "    sample_prop=args.sample_prop,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae700f2",
   "metadata": {},
   "source": [
    "# Helpful Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b80eead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(dataset, folds):\n",
    "    skf = StratifiedKFold(folds, shuffle=True, random_state=12345)\n",
    "      # Aggregate labels across tasks (e.g., sum or mean)\n",
    "    y = dataset.data.y.sum(dim=1)  # Sum across tasks\n",
    "    # OR select a single task\n",
    "    y = dataset.data.y[:, 0]  # Use the first task for stratification\n",
    "    #print(dataset.data.y.shape)\n",
    "\n",
    "    test_indices, train_indices = [], []\n",
    "    for _, idx in skf.split(torch.zeros(len(dataset)), y):\n",
    "        test_indices.append(torch.from_numpy(idx))\n",
    "\n",
    "    val_indices = [test_indices[i - 1] for i in range(folds)]\n",
    "\n",
    "    for i in range(folds):\n",
    "        train_mask = torch.ones(len(dataset), dtype=torch.bool)\n",
    "        train_mask[test_indices[i]] = 0\n",
    "        train_mask[val_indices[i]] = 0\n",
    "        train_indices.append(train_mask.nonzero().view(-1))\n",
    "\n",
    "    return train_indices, test_indices, val_indices\n",
    "\n",
    "def num_graphs(data):\n",
    "    if data.batch is not None:\n",
    "        return data.num_graphs\n",
    "    else:\n",
    "        return data.x.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b6cf0e",
   "metadata": {},
   "source": [
    "### Functions for Mmeasuring criterias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8689ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_parameters(model: nn.Module, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the total number of parameters of model\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    num_counted_elements = 0\n",
    "    for param in model.parameters():\n",
    "        if count_nonzero_only:\n",
    "            num_counted_elements += param.count_nonzero()\n",
    "        else:\n",
    "            num_counted_elements += param.numel()\n",
    "    return num_counted_elements\n",
    "\n",
    "# Function to get CPU usage\n",
    "def get_cpu_usage():\n",
    "    return psutil.cpu_percent(interval=1)\n",
    "\n",
    "\n",
    "\n",
    "# Function to approximate power consumption (Assume some average power usage per CPU percentage point)\n",
    "def estimate_power_usage(cpu_usage):\n",
    "    base_power_usage = 10  # Assumed base power usage in watts\n",
    "    power_per_percent = 0.5  # Assumed additional watts per CPU usage percent\n",
    "    return base_power_usage + (power_per_percent * cpu_usage)\n",
    "\n",
    "# The model size based on the number of parameters\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def calculate_model_size(model: nn.Module, \n",
    "                         qypte: str = 'fp32', \n",
    "                         include_metadata: bool = False,\n",
    "                         model_path: str = None) -> float:\n",
    "    \"\"\"\n",
    "    Calculate model size in KB/MB for different precisions.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        precision: 'fp32' (32-bit float) or 'int4' (4-bit integer)\n",
    "        include_metadata: Whether to include PyTorch metadata in size calculation\n",
    "        model_path: If provided, will check actual file size on disk\n",
    "        \n",
    "    Returns:\n",
    "        Size in KB (if include_metadata=False) or actual file size (if include_metadata=True)\n",
    "    \"\"\"\n",
    "    # Get total number of parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Calculate theoretical size\n",
    "    if qypte == 'FP32':\n",
    "        size_bits = total_params * 32\n",
    "    elif qypte== 'INT4':\n",
    "        size_bits = total_params * 4\n",
    "    elif qypte == 'INT8':\n",
    "        size_bits = total_params * 8    \n",
    "   \n",
    "    \n",
    "    size_bytes = size_bits / 8\n",
    "    size_kb = size_bytes / 1024\n",
    "    \n",
    "    # If checking actual file size\n",
    "    if include_metadata and model_path:\n",
    "        if not os.path.exists(model_path):\n",
    "            # Save model to temporary file if path doesn't exist\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        actual_size_kb = os.path.getsize(model_path) / 1024\n",
    "        return actual_size_kb\n",
    "    \n",
    "    return size_kb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f93b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## True\n",
    "def train(model, optimizer, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        target = data.y.view(-1).long()  # Ensure target is LongTensor\n",
    "\n",
    "        # Check if the last batch is smaller\n",
    "        if out.size(0) != target.size(0):\n",
    "            target = target[:out.size(0)]  # Truncate target to match output size\n",
    "\n",
    "        loss = F.nll_loss(out, target)  # Use the converted target\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * num_graphs(data)\n",
    "        optimizer.step()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "def eval_loss(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            # Convert data.y to LongTensor\n",
    "            target = data.y.view(-1).long()  # Ensure target is LongTensor\n",
    "            loss += F.nll_loss(out, target, reduction=\"sum\").item()\n",
    "    return loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_acc(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(data).max(1)[1]\n",
    "            # Convert data.y to LongTensor\n",
    "            target = data.y.view(-1).long()  # Ensure target is LongTensor\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a9d21c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df93c117",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e55ff8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_validation_with_val_set(\n",
    "    dataset,\n",
    "    model,\n",
    "    folds,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    lr_decay_factor,\n",
    "    lr_decay_step_size,\n",
    "    weight_decay,\n",
    "    qypte='FP32',\n",
    "    use_tqdm=True,\n",
    "    writer=None,\n",
    "    logger=None,\n",
    "):\n",
    "\n",
    "        \n",
    "        val_losses, accs, durations = [], [], []\n",
    "        quant_model_accuracy=[]\n",
    "        quant_model_loss=[]\n",
    "        t_quant_model=[]\n",
    "        Num_parm_quant_model=[]\n",
    "        quant_model_size=[]\n",
    "        quant_energy_consumption=[]\n",
    "        quant_cpu_usage=[]\n",
    "        quant_memory_usage=[]\n",
    "        max_acc=0.4\n",
    "        \n",
    "       \n",
    "        # Initialize a dictionary to store all results per iteration\n",
    "        Eva_iter = {\n",
    "            \"val losses per iter\": [],\n",
    "            \"durations per iter\": [],\n",
    "            \"quant model accuracy per iter\": [],\n",
    "            \"time inference of quant model per iter\": [],\n",
    "            \"number parmameters of quant model per iter\": [],  # Store the best accuracy for each fold\n",
    "            \"size of quant model per iter\": [],\n",
    "            \"energy consumption of quant model per iter\": [],\n",
    "            \"cpu usage of quant model per iter\": [],\n",
    "            \"total memory usage of quant model per iter\": [],\n",
    "            \"final_metrics\": {}  # Store final metrics (mean, std, etc.)\n",
    "        }\n",
    "        \n",
    "        for fold, (train_idx, test_idx, val_idx) in enumerate(zip(*k_fold(dataset, folds))):\n",
    "            train_dataset = dataset[train_idx.tolist()]\n",
    "            test_dataset = dataset[test_idx.tolist()]\n",
    "            val_dataset = dataset[val_idx.tolist()]\n",
    "            if \"adj\" in train_dataset[0]:\n",
    "                    train_loader = DenseLoader(train_dataset, batch_size, shuffle=True)\n",
    "                    val_loader = DenseLoader(val_dataset, batch_size, shuffle=False)\n",
    "                    test_loader = DenseLoader(test_dataset, batch_size, shuffle=False)\n",
    "            else:\n",
    "                    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "                    val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "                    test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "            model.to(device).reset_parameters()\n",
    "            optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            t_start = time.perf_counter()\n",
    "\n",
    "            #if use_tqdm:\n",
    "                #t = tqdm(total=epochs, desc=\"Fold #\" + str(fold))\n",
    "            Eva_fold= OrderedDict() #It is a dictionary to arrange output of this fold\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                    train_loss = train(model, optimizer, train_loader)\n",
    "                    val_loss = eval_loss(model, val_loader)\n",
    "                    val_losses.append(val_loss)\n",
    "                    \n",
    "                    accs.append(eval_acc(model, test_loader))\n",
    "                    eval_info = {\n",
    "                        \"fold\": fold,\n",
    "                        \"epoch\": epoch,\n",
    "                        \"train_loss\": train_loss,\n",
    "                        \"val_loss\": val_losses[-1],\n",
    "                        \"test_acc\": accs[-1],\n",
    "                    }\n",
    "                    acc_test=accs[-1]  \n",
    "                \n",
    "                    if logger is not None:\n",
    "                        logger(eval_info)\n",
    "\n",
    "                    if writer is not None:\n",
    "                        writer.add_scalar(f\"Fold{fold}/Train_Loss\", train_loss, epoch)\n",
    "                        writer.add_scalar(f\"Fold{fold}/Val_Loss\", val_loss, epoch)\n",
    "                        writer.add_scalar(\n",
    "                           f\"Fold{fold}/Lr\", optimizer.param_groups[0][\"lr\"], epoch\n",
    "                        )\n",
    "\n",
    "                    if epoch % lr_decay_step_size == 0:\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group[\"lr\"] = lr_decay_factor * param_group[\"lr\"]\n",
    "\n",
    "                    if epoch % 30 == 0:\n",
    "                        print(f\"Eval Epoch: {epoch} |Val_loss:{val_loss:.03f}| Train_Loss: {train_loss:.3f} | Acc_Val: {val_losses[-1]:.3f}|Fold: {fold}\")\n",
    "                   \n",
    "\n",
    "                \n",
    "                    if(acc_test>max_acc):\n",
    "                        path =  path2check+'/'+args.model+'_'+dataset_name+'_'+'quantized.pth.tar'\n",
    "                        #path = dir+'/'+args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'quantized.pth.tar'\n",
    "                        max_acc = acc_test\n",
    "                        torch.save({'state_dict': model.state_dict(), 'best_accu': acc_test}, path)\n",
    "  \n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.synchronize()\n",
    "                    t_end = time.perf_counter()\n",
    "                    durations.append(t_end - t_start)\n",
    "                    \n",
    "            # Start monitoring CPU and memory usage, model size, number of parametes, time inference and  power consumption\n",
    "            \n",
    "            \n",
    "            quant_model_path= path2check+'/'+args.model+'_'+dataset_name+'_'+'quantized.pth.tar'\n",
    "            #state = torch.load(quant_model_path)\n",
    "            #dict=state['state_dict']\n",
    "            #recover_model = lambda: model.load_state_dict(state['state_dict'])\n",
    "            \n",
    "            gc.collect()\n",
    "            time.sleep(5)  # Add a 5-second delay to stabilize the initial state\n",
    "            tracemalloc.start()  # Start tracking memory allocations\n",
    "            snapshot_before = tracemalloc.take_snapshot()#take a snapshot of the current memory state before starting the measurement.\n",
    "\n",
    "            t0 = time.perf_counter()\n",
    "            initial_cpu_usage = get_cpu_usage()\n",
    "            power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "\n",
    "\n",
    "            fold_quant_model_accuracy= eval_acc(model, test_loader)\n",
    "\n",
    "            fold_quant_cpu_usage = get_cpu_usage()\n",
    "            t1 = time.perf_counter()\n",
    "            fold_t_quant_model=t1-t0\n",
    "\n",
    "            snapshot_after = tracemalloc.take_snapshot()\n",
    "            tracemalloc.stop()\n",
    "            top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "            folde_quant_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "            fold_quant_energy_consumption = power_usage * fold_t_quant_model\n",
    "            #fold_quant_model_size = os.path.getsize(main_model_path)\n",
    "            fold_quant_model_size =calculate_model_size(model, qypte )\n",
    "            fold_num_parm_quant_model=get_num_parameters(model, count_nonzero_only=True)\n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5) \n",
    "            #Update Eva dictionary\n",
    "            Eva_fold.update({'quant model accuracy per fold': fold_quant_model_accuracy,\n",
    "                        'time inference of quant model per fold':fold_t_quant_model,\n",
    "                        'number parmameters of quant model per fold': fold_num_parm_quant_model,\n",
    "                        'size of quant model per fold': fold_quant_model_size, \n",
    "                        'energy consumption of quant model per fold':fold_quant_energy_consumption,\n",
    "                        'total memory usage of quant model per fold':folde_quant_total_memory_diff,\n",
    "                        'cpu usage of quant model per fold':fold_quant_cpu_usage\n",
    "                       })\n",
    "            \n",
    "            gc.collect()\n",
    "            time.sleep(5) \n",
    "   \n",
    "\n",
    "            quant_model_accuracy.append(Eva_fold['quant model accuracy per fold'])\n",
    "            t_quant_model.append(Eva_fold['time inference of quant model per fold'])\n",
    "            Num_parm_quant_model.append(int(Eva_fold['number parmameters of quant model per fold']))\n",
    "            quant_model_size.append(int(Eva_fold['size of quant model per fold']))\n",
    "            quant_energy_consumption.append(Eva_fold['energy consumption of quant model per fold'])\n",
    "            quant_cpu_usage.append(Eva_fold['cpu usage of quant model per fold'])\n",
    "            quant_memory_usage.append(Eva_fold['total memory usage of quant model per fold'])\n",
    "\n",
    "           \n",
    "\n",
    "     \n",
    "     \n",
    "        Eva_iter[\"quant model accuracy per iter\"]= stat.mean(quant_model_accuracy)\n",
    "        Eva_iter[\"time inference of quant model per iter\"]= stat.mean(t_quant_model)\n",
    "        Eva_iter[\"number parmameters of quant model per iter\"]=  stat.mean(Num_parm_quant_model)\n",
    "        Eva_iter[\"size of quant model per iter\"]= stat.mean(quant_model_size)\n",
    "        Eva_iter[\"energy consumption of quant model per iter\"]= stat.mean(quant_energy_consumption)\n",
    "        Eva_iter[\"cpu usage of quant model per iter\"]= stat.mean(quant_cpu_usage)\n",
    "        Eva_iter[\"total memory usage of quant model per iter\"]= stat.mean(quant_memory_usage)\n",
    "    \n",
    "    \n",
    "        loss, acc, duration = tensor(val_losses), tensor(accs), tensor(durations)\n",
    "        loss, acc = loss.view(folds, epochs), acc.view(folds, epochs)\n",
    "        loss, argmin = loss.min(dim=1)\n",
    "        acc = acc[torch.arange(folds, dtype=torch.long), argmin]\n",
    "\n",
    "        Eva_iter[\"val losses per iter\"]= loss.mean().item()\n",
    "        Eva_iter[\"durations per iter\"]= duration.mean().item()\n",
    "\n",
    "        \n",
    "      \n",
    "        \n",
    "        return Eva_iter, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598bed17",
   "metadata": {},
   "source": [
    "### Manual Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dca9294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are all list of criteria for measurements. \n",
    "# We collect all desired datas of each list across iterations. \n",
    "# Then, we compute average and std of each list.\n",
    "\n",
    "\n",
    "\n",
    "#quant model\n",
    "Quant_val_loss=[]\n",
    "Quant_duration=[]\n",
    "Quant_model_accuracy=[]\n",
    "T_quant_model=[]\n",
    "Num_parm_quant_model=[]\n",
    "Quant_model_size=[]\n",
    "Quant_Energy_Consumption=[]\n",
    "Quant_Cpu_Usage=[]\n",
    "Quant_Memory_Usage=[]\n",
    "\n",
    "\n",
    "# Here is the dictionary to record the list of all measurements\n",
    "Eva_measure={'quant validation loss':Quant_val_loss,\n",
    "             'quant duration':Quant_duration,\n",
    "            'quant model accuracy': Quant_model_accuracy,\n",
    "            'time inference of quant model':T_quant_model,\n",
    "            'number parmameters of quant model':Num_parm_quant_model,\n",
    "            'quant model size':Quant_model_size,\n",
    "            'energy consumption of quant model':Quant_Energy_Consumption,\n",
    "            'cpu usage of quant model':Quant_Cpu_Usage,\n",
    "            'memory usage of quant model':Quant_Memory_Usage}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c46537af",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations=1\n",
    "epochs=2\n",
    "folds=10\n",
    "batch_size=args.batch_size\n",
    "lr=args.lr\n",
    "lr_decay_factor=args.lr_decay_factor\n",
    "lr_decay_step_size=args.lr_decay_step_size\n",
    "weight_decay=args.wd\n",
    "writer=writer\n",
    "logger=None\n",
    "use_tqdm=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "606be124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************\n",
      "The iteration is :1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "#### load the quantized  model\n",
    "\n",
    "for i in range(iterations):\n",
    "    print('********************************************')\n",
    "    print(f'The iteration is :{i+1} ') \n",
    "\n",
    "  \n",
    "    Eva_iter, model=cross_validation_with_val_set(\n",
    "                                            dataset,\n",
    "                                            model,\n",
    "                                            folds,\n",
    "                                            epochs,\n",
    "                                            batch_size,\n",
    "                                            lr,\n",
    "                                            lr_decay_factor,\n",
    "                                            lr_decay_step_size,\n",
    "                                            weight_decay,\n",
    "                                            qypte,\n",
    "                                            use_tqdm=True,\n",
    "                                            writer=None,\n",
    "                                            logger=None,)\n",
    "\n",
    "\n",
    " \n",
    "    Quant_val_loss.append(Eva_iter[\"val losses per iter\"])\n",
    "    Quant_duration.append(Eva_iter[\"durations per iter\"])\n",
    "    Quant_model_accuracy.append(Eva_iter[\"quant model accuracy per iter\"])\n",
    "    T_quant_model.append(Eva_iter[\"time inference of quant model per iter\"])\n",
    "    Num_parm_quant_model.append(Eva_iter[\"number parmameters of quant model per iter\"])\n",
    "    Quant_model_size.append(Eva_iter[\"size of quant model per iter\"])\n",
    "    Quant_Energy_Consumption.append(Eva_iter[\"energy consumption of quant model per iter\"])\n",
    "    Quant_Cpu_Usage.append( Eva_iter[\"cpu usage of quant model per iter\"])\n",
    "    Quant_Memory_Usage.append(Eva_iter[\"total memory usage of quant model per iter\"])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b1df2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val losses per iter': 0.5456306338310242,\n",
       " 'durations per iter': 61.01333999633789,\n",
       " 'quant model accuracy per iter': 0.7128658359895682,\n",
       " 'time inference of quant model per iter': 2.8828856599982826,\n",
       " 'number parmameters of quant model per iter': 43015,\n",
       " 'size of quant model per iter': 168,\n",
       " 'energy consumption of quant model per iter': 118.64256476242474,\n",
       " 'cpu usage of quant model per iter': 61.76,\n",
       " 'total memory usage of quant model per iter': 48523.7,\n",
       " 'final_metrics': {}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eva_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2211bd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All measurement about DQ Quantization process of type:INT4 on modes:True  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Ave of quant loss validation', 0.547),\n",
       "             ('Std of quant loss validation', 0.005),\n",
       "             ('Ave of quant model duration', 147.96),\n",
       "             ('Std of quant model duration', 40.294),\n",
       "             ('Ave of quant model accuracy', 0.712),\n",
       "             ('Std of quant model accuracy', 0.0),\n",
       "             ('Ave of time inference of quant model', 3.218),\n",
       "             ('Std of time inference of quant model', 0.436),\n",
       "             ('Ave of number parmameters of quant model', 43014.95),\n",
       "             ('Std of number parmameters of quant model', 0.07071067811762578),\n",
       "             ('Ave of quant model size', 226429),\n",
       "             ('Std of quant_model_size', 0.0),\n",
       "             ('Ave of energy consumption of quant model', 166.41279508215425),\n",
       "             ('Std of energy consumption of quant model', 63.67225641116794),\n",
       "             ('Ave of cpu usage of quant model', 77.35),\n",
       "             ('Std of cpu usage of quant model', 31.918800102760756),\n",
       "             ('Ave of memory usage of quant model', 51769.100000000006),\n",
       "             ('Std of memory usage of quant model', 1099.2682020326058)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a dictionary to save all measurements. Aftre measuring, we can compute mean and std of each item.\n",
    "from collections import OrderedDict \n",
    "Eva_final = OrderedDict()\n",
    "\n",
    "\n",
    "\n",
    "quant_model_val_loss_mean =stat.mean(Quant_val_loss)\n",
    "quant_model_val_loss_std = stat.stdev(Quant_val_loss)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of quant loss validation':float(format(quant_model_val_loss_mean, '.3f'))})\n",
    "Eva_final.update({'Std of quant loss validation':float(format(quant_model_val_loss_std, '.3f'))})    \n",
    "\n",
    "quant_model_duration_mean =stat.mean(Quant_duration)\n",
    "quant_model_duration_std = stat.stdev(Quant_duration)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of quant model duration':float(format(quant_model_duration_mean , '.3f'))})\n",
    "Eva_final.update({'Std of quant model duration':float(format(quant_model_duration_std, '.3f'))})                                         \n",
    "                                     \n",
    "\n",
    "quant_model_accuracy_mean =stat.mean(Quant_model_accuracy)\n",
    "quant_model_accuracy_std = stat.stdev(Quant_model_accuracy)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of quant model accuracy':float(format(quant_model_accuracy_mean, '.3f'))})\n",
    "Eva_final.update({'Std of quant model accuracy':float(format(quant_model_accuracy_std, '.3f'))})\n",
    "                 \n",
    "\n",
    "t_quant_model_mean = stat.mean(T_quant_model)\n",
    "t_quant_model_std =stat.stdev(T_quant_model)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of time inference of quant model':float(format(t_quant_model_mean, '.3f'))})\n",
    "Eva_final.update({'Std of time inference of quant model':float(format(t_quant_model_std, '.3f'))})\n",
    "\n",
    "num_parm_quant_model_mean = stat.mean(Num_parm_quant_model)\n",
    "num_parm_quant_model_std = stat.stdev(Num_parm_quant_model)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of number parmameters of quant model':num_parm_quant_model_mean})\n",
    "Eva_final.update({'Std of number parmameters of quant model':num_parm_quant_model_std})\n",
    "\n",
    "quant_model_size_mean =stat.mean( Quant_model_size)\n",
    "quant_model_size_std = stat.stdev(Quant_model_size)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of quant model size':quant_model_size_mean})\n",
    "Eva_final.update({'Std of quant_model_size':quant_model_size_std })\n",
    "\n",
    "quant_energy_consumption_mean = stat.mean(Quant_Energy_Consumption)\n",
    "quant_energy_consumption_std = stat.stdev(Quant_Energy_Consumption)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of energy consumption of quant model':quant_energy_consumption_mean })\n",
    "Eva_final.update({'Std of energy consumption of quant model':quant_energy_consumption_std})\n",
    "\n",
    "\n",
    "quant_cpu_usage_mean = stat.mean(Quant_Cpu_Usage)\n",
    "quant_cpu_usage_std = stat.stdev(Quant_Cpu_Usage)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of cpu usage of quant model':quant_cpu_usage_mean})\n",
    "Eva_final.update({'Std of cpu usage of quant model':quant_cpu_usage_std})\n",
    "\n",
    "quant_memory_usage_mean = stat.mean(Quant_Memory_Usage)\n",
    "quant_memory_usage_std = stat.stdev(Quant_Memory_Usage)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of memory usage of quant model':quant_memory_usage_mean})\n",
    "Eva_final.update({'Std of memory usage of quant model':quant_memory_usage_std})\n",
    "\n",
    "#################################\n",
    "\n",
    "\n",
    "# Determing Quantization Method \n",
    "if args.DQ == True:\n",
    "    dq='DQ'\n",
    "else:\n",
    "    dq='QAT'\n",
    "print(f\"All measurement about {dq} Quantization process of type:{ qypte} on modes:{args.DQ}  \")   \n",
    "Eva_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd992e0a",
   "metadata": {},
   "source": [
    "### Recording the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8db61bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining mode\n",
    "if args.ste_abs:\n",
    "    mode = 'ste_abs'\n",
    "elif args.ste_mom:\n",
    "    mode = 'ste_mom'   \n",
    "elif args.gc_abs:\n",
    "    mode = 'gc_abs'\n",
    "elif args.gc_mom:\n",
    "    mode = 'gc_mom'\n",
    "elif args.ste_per:\n",
    "    mode = 'ste_per'\n",
    "elif args.gc_per:\n",
    "    mode = 'gc_per'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0221ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qypte='INT4'\n",
    "dataset_name='BBBP'\n",
    "\n",
    "if args.DQ == True:\n",
    "    dq='DQ'\n",
    "else:\n",
    "    dq='QAT'\n",
    "\n",
    "\n",
    "file_name = path2result+'/'+'Method_type_'+ qypte +'_and_Quantization_is_'+dq+'_On_'+dataset_name+'_with_Mode_'+mode+'.txt'\n",
    "\n",
    "with open(file_name, 'w') as f:\n",
    "    for key, value in vars(args).items():\n",
    "        f.write('%s:%s\\n'%(key, value))\n",
    "\n",
    "    for key, value in Eva_final.items():\n",
    "        f.write('%s:%s\\n'%(key, value))\n",
    "\n",
    "    for key, value in Eva_measure.items():\n",
    "        f.write('%s:%s\\n' % (key, ','.join(map(str, value))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c87e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7af468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf8aa4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_bit(dataset, state_dict, all_positive=True):\n",
    "    # Step 1: Collect layer-wise bit parameters\n",
    "    layer_bits = {}\n",
    "  \n",
    "    for key, param in state_dict.items():\n",
    "        if'quant' in key  and 'fea' in key:\n",
    "            layer_name = key.split('.quant_fea')[0]\n",
    "            layer_bits[layer_name] = param.abs().round() - 1\n",
    "\n",
    "    # Step 2: Per-graph analysis\n",
    "    for i, data in enumerate(dataset):\n",
    "        #print(f\"\\n===== Analyzing Graph {i+1}/{len(dataset)} =====\")\n",
    "        edge_index = data.edge_index\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, data.x.size(0)).cpu()\n",
    "        \n",
    "        # Step 3: Per-layer analysis within current graph\n",
    "        for layer_name, bits in layer_bits.items():\n",
    "            # Skip if bits tensor doesn't match current graph size\n",
    "            if bits.size(0) != deg.size(0):\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nLayer {layer_name}:\")\n",
    "            print(f\"Avg bits: {bits.mean().item():.2f}\")\n",
    "            \n",
    "            # Bit-degree correlation\n",
    "            for bit_val in range(0, 9):\n",
    "                mask = (bits == bit_val)\n",
    "                if mask.sum() > 0:\n",
    "                    avg_deg = deg[mask].mean().item()\n",
    "                    print(f\"  {bit_val}-bit nodes: {mask.sum().item()} nodes, Avg Degree={avg_deg:.1f}\")\n",
    "    \n",
    "    # Step 4: Weight quantization analysis\n",
    "    weight_bits = []\n",
    "    for key, param in state_dict.items():\n",
    "        if'quant' in key:\n",
    "            print(param)\n",
    "            bits = param.abs().round() - 1\n",
    "            print(bits)\n",
    "            weight_bits.append(bits.mean().item())\n",
    "    \n",
    "    print(\"\\n===== Weight Quantization Summary =====\")\n",
    "    if weight_bits:\n",
    "        print(f\"Avg weight bits: {sum(weight_bits)/len(weight_bits):.2f}\")\n",
    "    else:\n",
    "        print(\"No weight quantization parameters found\")\n",
    "    \n",
    "    print(\"Analysis complete\")\n",
    "    return sum(weight_bits)/len(weight_bits)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "892e071b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer conv1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.max_val:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer conv1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.0.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.1.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.2.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.0.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer convs.3.nn.2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin1.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.min_val:\n",
      "Avg bits: nan\n",
      "\n",
      "Layer lin2.layer_quant.features.max_val:\n",
      "Avg bits: nan\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "tensor([])\n",
      "\n",
      "===== Weight Quantization Summary =====\n",
      "Avg weight bits: nan\n",
      "Analysis complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save model by Danny bit =8\n",
    "model.to(device).reset_parameters()\n",
    "quant_model_path= path2check+'/'+args.model+'_'+dataset_name+'_'+'quantized.pth.tar'\n",
    "\n",
    "state = torch.load(quant_model_path,map_location=torch.device('cpu'))\n",
    "dict=state['state_dict']\n",
    "analysis_bit(dataset, model.state_dict(), all_positive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d926c248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.nn.2.layer_quant.features.min_val\n",
      "The average bits of current layer: tensor(21.)\n",
      "21bit:1 \n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "21bit_deg_mean:0.0\n",
      "\n",
      "\n",
      "conv1.nn.2.layer_quant.features.max_val\n",
      "The average bits of current layer: tensor(32.)\n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "\n",
      "\n",
      "convs.0.nn.0.layer_quant.features.min_val\n",
      "The average bits of current layer: tensor(16.)\n",
      "16bit:1 \n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "16bit_deg_mean:0.0\n",
      "\n",
      "\n",
      "convs.0.nn.0.layer_quant.features.max_val\n",
      "The average bits of current layer: tensor(17.)\n",
      "17bit:1 \n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "17bit_deg_mean:0.0\n",
      "\n",
      "\n",
      "convs.0.nn.2.layer_quant.features.min_val\n",
      "The average bits of current layer: tensor(11.)\n",
      "11bit:1 \n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "11bit_deg_mean:0.0\n",
      "\n",
      "\n",
      "convs.0.nn.2.layer_quant.features.max_val\n",
      "The average bits of current layer: tensor(13.)\n",
      "13bit:1 \n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "13bit_deg_mean:0.0\n",
      "\n",
      "\n",
      "convs.1.nn.0.layer_quant.features.min_val\n",
      "The average bits of current layer: tensor(66.)\n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "\n",
      "\n",
      "convs.1.nn.0.layer_quant.features.max_val\n",
      "The average bits of current layer: tensor(100.)\n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "\n",
      "\n",
      "convs.1.nn.2.layer_quant.features.min_val\n",
      "The average bits of current layer: tensor(58.)\n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "\n",
      "\n",
      "convs.1.nn.2.layer_quant.features.max_val\n",
      "The average bits of current layer: tensor(77.)\n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "\n",
      "\n",
      "convs.2.nn.0.layer_quant.features.min_val\n",
      "The average bits of current layer: tensor(77.)\n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "\n",
      "\n",
      "convs.2.nn.0.layer_quant.features.max_val\n",
      "The average bits of current layer: tensor(86.)\n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "\n",
      "\n",
      "convs.2.nn.2.layer_quant.features.min_val\n",
      "The average bits of current layer: tensor(79.)\n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "\n",
      "\n",
      "convs.2.nn.2.layer_quant.features.max_val\n",
      "The average bits of current layer: tensor(68.)\n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "\n",
      "\n",
      "convs.3.nn.0.layer_quant.features.min_val\n",
      "The average bits of current layer: tensor(69.)\n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "\n",
      "\n",
      "convs.3.nn.0.layer_quant.features.max_val\n",
      "The average bits of current layer: tensor(81.)\n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "\n",
      "\n",
      "convs.3.nn.2.layer_quant.features.min_val\n",
      "The average bits of current layer: tensor(60.)\n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "\n",
      "\n",
      "convs.3.nn.2.layer_quant.features.max_val\n",
      "The average bits of current layer: tensor(57.)\n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "\n",
      "\n",
      "lin1.layer_quant.features.min_val\n",
      "The average bits of current layer: tensor(9.)\n",
      "9bit:1 \n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "9bit_deg_mean:0.0\n",
      "\n",
      "\n",
      "lin1.layer_quant.features.max_val\n",
      "The average bits of current layer: tensor(15.)\n",
      "15bit:1 \n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "15bit_deg_mean:0.0\n",
      "\n",
      "\n",
      "lin2.layer_quant.features.min_val\n",
      "The average bits of current layer: tensor(11.)\n",
      "11bit:1 \n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "11bit_deg_mean:0.0\n",
      "\n",
      "\n",
      "lin2.layer_quant.features.max_val\n",
      "The average bits of current layer: tensor(11.)\n",
      "11bit:1 \n",
      "\n",
      "\n",
      "The average degree of the nodes using corresponding bitwidth:\n",
      "11bit_deg_mean:0.0\n",
      "\n",
      "\n",
      "Analysis complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "edge_index = dataset.edge_index\n",
    "row, col = edge_index\n",
    "deg = degree(col, dataset.x.size(0)).cpu()\n",
    "weight_bits=[]\n",
    "for key, param in dict.items():\n",
    "    \n",
    "        if'quant' in key and 'fea' in key:\n",
    "            print(key)\n",
    "            bit = param.abs().round() - 1\n",
    "            #print(bits)\n",
    "            #if bit.mean().item()>0:\n",
    "            weight_bits.append(bit.mean().item())\n",
    "            print('The average bits of current layer:',bit.mean())\n",
    "            for i in range(32): \n",
    "                if (bit==i).sum().item()!=0:\n",
    "                     print(f\"{i}bit:{format((bit==i).sum())} \")\n",
    "            \n",
    "            print('\\n')\n",
    "            print('The average degree of the nodes using corresponding bitwidth:')\n",
    "            for i in range(32):\n",
    "                index_bit = torch.where(bit==i)[0]\n",
    "                #print(index_bit )\n",
    "         \n",
    "                if  index_bit.nelement() !=0:\n",
    "                    print(f\"{i}bit_deg_mean:{deg[index_bit].mean()}\")\n",
    "            \n",
    "            print('\\n')\n",
    "\n",
    "print(\"Analysis complete\")\n",
    "   \n",
    "      \n",
    "sum(weight_bits)/len(weight_bits)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e117b504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.eps\n",
      "conv1.nn.0.weight\n",
      "conv1.nn.0.bias\n",
      "conv1.nn.2.weight\n",
      "conv1.nn.2.bias\n",
      "conv1.nn.2.layer_quant.inputs.min_val\n",
      "conv1.nn.2.layer_quant.inputs.max_val\n",
      "conv1.nn.2.layer_quant.features.min_val\n",
      "conv1.nn.2.layer_quant.features.max_val\n",
      "conv1.nn.2.layer_quant.weights.min_val\n",
      "conv1.nn.2.layer_quant.weights.max_val\n",
      "conv1.nn.4.weight\n",
      "conv1.nn.4.bias\n",
      "conv1.nn.4.running_mean\n",
      "conv1.nn.4.running_var\n",
      "conv1.nn.4.num_batches_tracked\n",
      "conv1.mp_quantizers.message_low.min_val\n",
      "conv1.mp_quantizers.message_low.max_val\n",
      "conv1.mp_quantizers.update_low.min_val\n",
      "conv1.mp_quantizers.update_low.max_val\n",
      "conv1.mp_quantizers.aggregate_low.min_val\n",
      "conv1.mp_quantizers.aggregate_low.max_val\n",
      "convs.0.eps\n",
      "convs.0.nn.0.weight\n",
      "convs.0.nn.0.bias\n",
      "convs.0.nn.0.layer_quant.inputs.min_val\n",
      "convs.0.nn.0.layer_quant.inputs.max_val\n",
      "convs.0.nn.0.layer_quant.features.min_val\n",
      "convs.0.nn.0.layer_quant.features.max_val\n",
      "convs.0.nn.0.layer_quant.weights.min_val\n",
      "convs.0.nn.0.layer_quant.weights.max_val\n",
      "convs.0.nn.2.weight\n",
      "convs.0.nn.2.bias\n",
      "convs.0.nn.2.layer_quant.inputs.min_val\n",
      "convs.0.nn.2.layer_quant.inputs.max_val\n",
      "convs.0.nn.2.layer_quant.features.min_val\n",
      "convs.0.nn.2.layer_quant.features.max_val\n",
      "convs.0.nn.2.layer_quant.weights.min_val\n",
      "convs.0.nn.2.layer_quant.weights.max_val\n",
      "convs.0.nn.4.weight\n",
      "convs.0.nn.4.bias\n",
      "convs.0.nn.4.running_mean\n",
      "convs.0.nn.4.running_var\n",
      "convs.0.nn.4.num_batches_tracked\n",
      "convs.0.mp_quantizers.message_low.min_val\n",
      "convs.0.mp_quantizers.message_low.max_val\n",
      "convs.0.mp_quantizers.update_low.min_val\n",
      "convs.0.mp_quantizers.update_low.max_val\n",
      "convs.0.mp_quantizers.aggregate_low.min_val\n",
      "convs.0.mp_quantizers.aggregate_low.max_val\n",
      "convs.1.eps\n",
      "convs.1.nn.0.weight\n",
      "convs.1.nn.0.bias\n",
      "convs.1.nn.0.layer_quant.inputs.min_val\n",
      "convs.1.nn.0.layer_quant.inputs.max_val\n",
      "convs.1.nn.0.layer_quant.features.min_val\n",
      "convs.1.nn.0.layer_quant.features.max_val\n",
      "convs.1.nn.0.layer_quant.weights.min_val\n",
      "convs.1.nn.0.layer_quant.weights.max_val\n",
      "convs.1.nn.2.weight\n",
      "convs.1.nn.2.bias\n",
      "convs.1.nn.2.layer_quant.inputs.min_val\n",
      "convs.1.nn.2.layer_quant.inputs.max_val\n",
      "convs.1.nn.2.layer_quant.features.min_val\n",
      "convs.1.nn.2.layer_quant.features.max_val\n",
      "convs.1.nn.2.layer_quant.weights.min_val\n",
      "convs.1.nn.2.layer_quant.weights.max_val\n",
      "convs.1.nn.4.weight\n",
      "convs.1.nn.4.bias\n",
      "convs.1.nn.4.running_mean\n",
      "convs.1.nn.4.running_var\n",
      "convs.1.nn.4.num_batches_tracked\n",
      "convs.1.mp_quantizers.message_low.min_val\n",
      "convs.1.mp_quantizers.message_low.max_val\n",
      "convs.1.mp_quantizers.update_low.min_val\n",
      "convs.1.mp_quantizers.update_low.max_val\n",
      "convs.1.mp_quantizers.aggregate_low.min_val\n",
      "convs.1.mp_quantizers.aggregate_low.max_val\n",
      "convs.2.eps\n",
      "convs.2.nn.0.weight\n",
      "convs.2.nn.0.bias\n",
      "convs.2.nn.0.layer_quant.inputs.min_val\n",
      "convs.2.nn.0.layer_quant.inputs.max_val\n",
      "convs.2.nn.0.layer_quant.features.min_val\n",
      "convs.2.nn.0.layer_quant.features.max_val\n",
      "convs.2.nn.0.layer_quant.weights.min_val\n",
      "convs.2.nn.0.layer_quant.weights.max_val\n",
      "convs.2.nn.2.weight\n",
      "convs.2.nn.2.bias\n",
      "convs.2.nn.2.layer_quant.inputs.min_val\n",
      "convs.2.nn.2.layer_quant.inputs.max_val\n",
      "convs.2.nn.2.layer_quant.features.min_val\n",
      "convs.2.nn.2.layer_quant.features.max_val\n",
      "convs.2.nn.2.layer_quant.weights.min_val\n",
      "convs.2.nn.2.layer_quant.weights.max_val\n",
      "convs.2.nn.4.weight\n",
      "convs.2.nn.4.bias\n",
      "convs.2.nn.4.running_mean\n",
      "convs.2.nn.4.running_var\n",
      "convs.2.nn.4.num_batches_tracked\n",
      "convs.2.mp_quantizers.message_low.min_val\n",
      "convs.2.mp_quantizers.message_low.max_val\n",
      "convs.2.mp_quantizers.update_low.min_val\n",
      "convs.2.mp_quantizers.update_low.max_val\n",
      "convs.2.mp_quantizers.aggregate_low.min_val\n",
      "convs.2.mp_quantizers.aggregate_low.max_val\n",
      "convs.3.eps\n",
      "convs.3.nn.0.weight\n",
      "convs.3.nn.0.bias\n",
      "convs.3.nn.0.layer_quant.inputs.min_val\n",
      "convs.3.nn.0.layer_quant.inputs.max_val\n",
      "convs.3.nn.0.layer_quant.features.min_val\n",
      "convs.3.nn.0.layer_quant.features.max_val\n",
      "convs.3.nn.0.layer_quant.weights.min_val\n",
      "convs.3.nn.0.layer_quant.weights.max_val\n",
      "convs.3.nn.2.weight\n",
      "convs.3.nn.2.bias\n",
      "convs.3.nn.2.layer_quant.inputs.min_val\n",
      "convs.3.nn.2.layer_quant.inputs.max_val\n",
      "convs.3.nn.2.layer_quant.features.min_val\n",
      "convs.3.nn.2.layer_quant.features.max_val\n",
      "convs.3.nn.2.layer_quant.weights.min_val\n",
      "convs.3.nn.2.layer_quant.weights.max_val\n",
      "convs.3.nn.4.weight\n",
      "convs.3.nn.4.bias\n",
      "convs.3.nn.4.running_mean\n",
      "convs.3.nn.4.running_var\n",
      "convs.3.nn.4.num_batches_tracked\n",
      "convs.3.mp_quantizers.message_low.min_val\n",
      "convs.3.mp_quantizers.message_low.max_val\n",
      "convs.3.mp_quantizers.update_low.min_val\n",
      "convs.3.mp_quantizers.update_low.max_val\n",
      "convs.3.mp_quantizers.aggregate_low.min_val\n",
      "convs.3.mp_quantizers.aggregate_low.max_val\n",
      "lin1.weight\n",
      "lin1.bias\n",
      "lin1.layer_quant.inputs.min_val\n",
      "lin1.layer_quant.inputs.max_val\n",
      "lin1.layer_quant.features.min_val\n",
      "lin1.layer_quant.features.max_val\n",
      "lin1.layer_quant.weights.min_val\n",
      "lin1.layer_quant.weights.max_val\n",
      "lin2.weight\n",
      "lin2.bias\n",
      "lin2.layer_quant.inputs.min_val\n",
      "lin2.layer_quant.inputs.max_val\n",
      "lin2.layer_quant.features.min_val\n",
      "lin2.layer_quant.features.max_val\n",
      "lin2.layer_quant.weights.min_val\n",
      "lin2.layer_quant.weights.max_val\n"
     ]
    }
   ],
   "source": [
    "for key, param in dict.items():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ed9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbbc839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97404765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantized_size(model):\n",
    "    total_bits = 0\n",
    "    \n",
    "    # Helper to get bitwidth from IntegerQuantizer\n",
    "    def get_bitwidth(quantizer):\n",
    "        if hasattr(quantizer, 'bitwidth'):\n",
    "            return quantizer.bitwidth\n",
    "        # Default to 8 bits if bitwidth isn't explicitly set\n",
    "        return 8  \n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        # Count quantized linear layers\n",
    "        if isinstance(module, LinearQuantized):\n",
    "            # Weights\n",
    "            if hasattr(module, 'layer_quant') and 'weights' in module.layer_quant:\n",
    "                quantizer = module.layer_quant['weights']\n",
    "                bitwidth =4 #get_bitwidth(quantizer)\n",
    "                total_bits += module.weight.numel() * bitwidth\n",
    "            \n",
    "            # Biases (typically remain FP32 - 32 bits)\n",
    "            if module.bias is not None:\n",
    "                total_bits += module.bias.numel() * 32\n",
    "        \n",
    "        # Count BatchNorm parameters (typically FP32)\n",
    "        elif isinstance(module, nn.BatchNorm1d):\n",
    "            total_bits += module.weight.numel() * 32  # gamma\n",
    "            total_bits += module.bias.numel() * 32    # beta\n",
    "            # Running stats (not parameters but still stored)\n",
    "            total_bits += module.running_mean.numel() * 32\n",
    "            total_bits += module.running_var.numel() * 32\n",
    "    \n",
    "    # Convert to bytes\n",
    "    return total_bits / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d2b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674624f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68172c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c04a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0e4ce",
   "metadata": {
    "id": "c6b0e4ce",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
