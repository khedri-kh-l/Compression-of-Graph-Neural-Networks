{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de7d5d",
   "metadata": {
    "id": "c5de7d5d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "################################################################################################\n",
    "# REACH INT8-DQ acc of 91.8%\n",
    "# python main.py --int8 --gc_per --lr 0.005 --DQ --low 0.0 --change 0.1 --wd 0.0002 --epochs 200\n",
    "################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4033d93",
   "metadata": {
    "id": "a4033d93"
   },
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b8ff90",
   "metadata": {
    "id": "c1b8ff90"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor\n",
    "from torch.optim import Adam\n",
    "from torch.nn import Linear, Sequential, ReLU, Identity, BatchNorm1d as BN\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import DataLoader, DenseDataLoader as DenseLoader\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import statistics as stat\n",
    "from tabulate import tabulate\n",
    "import statistics as stat\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55321c43",
   "metadata": {
    "id": "55321c43",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Quantization\n",
    "from dq.quantization import IntegerQuantizer\n",
    "from dq.linear_quantized import LinearQuantized\n",
    "from dq.baseline_quant import GINConvQuant\n",
    "from dq.multi_quant import evaluate_prob_mask, GINConvMultiQuant\n",
    "from dq.transforms import ProbabilisticHighDegreeMask\n",
    "\n",
    "#loading dataset and training\n",
    "from dataset import get_dataset\n",
    "from train_eval import cross_validation_with_val_set\n",
    "from gin import GIN\n",
    "import utils as utils\n",
    "\n",
    "# output dir and tensorboard writer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Computing Energy and cpu usage \n",
    "import psutil\n",
    "import itertools\n",
    "import tracemalloc\n",
    "import gc\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c38cbf8",
   "metadata": {
    "id": "6c38cbf8"
   },
   "source": [
    "### Setting Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b0f0830",
   "metadata": {
    "id": "5b0f0830",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model\",type=str,default='GIN')\n",
    "parser.add_argument(\"--epochs\", type=int, default=200)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "parser.add_argument(\"--num_layers\", type=int, default=5)\n",
    "parser.add_argument(\"--hidden\", type=int, default=64)\n",
    "parser.add_argument(\"--lr\", type=float, default=0.01)\n",
    "parser.add_argument(\"--wd\", type=float, default=4e-5)\n",
    "parser.add_argument(\"--noise\", type=float, default=1.0)\n",
    "parser.add_argument(\"--lr_decay_factor\", type=float, default=0.5)\n",
    "parser.add_argument(\"--lr_decay_step_size\", type=int, default=50)\n",
    "\n",
    "parser.add_argument(\"--path\", type=str, default=\"/datasets/\", help=\"where all datasets live\")\n",
    "parser.add_argument(\"--outdir\", type=str, default=\"D:/output/ProteinBINexps/INT8-DQ\")\n",
    "\n",
    "parser.add_argument(\"--DQ\", action=\"store_true\", help=\"enables DegreeQuant\")\n",
    "parser.add_argument(\"--low\", type=float, default=0.0)\n",
    "parser.add_argument(\"--change\", type=float, default=0.1)\n",
    "parser.add_argument(\"--sample_prop\", type=float, default=None)\n",
    "\n",
    "parser.add_argument(\"--result_folder\",type=str,default='result')\n",
    "# Path to checkpoint\n",
    "parser.add_argument(\"--check_folder\",type=str,default='checkpoint')\n",
    "# Path to dataset\n",
    "parser.add_argument(\"--path2dataset\",type=str,default='/')\n",
    "\n",
    "quant_mode = parser.add_mutually_exclusive_group(required=False)\n",
    "quant_mode.add_argument(\"--fp32\", action=\"store_true\", help=\"no quantization\")\n",
    "quant_mode.add_argument(\"--int8\", action=\"store_true\", help=\"INT8 quant\")\n",
    "quant_mode.add_argument(\"--int4\", action=\"store_true\", help=\"INT4 quant\")\n",
    "\n",
    "ste_mode = parser.add_mutually_exclusive_group(required=False)\n",
    "ste_mode.add_argument(\"--ste_abs\", action=\"store_true\", help=\"STE-ABS\")\n",
    "ste_mode.add_argument(\"--ste_mom\", action=\"store_true\", help=\"STE-MOM\")\n",
    "ste_mode.add_argument(\"--ste_per\", action=\"store_true\", help=\"STE-PER\")\n",
    "ste_mode.add_argument(\"--gc_abs\", action=\"store_true\", help=\"GC-ABS\")\n",
    "ste_mode.add_argument(\"--gc_mom\", action=\"store_true\", help=\"GC-MOM\")\n",
    "ste_mode.add_argument(\"--gc_per\", action=\"store_true\", help=\"GC-PER\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args(['--fp32', '--ste_abs'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b248ec",
   "metadata": {
    "id": "97b248ec"
   },
   "source": [
    "### Generating the qConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da52ea40",
   "metadata": {},
   "source": [
    "- INT4=True $\\Rightarrow$ args.int4=True\n",
    "- DQ=True $\\Rightarrow$ args.DQ=True\n",
    "- gc-per=True $\\Rightarrow$ args.gc_per=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24bd71f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24bd71f4",
    "outputId": "7c23602b-16af-4ad7-9f29-d1606b2ea527",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model='GIN', epochs=200, batch_size=128, num_layers=5, hidden=64, lr=0.01, wd=4e-05, noise=1.0, lr_decay_factor=0.5, lr_decay_step_size=50, path='/datasets/', outdir='D:/output/ProteinBINexps/INT8-DQ', DQ=True, low=0.0, change=0.1, sample_prop=None, result_folder='result', check_folder='checkpoint', path2dataset='/', fp32=False, int8=False, int4=True, ste_abs=False, ste_mom=False, ste_per=True, gc_abs=False, gc_mom=False, gc_per=False)\n"
     ]
    }
   ],
   "source": [
    "args.DQ=True\n",
    "args.fp32=False\n",
    "args.int4=True\n",
    "args.int8=False\n",
    "\n",
    "\n",
    "args.ste_abs=False\n",
    "args.ste_mom=False\n",
    "args.ste_per=True\n",
    "args.gc_abs=False\n",
    "args.gc_mom=False\n",
    "args.gc_per=False\n",
    "\n",
    "\n",
    "if args.fp32:\n",
    "    qypte = \"FP32\"\n",
    "elif args.int8:\n",
    "    qypte = \"INT8\"\n",
    "elif args.int4:\n",
    "    qypte = \"INT4\"\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "ste = False\n",
    "momentum = False\n",
    "percentile = None\n",
    "\n",
    "# ste quant\n",
    "if args.ste_abs:\n",
    "    ste = True\n",
    "elif args.ste_mom:\n",
    "    ste = True\n",
    "    momentum = True\n",
    "elif args.gc_abs:\n",
    "    pass\n",
    "elif args.gc_mom:\n",
    "    momentum = True\n",
    "elif args.ste_per:\n",
    "    ste = True\n",
    "    percentile = 0.01 if args.int4 else 0.001\n",
    "elif args.gc_per:\n",
    "    percentile = 0.01 if args.int4 else 0.001\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "if args.DQ:\n",
    "    DQ = {\"prob_mask_low\": args.low, \"prob_mask_change\": args.change}\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb8cf11",
   "metadata": {
    "id": "0bb8cf11"
   },
   "source": [
    "## Loading dataset and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4b42f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path, name, sparse=True, cleaned=False, DQ=None):\n",
    "    dataset = TUDataset(path, name, cleaned=cleaned)\n",
    "    dataset.data.edge_attr = None\n",
    "\n",
    "    if dataset.data.x is None:\n",
    "        max_degree = 0\n",
    "        degs = []\n",
    "        for data in dataset:\n",
    "            degs += [degree(data.edge_index[0], dtype=torch.long)]\n",
    "            max_degree = max(max_degree, degs[-1].max().item())\n",
    "\n",
    "        if max_degree < 1000:\n",
    "            dataset.transform = T.OneHotDegree(max_degree)\n",
    "        else:\n",
    "            deg = torch.cat(degs, dim=0).to(torch.float)\n",
    "            mean, std = deg.mean().item(), deg.std().item()\n",
    "            dataset.transform = NormalizedDegree(mean, std)\n",
    "\n",
    "    \n",
    "    \n",
    "    if not sparse:\n",
    "        num_nodes = max_num_nodes = 0\n",
    "        for data in dataset:\n",
    "            num_nodes += data.num_nodes\n",
    "            max_num_nodes = max(data.num_nodes, max_num_nodes)\n",
    "\n",
    "        # Filter out a few really large graphs in order to apply DiffPool.\n",
    "        if name == \"PROTEINS\":\n",
    "            num_nodes = min(int(num_nodes / len(dataset) * 1.5), max_num_nodes)\n",
    "        else:\n",
    "            num_nodes = min(int(num_nodes / len(dataset) * 5), max_num_nodes)\n",
    "\n",
    "        indices = []\n",
    "        for i, data in enumerate(dataset):\n",
    "            if data.num_nodes <= num_nodes:\n",
    "                indices.append(i)\n",
    "        dataset = dataset[torch.tensor(indices)]\n",
    "\n",
    "        if dataset.transform is None:\n",
    "            dataset.transform = T.ToDense(num_nodes)\n",
    "        else:\n",
    "            dataset.transform = T.Compose([dataset.transform, T.ToDense(num_nodes)])\n",
    " \n",
    "    \n",
    "    \n",
    "    if DQ is not None:\n",
    "        print(f\"Generating ProbabilisticHighDegreeMask: {DQ}\")\n",
    "        dq_transform = ProbabilisticHighDegreeMask(\n",
    "            DQ[\"prob_mask_low\"], min(DQ[\"prob_mask_low\"] + DQ[\"prob_mask_change\"], 1.0)\n",
    "        )\n",
    "        if dataset.transform is None:\n",
    "            dataset.transform = dq_transform\n",
    "        else:\n",
    "            dataset.transform = T.Compose([dataset.transform, dq_transform])\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Ensure dataset.transform is not None\n",
    "    if dataset.transform is None:\n",
    "        dataset.transform = T.Compose([])  # Assign an empty transform\n",
    "   \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "679add88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "679add88",
    "outputId": "0038419f-0775-4347-84d3-fb8c7ca864e7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ProbabilisticHighDegreeMask: {'prob_mask_low': 0.0, 'prob_mask_change': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_name='PROTEINS'\n",
    "dataset = get_dataset(args.path, dataset_name, sparse=True, DQ=DQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08148c9",
   "metadata": {
    "id": "f08148c9"
   },
   "source": [
    "###  Output dir and tensorboard writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43f396e8",
   "metadata": {
    "id": "43f396e8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def append_date_and_time_to_string(string):\n",
    "    now = datetime.utcnow().strftime(\"%m_%d_%H_%M_%S\")\n",
    "\n",
    "    return Path(string) / now\n",
    "\n",
    "\n",
    "def set_outputdir_and_writer(\n",
    "    model_name,\n",
    "    outdir,\n",
    "    num_layers,\n",
    "    hidden,\n",
    "    lr,\n",
    "    quant_mode,\n",
    "    ste,\n",
    "    momentum,\n",
    "    percentile,\n",
    "    is_DQ,\n",
    "    w_decay,\n",
    "    low,\n",
    "    change,\n",
    "):\n",
    "\n",
    "    layers = \"layers_\" + str(num_layers)\n",
    "    hidden = \"hidden_\" + str(hidden)\n",
    "\n",
    "    ste_config = \"STE_\" if ste else \"GC_\"\n",
    "    if momentum:\n",
    "        ste_config += \"MOM\"\n",
    "    elif percentile is not None:\n",
    "        ste_config += \"PER\"\n",
    "    else:\n",
    "        ste_config += \"ABS\"\n",
    "\n",
    "    if is_DQ:\n",
    "        quant_mode += \"_DQ_low\" + str(low) + \"_chng\" + str(change)\n",
    "\n",
    "    dir = (\n",
    "        Path(outdir)\n",
    "        / model_name)\n",
    "\n",
    "    dir = append_date_and_time_to_string(dir)\n",
    "\n",
    "    writer = SummaryWriter(dir)\n",
    "    print(f\"Output dir:{dir}\")\n",
    "\n",
    "    return dir, writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80486b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Setting path to save model and record the output\n",
    "path2result = args.result_folder+'/'+'_'+dataset_name\n",
    "path2check = args.check_folder+'/'+args.model+'_'+dataset_name\n",
    "if not os.path.exists(path2result):\n",
    "    os.makedirs(path2result)\n",
    "if not os.path.exists(path2check):\n",
    "    os.makedirs(path2check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43389e20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43389e20",
    "outputId": "a9767e16-e3d5-477c-8d01-3fd66118c5d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dir:D:\\output\\ProteinBINexps\\INT8-DQ\\GIN\\layers_5\\hidden_64\\INT4_DQ_low0.0_chng0.1\\STE_PER\\lr_0.01\\wd_4e-05\\04_22_08_09_22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# output dir and tensorboard writer\n",
    "dir, writer = utils.set_outputdir_and_writer(\n",
    "    \"GIN\",\n",
    "    args.outdir,\n",
    "    args.num_layers,\n",
    "    args.hidden,\n",
    "    args.lr,\n",
    "    qypte,\n",
    "    ste,\n",
    "    momentum,\n",
    "    percentile,\n",
    "    args.DQ,\n",
    "    args.wd,\n",
    "    args.low,\n",
    "    args.change,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807bacfe",
   "metadata": {},
   "source": [
    "## qGIN Model with Quatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac185aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_quantizer(qypte, ste, momentum, percentile, signed, sample_prop):\n",
    "    if qypte == \"FP32\":\n",
    "        return Identity\n",
    "    else:\n",
    "        return lambda: IntegerQuantizer(\n",
    "            4 if qypte == \"INT4\" else 8,\n",
    "            signed=signed,\n",
    "            use_ste=ste,\n",
    "            use_momentum=momentum,\n",
    "            percentile=percentile,\n",
    "            sample=sample_prop,\n",
    "        )\n",
    "\n",
    "\n",
    "def make_quantizers(qypte, dq, sign_input, ste, momentum, percentile, sample_prop):\n",
    "    if dq:\n",
    "        # GIN doesn't apply DQ to the LinearQuantize layers so we keep the \n",
    "        # default inputs, weights, features keys.\n",
    "        # See NOTE in the multi_quant.py file\n",
    "        layer_quantizers = {\n",
    "            \"inputs\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, sign_input, sample_prop\n",
    "            ),\n",
    "            \"weights\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "            \"features\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "        }\n",
    "        mp_quantizers = {\n",
    "            \"message_low\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "            \"message_high\": create_quantizer(\n",
    "                \"FP32\", ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "            \"update_low\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "            \"update_high\": create_quantizer(\n",
    "                \"FP32\", ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "            \"aggregate_low\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "            \"aggregate_high\": create_quantizer(\n",
    "                \"FP32\", ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "        }\n",
    "    else:\n",
    "        layer_quantizers = {\n",
    "            \"inputs\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, sign_input, sample_prop\n",
    "            ),\n",
    "            \"weights\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "            \"features\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "        }\n",
    "        mp_quantizers = {\n",
    "            \"message\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "            \"update_q\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "            \"aggregate\": create_quantizer(\n",
    "                qypte, ste, momentum, percentile, True, sample_prop\n",
    "            ),\n",
    "        }\n",
    "    return layer_quantizers, mp_quantizers\n",
    "\n",
    "\n",
    "class ResettableSequential(Sequential):\n",
    "    def reset_parameters(self):\n",
    "        for child in self.children():\n",
    "            if hasattr(child, \"reset_parameters\"):\n",
    "                child.reset_parameters()\n",
    "\n",
    "\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        num_layers,\n",
    "        hidden,\n",
    "        dq,\n",
    "        qypte,\n",
    "        ste,\n",
    "        momentum,\n",
    "        percentile,\n",
    "        sample_prop,\n",
    "    ):\n",
    "        super(GIN, self).__init__()\n",
    "\n",
    "        self.is_dq = dq\n",
    "        gin_layer = GINConvMultiQuant if dq else GINConvQuant \n",
    "\n",
    "        lq, mq = make_quantizers(\n",
    "            qypte,\n",
    "            dq,\n",
    "            False,\n",
    "            ste=ste,\n",
    "            momentum=momentum,\n",
    "            percentile=percentile,\n",
    "            sample_prop=sample_prop,\n",
    "        )\n",
    "        lq_signed, _ = make_quantizers(\n",
    "            qypte,\n",
    "            dq,\n",
    "            True,\n",
    "            ste=ste,\n",
    "            momentum=momentum,\n",
    "            percentile=percentile,\n",
    "            sample_prop=sample_prop,\n",
    "        )\n",
    "\n",
    "        # NOTE: see comment in multi_quant.py on the use of \n",
    "        # \"mask-aware\" MLPs.\n",
    "        self.conv1 = gin_layer(\n",
    "            ResettableSequential(\n",
    "                Linear(dataset.num_features, hidden),\n",
    "                ReLU(),\n",
    "                LinearQuantized(hidden, hidden, layer_quantizers=lq),\n",
    "                ReLU(),\n",
    "                BN(hidden),\n",
    "            ),\n",
    "            train_eps=True,\n",
    "            mp_quantizers=mq,\n",
    "        )\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(\n",
    "                gin_layer(\n",
    "                    ResettableSequential(\n",
    "                        LinearQuantized(hidden, hidden, layer_quantizers=lq_signed),\n",
    "                        ReLU(),\n",
    "                        LinearQuantized(hidden, hidden, layer_quantizers=lq),\n",
    "                        ReLU(),\n",
    "                        BN(hidden),\n",
    "                    ),\n",
    "                    train_eps=True,\n",
    "                    mp_quantizers=mq,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.lin1 = LinearQuantized(hidden, hidden, layer_quantizers=lq_signed)\n",
    "        self.lin2 = LinearQuantized(hidden, dataset.num_classes, layer_quantizers=lq)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        # NOTE: It is possible to use the same mask consistently or generate a \n",
    "        # new mask per layer. For other experiments we used a per-layer mask\n",
    "        # We did not observe major differences but we expect the impact will\n",
    "        # be layer and dataset dependent. Extensive experiments assessing the\n",
    "        # difference were not run, however, due to the high cost.\n",
    "         \n",
    "        #if hasattr(data, \"prob_mask\") and data.prob_mask is not None:\n",
    "            #mask = evaluate_prob_mask(data)\n",
    "        #else:\n",
    "            #mask = None\n",
    "            \n",
    "            \n",
    "                \n",
    "     \n",
    "        mask = evaluate_prob_mask(data)\n",
    "        \n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.conv1(x, edge_index, mask)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, mask)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        # NOTE: the linear layers from here do not contribute significantly to run-time\n",
    "        # Therefore you probably don't want to quantize these as it will likely have \n",
    "        # an impact on performance.\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # NOTE: This is a quantized final layer. You probably don't want to be\n",
    "        # this aggressive in practice.\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    \n",
    "model = GIN(\n",
    "    dataset,\n",
    "    num_layers=args.num_layers,\n",
    "    hidden=args.hidden,\n",
    "    dq=args.DQ,\n",
    "    qypte=qypte,\n",
    "    ste=ste,\n",
    "    momentum=momentum,\n",
    "    percentile=None,\n",
    "    sample_prop=args.sample_prop,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae700f2",
   "metadata": {},
   "source": [
    "### Helpful Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51e83377",
   "metadata": {
    "id": "51e83377",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def k_fold(dataset, folds):\n",
    "    skf = StratifiedKFold(folds, shuffle=True, random_state=12345)\n",
    "\n",
    "    test_indices, train_indices = [], []\n",
    "    for _, idx in skf.split(torch.zeros(len(dataset)), dataset.data.y):\n",
    "        test_indices.append(torch.from_numpy(idx))\n",
    "\n",
    "    val_indices = [test_indices[i - 1] for i in range(folds)]\n",
    "\n",
    "    for i in range(folds):\n",
    "        train_mask = torch.ones(len(dataset), dtype=torch.bool)\n",
    "        train_mask[test_indices[i]] = 0\n",
    "        train_mask[val_indices[i]] = 0\n",
    "        train_indices.append(train_mask.nonzero().view(-1))\n",
    "\n",
    "    return train_indices, test_indices, val_indices\n",
    "\n",
    "\n",
    "def train(model, optimizer, loader):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * num_graphs(data)\n",
    "        optimizer.step()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_acc(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(data).max(1)[1]\n",
    "        correct += pred.eq(data.y.view(-1)).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_loss(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "        loss += F.nll_loss(out, data.y.view(-1), reduction=\"sum\").item()\n",
    "    return loss / len(loader.dataset)\n",
    "\n",
    "def num_graphs(data):\n",
    "    if data.batch is not None:\n",
    "        return data.num_graphs\n",
    "    else:\n",
    "        return data.x.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b6cf0e",
   "metadata": {},
   "source": [
    "### Functions for Mmeasuring criterias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8689ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_parameters(model: nn.Module, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the total number of parameters of model\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    num_counted_elements = 0\n",
    "    for param in model.parameters():\n",
    "        if count_nonzero_only:\n",
    "            num_counted_elements += param.count_nonzero()\n",
    "        else:\n",
    "            num_counted_elements += param.numel()\n",
    "    return num_counted_elements\n",
    "\n",
    "# Function to get CPU usage\n",
    "def get_cpu_usage():\n",
    "    return psutil.cpu_percent(interval=1)\n",
    "\n",
    "\n",
    "\n",
    "# Function to approximate power consumption (Assume some average power usage per CPU percentage point)\n",
    "def estimate_power_usage(cpu_usage):\n",
    "    base_power_usage = 10  # Assumed base power usage in watts\n",
    "    power_per_percent = 0.5  # Assumed additional watts per CPU usage percent\n",
    "    return base_power_usage + (power_per_percent * cpu_usage)\n",
    "\n",
    "# The model size based on the number of parameters\n",
    "def calculate_model_size(model: nn.Module, \n",
    "                         qypte: str = 'fp32', \n",
    "                         include_metadata: bool = False,\n",
    "                         model_path: str = None) -> float:\n",
    "    \"\"\"\n",
    "    Calculate model size in KB/MB for different precisions.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        precision: 'fp32' (32-bit float) or 'int4' (4-bit integer)\n",
    "        include_metadata: Whether to include PyTorch metadata in size calculation\n",
    "        model_path: If provided, will check actual file size on disk\n",
    "        \n",
    "    Returns:\n",
    "        Size in KB (if include_metadata=False) or actual file size (if include_metadata=True)\n",
    "    \"\"\"\n",
    "    # Get total number of parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Calculate theoretical size\n",
    "    if qypte == 'FP32':\n",
    "        size_bits = total_params * 32\n",
    "    elif qypte== 'INT4':\n",
    "        size_bits = total_params * 4\n",
    "    elif qypte == 'INT8':\n",
    "        size_bits = total_params * 8    \n",
    "   \n",
    "    \n",
    "    size_bytes = size_bits / 8\n",
    "    size_kb = size_bytes / 1024\n",
    "    \n",
    "    # If checking actual file size\n",
    "    if include_metadata and model_path:\n",
    "        if not os.path.exists(model_path):\n",
    "            # Save model to temporary file if path doesn't exist\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        actual_size_kb = os.path.getsize(model_path) / 1024\n",
    "        return actual_size_kb\n",
    "    \n",
    "    return size_kb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f93b2ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df93c117",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0acce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_validation_with_val_set(\n",
    "    dataset,\n",
    "    model,\n",
    "    folds,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    lr_decay_factor,\n",
    "    lr_decay_step_size,\n",
    "    weight_decay,\n",
    "    qypte='FP32',\n",
    "    use_tqdm=True,\n",
    "    writer=None,\n",
    "    logger=None,\n",
    "):\n",
    "\n",
    "        \n",
    "        val_losses, accs, durations = [], [], []\n",
    "        quant_model_accuracy=[]\n",
    "        quant_model_loss=[]\n",
    "        t_quant_model=[]\n",
    "        Num_parm_quant_model=[]\n",
    "        quant_model_size=[]\n",
    "        quant_energy_consumption=[]\n",
    "        quant_cpu_usage=[]\n",
    "        quant_memory_usage=[]\n",
    "        max_acc=0.4\n",
    "        \n",
    "       \n",
    "        # Initialize a dictionary to store all results per iteration\n",
    "        Eva_iter = {\n",
    "            \"val losses per iter\": [],\n",
    "            \"durations per iter\": [],\n",
    "            \"quant model accuracy per iter\": [],\n",
    "            \"time inference of quant model per iter\": [],\n",
    "            \"number parmameters of quant model per iter\": [],  # Store the best accuracy for each fold\n",
    "            \"size of quant model per iter\": [],\n",
    "            \"energy consumption of quant model per iter\": [],\n",
    "            \"cpu usage of quant model per iter\": [],\n",
    "            \"total memory usage of quant model per iter\": [],\n",
    "            \"final_metrics\": {}  # Store final metrics (mean, std, etc.)\n",
    "        }\n",
    "        \n",
    "        for fold, (train_idx, test_idx, val_idx) in enumerate(zip(*k_fold(dataset, folds))):\n",
    "            train_dataset = dataset[train_idx.tolist()]\n",
    "            test_dataset = dataset[test_idx.tolist()]\n",
    "            val_dataset = dataset[val_idx.tolist()]\n",
    "            if \"adj\" in train_dataset[0]:\n",
    "                    train_loader = DenseLoader(train_dataset, batch_size, shuffle=True)\n",
    "                    val_loader = DenseLoader(val_dataset, batch_size, shuffle=False)\n",
    "                    test_loader = DenseLoader(test_dataset, batch_size, shuffle=False)\n",
    "            else:\n",
    "                    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "                    val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "                    test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "            model.to(device).reset_parameters()\n",
    "            optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            t_start = time.perf_counter()\n",
    "\n",
    "            #if use_tqdm:\n",
    "                #t = tqdm(total=epochs, desc=\"Fold #\" + str(fold))\n",
    "            Eva_fold= OrderedDict() #It is a dictionary to arrange output of this fold\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                    train_loss = train(model, optimizer, train_loader)\n",
    "                    val_loss = eval_loss(model, val_loader)\n",
    "                    val_losses.append(val_loss)\n",
    "                    \n",
    "                    accs.append(eval_acc(model, test_loader))\n",
    "                    eval_info = {\n",
    "                        \"fold\": fold,\n",
    "                        \"epoch\": epoch,\n",
    "                        \"train_loss\": train_loss,\n",
    "                        \"val_loss\": val_losses[-1],\n",
    "                        \"test_acc\": accs[-1],\n",
    "                    }\n",
    "                    acc_test=accs[-1]  \n",
    "                \n",
    "                    if logger is not None:\n",
    "                        logger(eval_info)\n",
    "\n",
    "                    if writer is not None:\n",
    "                        writer.add_scalar(f\"Fold{fold}/Train_Loss\", train_loss, epoch)\n",
    "                        writer.add_scalar(f\"Fold{fold}/Val_Loss\", val_loss, epoch)\n",
    "                        writer.add_scalar(\n",
    "                           f\"Fold{fold}/Lr\", optimizer.param_groups[0][\"lr\"], epoch\n",
    "                        )\n",
    "\n",
    "                    if epoch % lr_decay_step_size == 0:\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group[\"lr\"] = lr_decay_factor * param_group[\"lr\"]\n",
    "\n",
    "                    if epoch % 30 == 0:\n",
    "                        print(f\"Eval Epoch: {epoch} |Val_loss:{val_loss:.03f}| Train_Loss: {train_loss:.3f} | Acc_Val: {val_losses[-1]:.3f}|Fold: {fold}\")\n",
    "                   \n",
    "\n",
    "                \n",
    "                    if(acc_test>max_acc):\n",
    "                        path =  path2check+'/'+args.model+'_'+dataset_name+'_'+'quantized.pth.tar'\n",
    "                        #path = dir+'/'+args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'quantized.pth.tar'\n",
    "                        max_acc = acc_test\n",
    "                        torch.save({'state_dict': model.state_dict(), 'best_accu': acc_test}, path)\n",
    "  \n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.synchronize()\n",
    "                    t_end = time.perf_counter()\n",
    "                    durations.append(t_end - t_start)\n",
    "                    \n",
    "            # Start monitoring CPU and memory usage, model size, number of parametes, time inference and  power consumption\n",
    "            \n",
    "            \n",
    "            quant_model_path= path2check+'/'+args.model+'_'+dataset_name+'_'+'quantized.pth.tar'\n",
    "            state = torch.load(quant_model_path)\n",
    "            dict=state['state_dict']\n",
    "            recover_model = lambda: model.load_state_dict(state['state_dict'])\n",
    "            \n",
    "            gc.collect()\n",
    "            time.sleep(5)  # Add a 5-second delay to stabilize the initial state\n",
    "            tracemalloc.start()  # Start tracking memory allocations\n",
    "            snapshot_before = tracemalloc.take_snapshot()#take a snapshot of the current memory state before starting the measurement.\n",
    "\n",
    "            t0 = time.perf_counter()\n",
    "            initial_cpu_usage = get_cpu_usage()\n",
    "            power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "\n",
    "\n",
    "            fold_quant_model_accuracy= eval_acc(model, test_loader)\n",
    "\n",
    "            fold_quant_cpu_usage = get_cpu_usage()\n",
    "            t1 = time.perf_counter()\n",
    "            fold_t_quant_model=t1-t0\n",
    "\n",
    "            snapshot_after = tracemalloc.take_snapshot()\n",
    "            tracemalloc.stop()\n",
    "            top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "            folde_quant_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "            fold_quant_energy_consumption = power_usage * fold_t_quant_model\n",
    "             #fold_quant_model_size = os.path.getsize(main_model_path)\n",
    "            fold_quant_model_size =calculate_model_size(model, qypte )\n",
    "            fold_num_parm_quant_model=get_num_parameters(model, count_nonzero_only=True)\n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5) \n",
    "            #Update Eva dictionary\n",
    "            Eva_fold.update({'quant model accuracy per fold': fold_quant_model_accuracy,\n",
    "                        'time inference of quant model per fold':fold_t_quant_model,\n",
    "                        'number parmameters of quant model per fold': fold_num_parm_quant_model,\n",
    "                        'size of quant model per fold': fold_quant_model_size, \n",
    "                        'energy consumption of quant model per fold':fold_quant_energy_consumption,\n",
    "                        'total memory usage of quant model per fold':folde_quant_total_memory_diff,\n",
    "                        'cpu usage of quant model per fold':fold_quant_cpu_usage\n",
    "                       })\n",
    "            \n",
    "            gc.collect()\n",
    "            time.sleep(5) \n",
    "   \n",
    "\n",
    "            quant_model_accuracy.append(Eva_fold['quant model accuracy per fold'])\n",
    "            t_quant_model.append(Eva_fold['time inference of quant model per fold'])\n",
    "            Num_parm_quant_model.append(int(Eva_fold['number parmameters of quant model per fold']))\n",
    "            quant_model_size.append(int(Eva_fold['size of quant model per fold']))\n",
    "            quant_energy_consumption.append(Eva_fold['energy consumption of quant model per fold'])\n",
    "            quant_cpu_usage.append(Eva_fold['cpu usage of quant model per fold'])\n",
    "            quant_memory_usage.append(Eva_fold['total memory usage of quant model per fold'])\n",
    "\n",
    "           \n",
    "\n",
    "     \n",
    "     \n",
    "        Eva_iter[\"quant model accuracy per iter\"]= stat.mean(quant_model_accuracy)\n",
    "        Eva_iter[\"time inference of quant model per iter\"]= stat.mean(t_quant_model)\n",
    "        Eva_iter[\"number parmameters of quant model per iter\"]=  stat.mean(Num_parm_quant_model)\n",
    "        Eva_iter[\"size of quant model per iter\"]= stat.mean(quant_model_size)\n",
    "        Eva_iter[\"energy consumption of quant model per iter\"]= stat.mean(quant_energy_consumption)\n",
    "        Eva_iter[\"cpu usage of quant model per iter\"]= stat.mean(quant_cpu_usage)\n",
    "        Eva_iter[\"total memory usage of quant model per iter\"]= stat.mean(quant_memory_usage)\n",
    "    \n",
    "    \n",
    "        loss, acc, duration = tensor(val_losses), tensor(accs), tensor(durations)\n",
    "        loss, acc = loss.view(folds, epochs), acc.view(folds, epochs)\n",
    "        loss, argmin = loss.min(dim=1)\n",
    "        acc = acc[torch.arange(folds, dtype=torch.long), argmin]\n",
    "\n",
    "        Eva_iter[\"val losses per iter\"]= loss.mean().item()\n",
    "        Eva_iter[\"durations per iter\"]= duration.mean().item()\n",
    "\n",
    "        \n",
    "      \n",
    "        \n",
    "        return Eva_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598bed17",
   "metadata": {},
   "source": [
    "### Manual Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d3fac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca097874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd673d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are all list of criteria for measurements. \n",
    "# We collect all desired datas of each list across iterations. \n",
    "# Then, we compute average and std of each list.\n",
    "\n",
    "\n",
    "\n",
    "#quant model\n",
    "Quant_val_loss=[]\n",
    "Quant_duration=[]\n",
    "Quant_model_accuracy=[]\n",
    "T_quant_model=[]\n",
    "Num_parm_quant_model=[]\n",
    "Quant_model_size=[]\n",
    "Quant_Energy_Consumption=[]\n",
    "Quant_Cpu_Usage=[]\n",
    "Quant_Memory_Usage=[]\n",
    "\n",
    "\n",
    "# Here is the dictionary to record the list of all measurements\n",
    "Eva_measure={'quant validation loss':Quant_val_loss,\n",
    "             'quant duration':Quant_duration,\n",
    "            'quant model accuracy': Quant_model_accuracy,\n",
    "            'time inference of quant model':T_quant_model,\n",
    "            'number parmameters of quant model':Num_parm_quant_model,\n",
    "            'quant model size':Quant_model_size,\n",
    "            'energy consumption of quant model':Quant_Energy_Consumption,\n",
    "            'cpu usage of quant model':Quant_Cpu_Usage,\n",
    "            'memory usage of quant model':Quant_Memory_Usage}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31de1159",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations=1\n",
    "epochs=10\n",
    "folds=10\n",
    "batch_size=args.batch_size\n",
    "lr=args.lr\n",
    "lr_decay_factor=args.lr_decay_factor\n",
    "lr_decay_step_size=args.lr_decay_step_size\n",
    "weight_decay=args.wd\n",
    "writer=writer\n",
    "logger=None\n",
    "use_tqdm=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "592618cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************\n",
      "The iteration is :1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "#### load the quantized  model\n",
    "\n",
    "for i in range(iterations):\n",
    "    print('********************************************')\n",
    "    print(f'The iteration is :{i+1} ')\n",
    "   \n",
    " \n",
    "\n",
    "    \n",
    "\n",
    "    Eva_iter=cross_validation_with_val_set(\n",
    "                                        dataset,\n",
    "                                        model,\n",
    "                                        folds,\n",
    "                                        epochs,\n",
    "                                        batch_size,\n",
    "                                        lr,\n",
    "                                        lr_decay_factor,\n",
    "                                        lr_decay_step_size,\n",
    "                                        weight_decay,\n",
    "                                        qypte,\n",
    "                                        use_tqdm=True,\n",
    "                                        writer=None,\n",
    "                                        logger=None,)\n",
    "\n",
    "\n",
    " \n",
    "    Quant_val_loss.append(Eva_iter[\"val losses per iter\"])\n",
    "    Quant_duration.append(Eva_iter[\"durations per iter\"])\n",
    "    Quant_model_accuracy.append(Eva_iter[\"quant model accuracy per iter\"])\n",
    "    T_quant_model.append(Eva_iter[\"time inference of quant model per iter\"])\n",
    "    Num_parm_quant_model.append(Eva_iter[\"number parmameters of quant model per iter\"])\n",
    "    Quant_model_size.append(Eva_iter[\"size of quant model per iter\"])\n",
    "    Quant_Energy_Consumption.append(Eva_iter[\"energy consumption of quant model per iter\"])\n",
    "    Quant_Cpu_Usage.append( Eva_iter[\"cpu usage of quant model per iter\"])\n",
    "    Quant_Memory_Usage.append(Eva_iter[\"total memory usage of quant model per iter\"])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3265c7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All measurement about DQ Quantization process of type:INT4 on modes:True  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Ave of quant loss validation', 0.674),\n",
       "             ('Std of quant loss validation', 0.001),\n",
       "             ('Ave of quant model duration', 167.966),\n",
       "             ('Std of quant model duration', 26.397),\n",
       "             ('Ave of quant model accuracy', 0.596),\n",
       "             ('Std of quant model accuracy', 0.001),\n",
       "             ('Ave of time inference of quant model', 3.132),\n",
       "             ('Std of time inference of quant model', 0.239),\n",
       "             ('Ave of number parmameters of quant model', 42631),\n",
       "             ('Std of number parmameters of quant model', 0.0),\n",
       "             ('Ave of quant model size', 225501),\n",
       "             ('Std of quant_model_size', 0.0),\n",
       "             ('Ave of energy consumption of quant model', 172.8554628812924),\n",
       "             ('Std of energy consumption of quant model', 35.17199120703456),\n",
       "             ('Ave of cpu usage of quant model', 88.49000000000001),\n",
       "             ('Std of cpu usage of quant model', 15.45735423673793),\n",
       "             ('Ave of memory usage of quant model', 46533.75),\n",
       "             ('Std of memory usage of quant model', 741.9671454990464)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a dictionary to save all measurements. Aftre measuring, we can compute mean and std of each item.\n",
    "from collections import OrderedDict \n",
    "Eva_final = OrderedDict()\n",
    "\n",
    "\n",
    "\n",
    "quant_model_val_loss_mean =stat.mean(Quant_val_loss)\n",
    "quant_model_val_loss_std = stat.stdev(Quant_val_loss)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of quant loss validation':float(format(quant_model_val_loss_mean, '.3f'))})\n",
    "Eva_final.update({'Std of quant loss validation':float(format(quant_model_val_loss_std, '.3f'))})    \n",
    "\n",
    "quant_model_duration_mean =stat.mean(Quant_duration)\n",
    "quant_model_duration_std = stat.stdev(Quant_duration)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of quant model duration':float(format(quant_model_duration_mean , '.3f'))})\n",
    "Eva_final.update({'Std of quant model duration':float(format(quant_model_duration_std, '.3f'))})                                         \n",
    "                                     \n",
    "\n",
    "quant_model_accuracy_mean =stat.mean(Quant_model_accuracy)\n",
    "quant_model_accuracy_std = stat.stdev(Quant_model_accuracy)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of quant model accuracy':float(format(quant_model_accuracy_mean, '.3f'))})\n",
    "Eva_final.update({'Std of quant model accuracy':float(format(quant_model_accuracy_std, '.3f'))})\n",
    "                 \n",
    "\n",
    "t_quant_model_mean = stat.mean(T_quant_model)\n",
    "t_quant_model_std =stat.stdev(T_quant_model)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of time inference of quant model':float(format(t_quant_model_mean, '.3f'))})\n",
    "Eva_final.update({'Std of time inference of quant model':float(format(t_quant_model_std, '.3f'))})\n",
    "\n",
    "num_parm_quant_model_mean = stat.mean(Num_parm_quant_model)\n",
    "num_parm_quant_model_std = stat.stdev(Num_parm_quant_model)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of number parmameters of quant model':num_parm_quant_model_mean})\n",
    "Eva_final.update({'Std of number parmameters of quant model':num_parm_quant_model_std})\n",
    "\n",
    "quant_model_size_mean =stat.mean( Quant_model_size)\n",
    "quant_model_size_std = stat.stdev(Quant_model_size)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of quant model size':quant_model_size_mean})\n",
    "Eva_final.update({'Std of quant_model_size':quant_model_size_std })\n",
    "\n",
    "quant_energy_consumption_mean = stat.mean(Quant_Energy_Consumption)\n",
    "quant_energy_consumption_std = stat.stdev(Quant_Energy_Consumption)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of energy consumption of quant model':quant_energy_consumption_mean })\n",
    "Eva_final.update({'Std of energy consumption of quant model':quant_energy_consumption_std})\n",
    "\n",
    "\n",
    "quant_cpu_usage_mean = stat.mean(Quant_Cpu_Usage)\n",
    "quant_cpu_usage_std = stat.stdev(Quant_Cpu_Usage)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of cpu usage of quant model':quant_cpu_usage_mean})\n",
    "Eva_final.update({'Std of cpu usage of quant model':quant_cpu_usage_std})\n",
    "\n",
    "quant_memory_usage_mean = stat.mean(Quant_Memory_Usage)\n",
    "quant_memory_usage_std = stat.stdev(Quant_Memory_Usage)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of memory usage of quant model':quant_memory_usage_mean})\n",
    "Eva_final.update({'Std of memory usage of quant model':quant_memory_usage_std})\n",
    "\n",
    "#################################\n",
    "\n",
    "\n",
    "# Determing Quantization Method \n",
    "if args.DQ == True:\n",
    "    dq='DQ'\n",
    "else:\n",
    "    dq='QAT'\n",
    "print(f\"All measurement about {dq} Quantization process of type:{ qypte} on modes:{args.DQ}  \")   \n",
    "Eva_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49202d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fac3409f",
   "metadata": {},
   "source": [
    "### Recording the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be35c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining mode\n",
    "if args.ste_abs:\n",
    "    mode = 'ste_abs'\n",
    "elif args.ste_mom:\n",
    "    mode = 'ste_mom'   \n",
    "elif args.gc_abs:\n",
    "    mode = 'gc_abs'\n",
    "elif args.gc_mom:\n",
    "    mode = 'gc_mom'\n",
    "elif args.ste_per:\n",
    "    mode = 'ste_per'\n",
    "elif args.gc_per:\n",
    "    mode = 'gc_per'\n",
    "    \n",
    "    \n",
    "# Determing qypte   \n",
    "\n",
    "if args.fp32:\n",
    "    qypte = \"FP32\"\n",
    "elif args.int8:\n",
    "    qypte = \"INT8\"\n",
    "elif args.int4:\n",
    "    qypte = \"INT4\"\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    \n",
    "# Determing Quantization Method \n",
    "if args.DQ == True:\n",
    "    dq='DQ'\n",
    "else:\n",
    "    dq='QAT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0221ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "file_name = path2result+'/'+'Method_type_'+ qypte +'_and_Quantization_is_'+dq+'_On_'+dataset_name+'_with_Mode_'+mode+'.txt'\n",
    "\n",
    "with open(file_name, 'w') as f:\n",
    "    for key, value in vars(args).items():\n",
    "        f.write('%s:%s\\n'%(key, value))\n",
    "\n",
    "    for key, value in Eva_final.items():\n",
    "        f.write('%s:%s\\n'%(key, value))\n",
    "\n",
    "    for key, value in Eva_measure.items():\n",
    "        f.write('%s:%s\\n' % (key, ','.join(map(str, value))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c91374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76118f69",
   "metadata": {
    "id": "76118f69",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0e4ce",
   "metadata": {
    "id": "c6b0e4ce",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
