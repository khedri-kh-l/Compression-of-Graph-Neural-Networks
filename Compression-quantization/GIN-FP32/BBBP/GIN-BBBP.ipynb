{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a7ccc6",
   "metadata": {},
   "source": [
    "## Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "h63j2srEtN_2",
   "metadata": {
    "id": "h63j2srEtN_2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import tensor\n",
    "from torch.nn import Linear, Sequential, ReLU, Identity, BatchNorm1d as BN\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree,remove_self_loops\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import TUDataset,Planetoid,GNNBenchmarkDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_scatter import scatter_mean\n",
    "from torch.autograd.function import InplaceFunction\n",
    "from torch_geometric.nn import GCNConv,GINConv,global_mean_pool,TopKPooling\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import argparse\n",
    "import statistics as stat\n",
    "from tabulate import tabulate\n",
    "from collections import OrderedDict\n",
    "\n",
    "# CPU and Enegusage \n",
    "import psutil\n",
    "import itertools\n",
    "import tracemalloc\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577dbc83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52bdaabe",
   "metadata": {},
   "source": [
    "### Functions for Mmeasuring criterias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cb681d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_parameters(model: nn.Module, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the total number of parameters of model\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    num_counted_elements = 0\n",
    "    for param in model.parameters():\n",
    "        if count_nonzero_only:\n",
    "            num_counted_elements += param.count_nonzero()\n",
    "        else:\n",
    "            num_counted_elements += param.numel()\n",
    "    return num_counted_elements\n",
    "\n",
    "# Function to get CPU usage\n",
    "def get_cpu_usage():\n",
    "    return psutil.cpu_percent(interval=1)\n",
    "\n",
    "\n",
    "\n",
    "# Function to approximate power consumption (Assume some average power usage per CPU percentage point)\n",
    "def estimate_power_usage(cpu_usage):\n",
    "    base_power_usage = 10  # Assumed base power usage in watts\n",
    "    power_per_percent = 0.5  # Assumed additional watts per CPU usage percent\n",
    "    return base_power_usage + (power_per_percent * cpu_usage)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_model_size(model: nn.Module, \n",
    "                         qypte: str = 'fp32', \n",
    "                         include_metadata: bool = False,\n",
    "                         model_path: str = None) -> float:\n",
    "    \"\"\"\n",
    "    Calculate model size in KB/MB for different precisions.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        precision: 'fp32' (32-bit float) or 'int4' (4-bit integer)\n",
    "        include_metadata: Whether to include PyTorch metadata in size calculation\n",
    "        model_path: If provided, will check actual file size on disk\n",
    "        \n",
    "    Returns:\n",
    "        Size in KB (if include_metadata=False) or actual file size (if include_metadata=True)\n",
    "    \"\"\"\n",
    "    # Get total number of parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Calculate theoretical size\n",
    "    if qypte == 'FP32':\n",
    "        size_bits = total_params * 32\n",
    "    elif qypte== 'INT4':\n",
    "        size_bits = total_params * 4\n",
    "    elif qypte == 'INT8':\n",
    "        size_bits = total_params * 8    \n",
    "   \n",
    "    \n",
    "    size_bytes = size_bits / 8\n",
    "    size_kb = size_bytes / 1024\n",
    "    \n",
    "    # If checking actual file size\n",
    "    if include_metadata and model_path:\n",
    "        if not os.path.exists(model_path):\n",
    "            # Save model to temporary file if path doesn't exist\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        actual_size_kb = os.path.getsize(model_path) / 1024\n",
    "        return actual_size_kb\n",
    "    \n",
    "    return size_kb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Byte = 8\n",
    "KiB = 1024 * Byte\n",
    "MiB = 1024 * KiB\n",
    "GiB = 1024 * MiB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f1df7f",
   "metadata": {},
   "source": [
    "## GIN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96d3711",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GIN Model\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import GINConv\n",
    "\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden_units, num_deg=1000, init='norm'):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(9, hidden_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_units, hidden_units),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_units),\n",
    "            ),\n",
    "            train_eps=True,\n",
    "        )\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(\n",
    "                GINConv(\n",
    "                    nn.Sequential(\n",
    "                        nn.Linear(hidden_units, hidden_units),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(hidden_units, hidden_units),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm1d(hidden_units),\n",
    "                    ),\n",
    "                    train_eps=True,\n",
    "                )\n",
    "            )\n",
    "        self.bn_list = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.bn_list.append(nn.BatchNorm1d(hidden_units))\n",
    "        \n",
    "        self.lin1 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.lin2 = nn.Linear(hidden_units, dataset.num_classes)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        #print(self.conv1(x, edge_index))\n",
    "       # print(self.conv1(x, edge_index).size)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn_list[0](x)\n",
    "        i = 1\n",
    "        for conv in self.convs:\n",
    "            x= conv(x, edge_index)\n",
    "            x = self.bn_list[i](x)\n",
    "            i += 1\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7911a",
   "metadata": {},
   "source": [
    "# Helpful Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51cf5b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NormalizedDegree(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, data):\n",
    "        deg = degree(data.edge_index[0], dtype=torch.float)\n",
    "        deg = (deg - self.mean) / self.std\n",
    "        data.x = deg.view(-1, 1)\n",
    "        return data\n",
    "\n",
    "\n",
    "def num_graphs(data):\n",
    "    if data.batch is not None:\n",
    "        return data.num_graphs\n",
    "    else:\n",
    "        return data.x.size(0)\n",
    "\n",
    "\n",
    "def train(model, optimizer, loader,a_loss, a_storage=1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        #out,bit_sum = model(data)\n",
    "        out = model(data)\n",
    "        target = data.y.view(-1)\n",
    "\n",
    "        # Check if the last batch is smaller\n",
    "        if out.size(0) != target.size(0):\n",
    "            target = target[:out.size(0)]  # Truncate target to match output size\n",
    "\n",
    "        loss = F.cross_entropy(out, target)\n",
    "        #loss_store = a_loss*F.relu(bit_sum-a_storage)**2\n",
    "        #loss_store.backward(retain_graph=True)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * num_graphs(data)\n",
    "        optimizer.step()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "##\n",
    "\n",
    "\n",
    "\n",
    "def eval_acc(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:        \n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            #pred = model(data)[0].max(1)[1] \n",
    "            pred = model(data).max(1)[1]\n",
    "\n",
    "        target = data.y.view(-1)\n",
    "        # Check if the last batch is smaller\n",
    "        if pred.size(0) != target.size(0):\n",
    "            target = target[:pred.size(0)]  # Truncate target to match output size                 \n",
    "        correct+=pred.eq(target).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_loss(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    for data in loader:     \n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "           #out = model(data)[0]\n",
    "            out = model(data)\n",
    "\n",
    "        target = data.y.view(-1)\n",
    "\n",
    "        # Check if the last batch is smaller\n",
    "        if out.size(0) != target.size(0):\n",
    "            target = target[:out.size(0)]  # Truncate target to match output size                \n",
    "        loss += F.cross_entropy(out, target, reduction=\"sum\").item()\n",
    "     \n",
    "    return loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "# Real k_fold\n",
    "def k_fold(dataset, folds):\n",
    "    skf = StratifiedKFold(folds, shuffle=True, random_state=12345)\n",
    "\n",
    "    test_indices, train_indices = [], []\n",
    "    for _, idx in skf.split(torch.zeros(len(dataset)), dataset.data.y):\n",
    "        test_indices.append(torch.from_numpy(idx))\n",
    "\n",
    "    val_indices = [test_indices[i - 1] for i in range(folds)]\n",
    "\n",
    "    for i in range(folds):\n",
    "        train_mask = torch.ones(len(dataset), dtype=torch.bool)\n",
    "        train_mask[test_indices[i]] = 0\n",
    "        train_mask[val_indices[i]] = 0\n",
    "        train_indices.append(train_mask.nonzero().view(-1))\n",
    "\n",
    "    return train_indices, test_indices, val_indices\n",
    "\n",
    "\n",
    "def load_checkpoint(model, checkpoint):\n",
    "    if checkpoint != 'No':\n",
    "        print(\"loading checkpoint...\")\n",
    "        model_dict = model.state_dict()\n",
    "        modelCheckpoint = torch.load(checkpoint)\n",
    "        pretrained_dict = modelCheckpoint['state_dict']\n",
    "        new_dict = {k: v for k, v in pretrained_dict.items() if ((k in model_dict.keys()))}\n",
    "        model_dict.update(new_dict)\n",
    "        print('Total : {}, update: {}'.format(len(pretrained_dict), len(new_dict)))\n",
    "        model.load_state_dict(model_dict)\n",
    "        print(\"loaded finished!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c9e20",
   "metadata": {
    "id": "5e53efba"
   },
   "source": [
    "## Setting Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a25086e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a25086e1",
    "outputId": "ccea8abe-0fbd-4eb1-815a-2c21d54b55d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model='GIN', gpu_id=0, dataset_name='BBBP', num_deg=1000, num_layers=5, hidden_units=64, batch_size=64, bit=4, max_epoch=100, max_cycle=2000, folds=10, weight_decay=0, lr=0.01, a_loss=0.001, lr_quant_scale_fea=0.02, lr_quant_scale_xw=0.01, lr_quant_scale_weight=0.02, lr_quant_bit_fea=0.008, lr_quant_bit_weight=0.0001, lr_step_size=50, lr_decay_factor=0.5, lr_schedule_patience=10, is_naive=False, resume=True, store_ckpt=True, uniform=True, use_norm_quant=True, a_storage=1, result_folder='result', check_folder='checkpoint', pathdataset='/')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "\n",
    "# Clearing the arguments\n",
    "sys.argv = ['']\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model',type=str,default='GIN')\n",
    "parser.add_argument('--gpu_id',type=int,default=0)\n",
    "parser.add_argument('--dataset_name',type=str,default='BBBP')\n",
    "parser.add_argument('--num_deg',type=int,default=1000)\n",
    "parser.add_argument('--num_layers', type=int, default=5)\n",
    "parser.add_argument('--hidden_units',type=int,default=64)\n",
    "parser.add_argument('--batch-size',type=int,default=64)\n",
    "parser.add_argument('--bit',type=int,default=4)\n",
    "parser.add_argument('--max_epoch',type=int,default=100)\n",
    "parser.add_argument('--max_cycle',type=int,default=2000)\n",
    "parser.add_argument('--folds',type=int,default=10)\n",
    "parser.add_argument('--weight_decay',type=float,default=0)\n",
    "parser.add_argument('--lr',type=float,default=0.01)\n",
    "parser.add_argument('--a_loss',type=float,default=0.001)\n",
    "parser.add_argument('--lr_quant_scale_fea',type=float,default=0.02)\n",
    "parser.add_argument('--lr_quant_scale_xw',type=float,default=1e-2)\n",
    "parser.add_argument('--lr_quant_scale_weight',type=float,default=0.02)\n",
    "parser.add_argument('--lr_quant_bit_fea',type=float,default=0.008)\n",
    "parser.add_argument('--lr_quant_bit_weight',type=float,default=0.0001)\n",
    "parser.add_argument('--lr_step_size',type=int, default=50)\n",
    "parser.add_argument('--lr_decay_factor',type=float,default=0.5)\n",
    "parser.add_argument('--lr_schedule_patience',type=int,default=10)\n",
    "parser.add_argument('--is_naive',type=bool,default=False)\n",
    "###############################################################\n",
    "parser.add_argument('--resume',type=bool,default=True)\n",
    "parser.add_argument('--store_ckpt',type=bool,default=True)\n",
    "parser.add_argument('--uniform',type=bool,default=True)\n",
    "parser.add_argument('--use_norm_quant',type=bool,default=True)\n",
    "###############################################################\n",
    "# The target memory size of nodes features\n",
    "parser.add_argument('--a_storage',type=float,default=1)\n",
    "# Path to results\n",
    "parser.add_argument('--result_folder',type=str,default='result')\n",
    "# Path to checkpoint\n",
    "parser.add_argument('--check_folder',type=str,default='checkpoint')\n",
    "# Path to dataset\n",
    "parser.add_argument('--pathdataset',type=str,default='/')\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1be47ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "model = args.model\n",
    "dataset_name = args.dataset_name\n",
    "num_layers = args.num_layers\n",
    "hidden_units=args.hidden_units\n",
    "bit=args.bit\n",
    "max_epoch = args.max_epoch\n",
    "resume = args.resume\n",
    "\n",
    "\n",
    "# Path direction\n",
    "pathresult = args.result_folder+'/'+args.model+'_'+dataset_name\n",
    "pathcheck = args.check_folder+'/'+args.model+'_'+dataset_name\n",
    "if not os.path.exists(pathresult):\n",
    "    os.makedirs(pathresult)\n",
    "if not os.path.exists(pathcheck):\n",
    "    os.makedirs(pathcheck)\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7160de",
   "metadata": {},
   "source": [
    "## Loading Dataset and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faf50762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import MoleculeNet\n",
    "def load_MolecueNet(dataset_dir, dataset_name, task=None):\n",
    "    \"\"\" Attention the multi-task problems not solved yet \"\"\"\n",
    "    molecule_net_dataset_names = {name.lower(): name for name in MoleculeNet.names.keys()}\n",
    "    dataset = MoleculeNet(root=dataset_dir, name=molecule_net_dataset_names[dataset_name.lower()])\n",
    "    dataset.data.x = dataset.data.x.float()\n",
    "    if task is None:\n",
    "        dataset.data.y = dataset.data.y.squeeze().long()\n",
    "    else:\n",
    "        dataset.data.y = dataset.data.y[task].long()\n",
    "    dataset.node_type_dict = None\n",
    "    dataset.node_color = None\n",
    "    return dataset\n",
    "\n",
    "def get_dataset(dataset_dir, dataset_name, task=None):\n",
    "   \n",
    "    molecule_net_dataset_names = [name.lower() for name in MoleculeNet.names.keys()]\n",
    "    dataset=load_MolecueNet(dataset_dir, dataset_name, task)\n",
    "\n",
    "    return  dataset\n",
    "    \n",
    "\n",
    "\n",
    "def get_dataloader(dataset, batch_size, random_split_flag=True, data_split_ratio=None, seed=2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset:\n",
    "        batch_size: int\n",
    "        random_split_flag: bool\n",
    "        data_split_ratio: list, training, validation and testing ratio\n",
    "        seed: random seed to split the dataset randomly\n",
    "    Returns:\n",
    "        a dictionary of training, validation, and testing dataLoader\n",
    "    \"\"\"\n",
    "\n",
    "    if not random_split_flag and hasattr(dataset, 'supplement'):\n",
    "        assert 'split_indices' in dataset.supplement.keys(), \"split idx\"\n",
    "        split_indices = dataset.supplement['split_indices']\n",
    "        train_indices = torch.where(split_indices == 0)[0].numpy().tolist()\n",
    "        dev_indices = torch.where(split_indices == 1)[0].numpy().tolist()\n",
    "        test_indices = torch.where(split_indices == 2)[0].numpy().tolist()\n",
    "\n",
    "        train = Subset(dataset, train_indices)\n",
    "        eval = Subset(dataset, dev_indices)\n",
    "        test = Subset(dataset, test_indices)\n",
    "    else:\n",
    "        num_train = int(0.8 * len(dataset))\n",
    "        num_eval = int(0.1 * len(dataset))\n",
    "        num_test = len(dataset) - num_train - num_eval\n",
    "\n",
    "        train, eval, test = random_split(dataset, lengths=[num_train, num_eval, num_test],\n",
    "                                         generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    dataloader = dict()\n",
    "    dataloader['train'] = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    dataloader['eval'] = DataLoader(eval, batch_size=batch_size, shuffle=False)\n",
    "    dataloader['test'] = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "172933e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv\n",
      "Processing...\n",
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\datasets\\molecule_net.py:213: UserWarning: Skipping molecule 'O=N([O-])C1=C(CN=C1NCCSCc2ncccc2)Cc3ccccc3' since it resulted in zero atoms\n",
      "  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n",
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\datasets\\molecule_net.py:213: UserWarning: Skipping molecule 'c1(nc(NC(N)=[NH2])sc1)CSCCNC(=[NH]C#N)NC' since it resulted in zero atoms\n",
      "  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n",
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\datasets\\molecule_net.py:213: UserWarning: Skipping molecule 'Cc1nc(sc1)\\[NH]=C(\\N)N' since it resulted in zero atoms\n",
      "  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n",
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\datasets\\molecule_net.py:213: UserWarning: Skipping molecule 's1cc(CSCCN\\C(NC)=[NH]\\C#N)nc1\\[NH]=C(\\N)N' since it resulted in zero atoms\n",
      "  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n",
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\datasets\\molecule_net.py:213: UserWarning: Skipping molecule 'c1c(c(ncc1)CSCCN\\C(=[NH]\\C#N)NCC)Br' since it resulted in zero atoms\n",
      "  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n",
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\datasets\\molecule_net.py:213: UserWarning: Skipping molecule 'n1c(csc1\\[NH]=C(\\N)N)c1ccccc1' since it resulted in zero atoms\n",
      "  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n",
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\datasets\\molecule_net.py:213: UserWarning: Skipping molecule 'n1c(csc1\\[NH]=C(\\N)N)c1cccc(c1)N' since it resulted in zero atoms\n",
      "  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n",
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\datasets\\molecule_net.py:213: UserWarning: Skipping molecule 'n1c(csc1\\[NH]=C(\\N)N)c1cccc(c1)NC(C)=O' since it resulted in zero atoms\n",
      "  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n",
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\datasets\\molecule_net.py:213: UserWarning: Skipping molecule 'n1c(csc1\\[NH]=C(\\N)N)c1cccc(c1)N\\C(NC)=[NH]\\C#N' since it resulted in zero atoms\n",
      "  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n",
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\datasets\\molecule_net.py:213: UserWarning: Skipping molecule 's1cc(nc1\\[NH]=C(\\N)N)C' since it resulted in zero atoms\n",
      "  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n",
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\datasets\\molecule_net.py:213: UserWarning: Skipping molecule 'c1(cc(N\\C(=[NH]\\c2cccc(c2)CC)C)ccc1)CC' since it resulted in zero atoms\n",
      "  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n",
      "Done!\n",
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args.dataset_name=='BBBP'\n",
    "dataset = get_dataset(args.pathdataset, args.dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b2ed79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if dataset.data.x is None:\n",
    "    max_degree = 0\n",
    "    degs = []\n",
    "    for data in dataset:\n",
    "        degs += [degree(data.edge_index[0], dtype=torch.long)]\n",
    "        max_degree = max(max_degree, degs[-1].max().item())\n",
    "\n",
    "    if max_degree < 1000:\n",
    "        dataset.transform = T.OneHotDegree(max_degree)\n",
    "    else:\n",
    "        deg = torch.cat(degs, dim=0).to(torch.float)\n",
    "        mean, std = deg.mean().item(), deg.std().item()\n",
    "        dataset.transform = NormalizedDegree(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1193d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(bit=32, max_epoch=5):\n",
    "        args.batch_size=16\n",
    "        args.max_epoch=5\n",
    "        args.batch_size=32\n",
    "        max_acc =0.65\n",
    "        if bit== 32:\n",
    "            qypte='FP32'\n",
    "        elif bit== 4:\n",
    "            qypte= 'INT4'    \n",
    "        elif bit== 8:\n",
    "            qypte = 'INT8'\n",
    "            \n",
    "        \n",
    "     \n",
    "      \n",
    "        val_losses, accu, durations = [], [], []\n",
    "        main_model_accuracy=[]\n",
    "        main_model_loss=[]\n",
    "        t_main_model=[]\n",
    "        Num_parm_main_model=[]\n",
    "        main_model_size=[]\n",
    "        main_energy_consumption=[]\n",
    "        main_cpu_usage=[]\n",
    "        main_memory_usage=[]\n",
    "       \n",
    " \n",
    "        #Eva= OrderedDict()\n",
    "        #Eva=dict()\n",
    "        # Initialize a dictionary to store all results per iteration\n",
    "        Eva_iter = {\n",
    "            \"val losses per iter\": [],\n",
    "            \"durations per iter\": [],\n",
    "            \"main model accuracy per iter\": [],\n",
    "            \"time inference of main model per iter\": [],\n",
    "            \"number parmameters of main model per iter\": [],  # Store the best accuracy for each fold\n",
    "            \"size of main model per iter\": [],\n",
    "            \"energy consumption of main model per iter\": [],\n",
    "            \"cpu usage of main model per iter\": [],\n",
    "            \"total memory usage of main model per iter\": [],\n",
    "            \"final_metrics\": {}  # Store final metrics (mean, std, etc.)\n",
    "        }\n",
    "    \n",
    "        \n",
    "     \n",
    "\n",
    "\n",
    "        for fold, (train_idx, test_idx, val_idx) in enumerate(zip(*k_fold(dataset, args.folds))):\n",
    "            print_max_acc=0\n",
    "            train_dataset = dataset[train_idx.tolist()]\n",
    "            test_dataset = dataset[test_idx.tolist()]\n",
    "            val_dataset = dataset[val_idx.tolist()]\n",
    "            train_loader = DataLoader(train_dataset, args.batch_size, num_workers=0,shuffle=False, drop_last=True)\n",
    "            val_loader = DataLoader(val_dataset, args.batch_size,num_workers=0,shuffle=False,drop_last=True)\n",
    "            test_loader = DataLoader(test_dataset, args.batch_size,num_workers=0,shuffle=False, drop_last=True)\n",
    "            k=0\n",
    "\n",
    "\n",
    "            model=GIN(train_dataset, args.num_layers,hidden_units=args.hidden_units,\n",
    "                num_deg=args.num_deg).to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size, gamma=args.lr_decay_factor)\n",
    "            \n",
    "            t_start = time.perf_counter()\n",
    "\n",
    "            Eva_fold= OrderedDict() #It is a dictionary to arrange output of this fold\n",
    "        \n",
    "            for epoch in range(max_epoch):\n",
    "                #t = tqdm(epoch)\n",
    "                train_loss=0\n",
    "                train_loss = train(model,optimizer,train_loader,args.a_loss, args.a_storage)\n",
    "                start = time.process_time()\n",
    "                val_loss = eval_loss(model,val_loader)\n",
    "                val_losses.append(val_loss)\n",
    "                end = time.process_time()\n",
    "                acc = eval_acc(model,test_loader)\n",
    "                \n",
    "\n",
    "                if epoch % 30 == 0:\n",
    "                    print(f\"Eval Epoch: {epoch} |Val_loss:{val_loss:.03f}| Train_Loss: {train_loss:.3f} | Acc: {acc:.3f}|Fold: {fold}\")\n",
    "                accu.append(acc)\n",
    "                if(acc>max_acc):\n",
    "                    max_acc = acc\n",
    "                    #path=pathcheck+'/'+args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'main.pth.tar'\n",
    "                    main_model_path='main.pth'\n",
    "                    torch.save({'state_dict': model.state_dict()}, main_model_path)\n",
    "                if(acc>print_max_acc):\n",
    "                    print_max_acc = acc\n",
    "\n",
    "            t_end = time.perf_counter()\n",
    "            durations.append(t_end - t_start)      \n",
    "                    \n",
    "            # Start monitoring CPU and memory usage, model size, number of parametes, time inference and  power consumption\n",
    "            #main_model_path=pathcheck+'/'+args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'main.pth.tar'\n",
    "            main_model_path='main.pth'\n",
    "            #state = torch.load(main_model_path)\n",
    "            #dict=state['state_dict']\n",
    "            #recover_model = lambda: model.load_state_dict(state['state_dict'])\n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5)  # Add a 5-second delay to stabilize the initial state\n",
    "            tracemalloc.start()  # Start tracking memory allocations\n",
    "            snapshot_before = tracemalloc.take_snapshot()#take a snapshot of the current memory state before starting the measurement.\n",
    "\n",
    "            t0 = time.perf_counter()\n",
    "            initial_cpu_usage = get_cpu_usage()\n",
    "            power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "\n",
    "\n",
    "            fold_main_model_accuracy= eval_acc(model, test_loader)\n",
    "\n",
    "            fold_main_cpu_usage = get_cpu_usage()\n",
    "            t1 = time.perf_counter()\n",
    "            fold_t_main_model=t1-t0\n",
    "\n",
    "            snapshot_after = tracemalloc.take_snapshot()\n",
    "            tracemalloc.stop()\n",
    "            top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "            folde_main_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "            fold_main_energy_consumption = power_usage * fold_t_main_model\n",
    "            fold_main_model_size = os.path.getsize(main_model_path)\n",
    "            #fold_main_model_size =calculate_model_size(model, qypte )\n",
    "            fold_num_parm_main_model=get_num_parameters(model, count_nonzero_only=True)\n",
    "            \n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5) \n",
    "            #Update Eva dictionary\n",
    "            Eva_fold.update({'main model accuracy per fold': fold_main_model_accuracy,\n",
    "                        'time inference of main model per fold':fold_t_main_model,\n",
    "                        'number parmameters of main model per fold': fold_num_parm_main_model,\n",
    "                        'size of main model per fold': fold_main_model_size, \n",
    "                        'energy consumption of main model per fold':fold_main_energy_consumption,\n",
    "                        'total memory usage of main model per fold':folde_main_total_memory_diff,\n",
    "                        'cpu usage of main model per fold':fold_main_cpu_usage\n",
    "                       })\n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5) \n",
    "\n",
    "\n",
    "            main_model_accuracy.append(Eva_fold['main model accuracy per fold'])\n",
    "            t_main_model.append(Eva_fold['time inference of main model per fold'])\n",
    "            Num_parm_main_model.append(int(Eva_fold['number parmameters of main model per fold']))\n",
    "            main_model_size.append(int(Eva_fold['size of main model per fold']))\n",
    "            main_energy_consumption.append(Eva_fold['energy consumption of main model per fold'])\n",
    "            main_cpu_usage.append(Eva_fold['cpu usage of main model per fold'])\n",
    "            main_memory_usage.append(Eva_fold['total memory usage of main model per fold'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        Eva_iter[\"main model accuracy per iter\"]= stat.mean(main_model_accuracy)\n",
    "        Eva_iter[\"time inference of main model per iter\"]= stat.mean(t_main_model)\n",
    "        Eva_iter[\"number parmameters of main model per iter\"]=  stat.mean(Num_parm_main_model)\n",
    "        Eva_iter[\"size of main model per iter\"]= stat.mean(main_model_size)\n",
    "        Eva_iter[\"energy consumption of main model per iter\"]= stat.mean(main_energy_consumption)\n",
    "        Eva_iter[\"cpu usage of main model per iter\"]= stat.mean(main_cpu_usage)\n",
    "        Eva_iter[\"total memory usage of main model per iter\"]= stat.mean(main_memory_usage)\n",
    "\n",
    "\n",
    "        loss, acc, duration = tensor(val_losses), tensor(accu), tensor(durations)\n",
    "        loss, acc = loss.view(args.folds, max_epoch), acc.view(args.folds, max_epoch)\n",
    "        loss, argmin = loss.min(dim=1)\n",
    "        acc = acc[torch.arange(args.folds, dtype=torch.long), argmin]\n",
    "\n",
    "        Eva_iter[\"val losses per iter\"]= loss.mean().item()\n",
    "        Eva_iter[\"durations per iter\"]= duration.mean().item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return Eva_iter , model                                       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecff4141",
   "metadata": {},
   "source": [
    "## Manual Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a05caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are all list of criteria for measurements. \n",
    "# We collect all desired datas of each list across iterations. \n",
    "# Then, we compute average and std of each list.\n",
    "\n",
    "\n",
    "\n",
    "#Main model\n",
    "Main_val_loss=[]\n",
    "Main_duration=[]\n",
    "Main_model_accuracy=[]\n",
    "T_Main_model=[]\n",
    "Num_parm_Main_model=[]\n",
    "Main_model_size=[]\n",
    "Main_Energy_Consumption=[]\n",
    "Main_Cpu_Usage=[]\n",
    "Main_Memory_Usage=[]\n",
    "\n",
    "\n",
    "# Here is the dictionary to record the list of all measurements\n",
    "Eva_measure={'Main validation loss':Main_val_loss,\n",
    "             'Main duration':Main_duration,\n",
    "            'Main model accuracy': Main_model_accuracy,\n",
    "            'time inference of Main model':T_Main_model,\n",
    "            'number parmameters of Main model':Num_parm_Main_model,\n",
    "            'Main model size':Main_model_size,\n",
    "            'energy consumption of Main model':Main_Energy_Consumption,\n",
    "            'cpu usage of Main model':Main_Cpu_Usage,\n",
    "            'memory usage of Main model':Main_Memory_Usage}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dcae3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.max_epoch=100\n",
    "max_epoch = args.max_epoch\n",
    "iterations=4\n",
    "args.bit=32\n",
    "bit=args.bit\n",
    "folds=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab68daa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************\n",
      "The iteration is :1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Epoch: 0 |Val_loss:2.509| Train_Loss: 0.518 | Acc: 0.706|Fold: 0\n",
      "Eval Epoch: 30 |Val_loss:0.683| Train_Loss: 0.549 | Acc: 0.706|Fold: 0\n",
      "Eval Epoch: 60 |Val_loss:8.445| Train_Loss: 0.555 | Acc: 0.598|Fold: 0\n",
      "Eval Epoch: 90 |Val_loss:0.527| Train_Loss: 0.560 | Acc: 0.706|Fold: 0\n",
      "Eval Epoch: 0 |Val_loss:8.450| Train_Loss: 0.508 | Acc: 0.706|Fold: 1\n",
      "Eval Epoch: 30 |Val_loss:1.552| Train_Loss: 0.542 | Acc: 0.412|Fold: 1\n",
      "Eval Epoch: 60 |Val_loss:808037.744| Train_Loss: 0.553 | Acc: 0.672|Fold: 1\n",
      "Eval Epoch: 90 |Val_loss:1135981.980| Train_Loss: 0.553 | Acc: 0.672|Fold: 1\n",
      "Eval Epoch: 0 |Val_loss:4.576| Train_Loss: 0.489 | Acc: 0.706|Fold: 2\n",
      "Eval Epoch: 30 |Val_loss:711.379| Train_Loss: 0.551 | Acc: 0.706|Fold: 2\n",
      "Eval Epoch: 60 |Val_loss:0.543| Train_Loss: 0.553 | Acc: 0.706|Fold: 2\n",
      "Eval Epoch: 90 |Val_loss:464.267| Train_Loss: 0.553 | Acc: 0.706|Fold: 2\n",
      "Eval Epoch: 0 |Val_loss:7.385| Train_Loss: 0.516 | Acc: 0.706|Fold: 3\n",
      "Eval Epoch: 30 |Val_loss:0.529| Train_Loss: 0.553 | Acc: 0.706|Fold: 3\n",
      "Eval Epoch: 60 |Val_loss:73.191| Train_Loss: 0.545 | Acc: 0.637|Fold: 3\n",
      "Eval Epoch: 90 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 3\n",
      "Eval Epoch: 0 |Val_loss:4.223| Train_Loss: 0.500 | Acc: 0.706|Fold: 4\n",
      "Eval Epoch: 30 |Val_loss:3188277.176| Train_Loss: 0.544 | Acc: 0.706|Fold: 4\n",
      "Eval Epoch: 60 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 4\n",
      "Eval Epoch: 90 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 4\n",
      "Eval Epoch: 0 |Val_loss:4.165| Train_Loss: 0.498 | Acc: 0.706|Fold: 5\n",
      "Eval Epoch: 30 |Val_loss:0.607| Train_Loss: 0.549 | Acc: 0.598|Fold: 5\n",
      "Eval Epoch: 60 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 5\n",
      "Eval Epoch: 90 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 5\n",
      "Eval Epoch: 0 |Val_loss:3.305| Train_Loss: 0.488 | Acc: 0.706|Fold: 6\n",
      "Eval Epoch: 30 |Val_loss:29212.262| Train_Loss: 0.548 | Acc: 0.613|Fold: 6\n",
      "Eval Epoch: 60 |Val_loss:440.697| Train_Loss: 0.550 | Acc: 0.412|Fold: 6\n",
      "Eval Epoch: 90 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 6\n",
      "Eval Epoch: 0 |Val_loss:3.405| Train_Loss: 0.480 | Acc: 0.706|Fold: 7\n",
      "Eval Epoch: 30 |Val_loss:39847.904| Train_Loss: 0.541 | Acc: 0.706|Fold: 7\n",
      "Eval Epoch: 60 |Val_loss:348.284| Train_Loss: 0.544 | Acc: 0.559|Fold: 7\n",
      "Eval Epoch: 90 |Val_loss:264951.577| Train_Loss: 0.560 | Acc: 0.260|Fold: 7\n",
      "Eval Epoch: 0 |Val_loss:3.203| Train_Loss: 0.506 | Acc: 0.706|Fold: 8\n",
      "Eval Epoch: 30 |Val_loss:2.293| Train_Loss: 0.793 | Acc: 0.250|Fold: 8\n",
      "Eval Epoch: 60 |Val_loss:13808.718| Train_Loss: 0.553 | Acc: 0.691|Fold: 8\n",
      "Eval Epoch: 90 |Val_loss:13808.718| Train_Loss: 0.553 | Acc: 0.691|Fold: 8\n",
      "Eval Epoch: 0 |Val_loss:2.651| Train_Loss: 0.534 | Acc: 0.714|Fold: 9\n",
      "Eval Epoch: 30 |Val_loss:162764.066| Train_Loss: 0.559 | Acc: 0.714|Fold: 9\n",
      "Eval Epoch: 60 |Val_loss:0.528| Train_Loss: 0.561 | Acc: 0.714|Fold: 9\n",
      "Eval Epoch: 90 |Val_loss:20978212.050| Train_Loss: 0.558 | Acc: 0.714|Fold: 9\n",
      "********************************************\n",
      "The iteration is :2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Epoch: 0 |Val_loss:3.091| Train_Loss: 0.526 | Acc: 0.706|Fold: 0\n",
      "Eval Epoch: 30 |Val_loss:1.057| Train_Loss: 0.568 | Acc: 0.706|Fold: 0\n",
      "Eval Epoch: 60 |Val_loss:4.618| Train_Loss: 0.431 | Acc: 0.706|Fold: 0\n",
      "Eval Epoch: 90 |Val_loss:3.502| Train_Loss: 0.559 | Acc: 0.706|Fold: 0\n",
      "Eval Epoch: 0 |Val_loss:2.564| Train_Loss: 0.504 | Acc: 0.706|Fold: 1\n",
      "Eval Epoch: 30 |Val_loss:25477.706| Train_Loss: 0.514 | Acc: 0.706|Fold: 1\n",
      "Eval Epoch: 60 |Val_loss:782.211| Train_Loss: 0.548 | Acc: 0.706|Fold: 1\n",
      "Eval Epoch: 90 |Val_loss:6.133| Train_Loss: 0.548 | Acc: 0.706|Fold: 1\n",
      "Eval Epoch: 0 |Val_loss:3.344| Train_Loss: 0.508 | Acc: 0.706|Fold: 2\n",
      "Eval Epoch: 30 |Val_loss:1629790607127090.250| Train_Loss: 0.655 | Acc: 0.706|Fold: 2\n",
      "Eval Epoch: 60 |Val_loss:98.779| Train_Loss: 0.553 | Acc: 0.706|Fold: 2\n",
      "Eval Epoch: 90 |Val_loss:98.779| Train_Loss: 0.553 | Acc: 0.706|Fold: 2\n",
      "Eval Epoch: 0 |Val_loss:3.785| Train_Loss: 0.486 | Acc: 0.706|Fold: 3\n",
      "Eval Epoch: 30 |Val_loss:0.542| Train_Loss: 0.591 | Acc: 0.696|Fold: 3\n",
      "Eval Epoch: 60 |Val_loss:0.531| Train_Loss: 0.552 | Acc: 0.706|Fold: 3\n",
      "Eval Epoch: 90 |Val_loss:0.764| Train_Loss: 0.548 | Acc: 0.706|Fold: 3\n",
      "Eval Epoch: 0 |Val_loss:4.357| Train_Loss: 0.481 | Acc: 0.706|Fold: 4\n",
      "Eval Epoch: 30 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 4\n",
      "Eval Epoch: 60 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 4\n",
      "Eval Epoch: 90 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 4\n",
      "Eval Epoch: 0 |Val_loss:3.212| Train_Loss: 0.484 | Acc: 0.706|Fold: 5\n",
      "Eval Epoch: 30 |Val_loss:0.530| Train_Loss: 0.552 | Acc: 0.706|Fold: 5\n",
      "Eval Epoch: 60 |Val_loss:55562.751| Train_Loss: 1.606 | Acc: 0.265|Fold: 5\n",
      "Eval Epoch: 90 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 5\n",
      "Eval Epoch: 0 |Val_loss:4.328| Train_Loss: 0.488 | Acc: 0.706|Fold: 6\n",
      "Eval Epoch: 30 |Val_loss:0.527| Train_Loss: 0.552 | Acc: 0.706|Fold: 6\n",
      "Eval Epoch: 60 |Val_loss:0.557| Train_Loss: 0.548 | Acc: 0.706|Fold: 6\n",
      "Eval Epoch: 90 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 6\n",
      "Eval Epoch: 0 |Val_loss:5.184| Train_Loss: 0.508 | Acc: 0.706|Fold: 7\n",
      "Eval Epoch: 30 |Val_loss:2.668| Train_Loss: 0.525 | Acc: 0.706|Fold: 7\n",
      "Eval Epoch: 60 |Val_loss:121.448| Train_Loss: 0.557 | Acc: 0.706|Fold: 7\n",
      "Eval Epoch: 90 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 7\n",
      "Eval Epoch: 0 |Val_loss:4.079| Train_Loss: 0.487 | Acc: 0.706|Fold: 8\n",
      "Eval Epoch: 30 |Val_loss:92281419940161.250| Train_Loss: 0.558 | Acc: 0.706|Fold: 8\n",
      "Eval Epoch: 60 |Val_loss:0.542| Train_Loss: 0.550 | Acc: 0.706|Fold: 8\n",
      "Eval Epoch: 90 |Val_loss:0.531| Train_Loss: 0.553 | Acc: 0.706|Fold: 8\n",
      "Eval Epoch: 0 |Val_loss:3.104| Train_Loss: 0.529 | Acc: 0.714|Fold: 9\n",
      "Eval Epoch: 30 |Val_loss:70144445153038416.000| Train_Loss: 0.553 | Acc: 0.266|Fold: 9\n",
      "Eval Epoch: 60 |Val_loss:0.531| Train_Loss: 0.559 | Acc: 0.714|Fold: 9\n",
      "Eval Epoch: 90 |Val_loss:0.531| Train_Loss: 0.559 | Acc: 0.714|Fold: 9\n",
      "********************************************\n",
      "The iteration is :3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Epoch: 0 |Val_loss:3.999| Train_Loss: 0.503 | Acc: 0.706|Fold: 0\n",
      "Eval Epoch: 30 |Val_loss:4755210.877| Train_Loss: 0.553 | Acc: 0.235|Fold: 0\n",
      "Eval Epoch: 60 |Val_loss:0.571| Train_Loss: 0.555 | Acc: 0.706|Fold: 0\n",
      "Eval Epoch: 90 |Val_loss:0.526| Train_Loss: 0.556 | Acc: 0.706|Fold: 0\n",
      "Eval Epoch: 0 |Val_loss:3.299| Train_Loss: 0.504 | Acc: 0.706|Fold: 1\n",
      "Eval Epoch: 30 |Val_loss:0.532| Train_Loss: 0.565 | Acc: 0.706|Fold: 1\n",
      "Eval Epoch: 60 |Val_loss:2178387.961| Train_Loss: 0.546 | Acc: 0.250|Fold: 1\n",
      "Eval Epoch: 90 |Val_loss:0.571| Train_Loss: 0.553 | Acc: 0.701|Fold: 1\n",
      "Eval Epoch: 0 |Val_loss:2.226| Train_Loss: 0.530 | Acc: 0.706|Fold: 2\n",
      "Eval Epoch: 30 |Val_loss:154.430| Train_Loss: 0.557 | Acc: 0.250|Fold: 2\n",
      "Eval Epoch: 60 |Val_loss:0.989| Train_Loss: 0.553 | Acc: 0.686|Fold: 2\n",
      "Eval Epoch: 90 |Val_loss:0.989| Train_Loss: 0.553 | Acc: 0.686|Fold: 2\n",
      "Eval Epoch: 0 |Val_loss:3.362| Train_Loss: 0.516 | Acc: 0.706|Fold: 3\n",
      "Eval Epoch: 30 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 3\n",
      "Eval Epoch: 60 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 3\n",
      "Eval Epoch: 90 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 3\n",
      "Eval Epoch: 0 |Val_loss:3.553| Train_Loss: 0.503 | Acc: 0.706|Fold: 4\n",
      "Eval Epoch: 30 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 4\n",
      "Eval Epoch: 60 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 4\n",
      "Eval Epoch: 90 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 4\n",
      "Eval Epoch: 0 |Val_loss:4.457| Train_Loss: 0.512 | Acc: 0.706|Fold: 5\n",
      "Eval Epoch: 30 |Val_loss:754.614| Train_Loss: 0.542 | Acc: 0.289|Fold: 5\n",
      "Eval Epoch: 60 |Val_loss:0.583| Train_Loss: 0.895 | Acc: 0.706|Fold: 5\n",
      "Eval Epoch: 90 |Val_loss:0.527| Train_Loss: 0.553 | Acc: 0.691|Fold: 5\n",
      "Eval Epoch: 0 |Val_loss:4.271| Train_Loss: 0.516 | Acc: 0.706|Fold: 6\n",
      "Eval Epoch: 30 |Val_loss:0.557| Train_Loss: 0.551 | Acc: 0.691|Fold: 6\n",
      "Eval Epoch: 60 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 6\n",
      "Eval Epoch: 90 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 6\n",
      "Eval Epoch: 0 |Val_loss:2.866| Train_Loss: 0.517 | Acc: 0.706|Fold: 7\n",
      "Eval Epoch: 30 |Val_loss:0.532| Train_Loss: 0.555 | Acc: 0.706|Fold: 7\n",
      "Eval Epoch: 60 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.711|Fold: 7\n",
      "Eval Epoch: 90 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.711|Fold: 7\n",
      "Eval Epoch: 0 |Val_loss:5.065| Train_Loss: 0.495 | Acc: 0.706|Fold: 8\n",
      "Eval Epoch: 30 |Val_loss:0.528| Train_Loss: 0.551 | Acc: 0.706|Fold: 8\n",
      "Eval Epoch: 60 |Val_loss:0.529| Train_Loss: 0.553 | Acc: 0.706|Fold: 8\n",
      "Eval Epoch: 90 |Val_loss:0.586| Train_Loss: 0.546 | Acc: 0.706|Fold: 8\n",
      "Eval Epoch: 0 |Val_loss:11.297| Train_Loss: 0.512 | Acc: 0.714|Fold: 9\n",
      "Eval Epoch: 30 |Val_loss:0.629| Train_Loss: 0.553 | Acc: 0.714|Fold: 9\n",
      "Eval Epoch: 60 |Val_loss:2.145| Train_Loss: 0.558 | Acc: 0.709|Fold: 9\n",
      "Eval Epoch: 90 |Val_loss:5.014| Train_Loss: 0.446 | Acc: 0.714|Fold: 9\n",
      "********************************************\n",
      "The iteration is :4 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Epoch: 0 |Val_loss:2.545| Train_Loss: 0.502 | Acc: 0.706|Fold: 0\n",
      "Eval Epoch: 30 |Val_loss:34.888| Train_Loss: 0.673 | Acc: 0.230|Fold: 0\n",
      "Eval Epoch: 60 |Val_loss:0.527| Train_Loss: 0.560 | Acc: 0.706|Fold: 0\n",
      "Eval Epoch: 90 |Val_loss:0.527| Train_Loss: 0.560 | Acc: 0.706|Fold: 0\n",
      "Eval Epoch: 0 |Val_loss:3.729| Train_Loss: 0.510 | Acc: 0.706|Fold: 1\n",
      "Eval Epoch: 30 |Val_loss:3397930648242744320.000| Train_Loss: 0.548 | Acc: 0.706|Fold: 1\n",
      "Eval Epoch: 60 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 1\n",
      "Eval Epoch: 90 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 1\n",
      "Eval Epoch: 0 |Val_loss:2.761| Train_Loss: 0.506 | Acc: 0.706|Fold: 2\n",
      "Eval Epoch: 30 |Val_loss:46792213.333| Train_Loss: 0.558 | Acc: 0.706|Fold: 2\n",
      "Eval Epoch: 60 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 2\n",
      "Eval Epoch: 90 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 2\n",
      "Eval Epoch: 0 |Val_loss:3.364| Train_Loss: 0.490 | Acc: 0.706|Fold: 3\n",
      "Eval Epoch: 30 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.701|Fold: 3\n",
      "Eval Epoch: 60 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.701|Fold: 3\n",
      "Eval Epoch: 90 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.701|Fold: 3\n",
      "Eval Epoch: 0 |Val_loss:3.431| Train_Loss: 0.486 | Acc: 0.706|Fold: 4\n",
      "Eval Epoch: 30 |Val_loss:255330.985| Train_Loss: 0.554 | Acc: 0.706|Fold: 4\n",
      "Eval Epoch: 60 |Val_loss:75996.682| Train_Loss: 0.552 | Acc: 0.706|Fold: 4\n",
      "Eval Epoch: 90 |Val_loss:0.858| Train_Loss: 0.553 | Acc: 0.706|Fold: 4\n",
      "Eval Epoch: 0 |Val_loss:3.631| Train_Loss: 0.483 | Acc: 0.706|Fold: 5\n",
      "Eval Epoch: 30 |Val_loss:0.524| Train_Loss: 0.555 | Acc: 0.706|Fold: 5\n",
      "Eval Epoch: 60 |Val_loss:1886486.657| Train_Loss: 0.552 | Acc: 0.706|Fold: 5\n",
      "Eval Epoch: 90 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 5\n",
      "Eval Epoch: 0 |Val_loss:2.879| Train_Loss: 0.493 | Acc: 0.706|Fold: 6\n",
      "Eval Epoch: 30 |Val_loss:37.847| Train_Loss: 0.555 | Acc: 0.706|Fold: 6\n",
      "Eval Epoch: 60 |Val_loss:20480327680.000| Train_Loss: 0.548 | Acc: 0.706|Fold: 6\n",
      "Eval Epoch: 90 |Val_loss:0.530| Train_Loss: 0.553 | Acc: 0.706|Fold: 6\n",
      "Eval Epoch: 0 |Val_loss:3.477| Train_Loss: 0.521 | Acc: 0.706|Fold: 7\n",
      "Eval Epoch: 30 |Val_loss:0.671| Train_Loss: 0.549 | Acc: 0.324|Fold: 7\n",
      "Eval Epoch: 60 |Val_loss:0.532| Train_Loss: 0.554 | Acc: 0.706|Fold: 7\n",
      "Eval Epoch: 90 |Val_loss:789.593| Train_Loss: 0.553 | Acc: 0.706|Fold: 7\n",
      "Eval Epoch: 0 |Val_loss:2.497| Train_Loss: 0.479 | Acc: 0.706|Fold: 8\n",
      "Eval Epoch: 30 |Val_loss:1104782466.510| Train_Loss: 0.554 | Acc: 0.284|Fold: 8\n",
      "Eval Epoch: 60 |Val_loss:1287.845| Train_Loss: 0.571 | Acc: 0.701|Fold: 8\n",
      "Eval Epoch: 90 |Val_loss:12556681974161.930| Train_Loss: 0.553 | Acc: 0.691|Fold: 8\n",
      "Eval Epoch: 0 |Val_loss:3.487| Train_Loss: 0.511 | Acc: 0.714|Fold: 9\n",
      "Eval Epoch: 30 |Val_loss:5143.375| Train_Loss: 0.529 | Acc: 0.345|Fold: 9\n",
      "Eval Epoch: 60 |Val_loss:0.539| Train_Loss: 0.554 | Acc: 0.714|Fold: 9\n",
      "Eval Epoch: 90 |Val_loss:0.531| Train_Loss: 0.559 | Acc: 0.714|Fold: 9\n"
     ]
    }
   ],
   "source": [
    "#### load the Mainized  model\n",
    "\n",
    "for i in range(iterations):\n",
    "    print('********************************************')\n",
    "    print(f'The iteration is :{i+1} ')\n",
    "   \n",
    " \n",
    "\n",
    "    \n",
    "    Eva_iter, model=run(bit, max_epoch)\n",
    "\n",
    " \n",
    "    Main_val_loss.append(Eva_iter[\"val losses per iter\"])\n",
    "    Main_duration.append(Eva_iter[\"durations per iter\"])\n",
    "    Main_model_accuracy.append(Eva_iter[\"main model accuracy per iter\"])\n",
    "    T_Main_model.append(Eva_iter[\"time inference of main model per iter\"])\n",
    "    Num_parm_Main_model.append(Eva_iter[\"number parmameters of main model per iter\"])\n",
    "    Main_model_size.append(Eva_iter[\"size of main model per iter\"])\n",
    "    Main_Energy_Consumption.append(Eva_iter[\"energy consumption of main model per iter\"])\n",
    "    Main_Cpu_Usage.append( Eva_iter[\"cpu usage of main model per iter\"])\n",
    "    Main_Memory_Usage.append(Eva_iter[\"total memory usage of main model per iter\"])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40b2e228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7042717086834734"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eva_iter[\"main model accuracy per iter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "686e750d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6807422969187675,\n",
       " 0.6978991596638655,\n",
       " 0.5876050420168067,\n",
       " 0.7057422969187676,\n",
       " 0.7042717086834734]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Main_model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc44fe2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6807422969187675,\n",
       " 0.6978991596638655,\n",
       " 0.5876050420168067,\n",
       " 0.7057422969187676,\n",
       " 0.7042717086834734]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.6807422969187675,\n",
    " 0.6978991596638655,\n",
    " 0.5876050420168067,\n",
    " 0.7057422969187676,\n",
    " 0.7042717086834734]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5cc3d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All measurement about Main model of type:32 \n",
      "main_model_val_loss - 0.519  0.002\n",
      "main model duration - 418.954  112.761\n",
      "quant model accuracy - 0.675  0.050\n",
      "time inference of main model- 0.675  0.050\n",
      "num parm main model: 43655.000  0.000\n",
      "main modelsize: 199749.000  0.000\n",
      "main energy consumption: 73.138  46.588\n",
      "main cpu usage: 39.410  27.563\n",
      "main memory usage: 45707.060  199.006\n"
     ]
    }
   ],
   "source": [
    "# This is a dictionary to save all measurements. Aftre measuring, we can compute mean and std of each item.\n",
    "from collections import OrderedDict \n",
    "Eva_final = OrderedDict()\n",
    "\n",
    "print(f\"All measurement about Main model of type:{ bit} \")   \n",
    "\n",
    "main_model_val_loss_mean =stat.mean(Main_val_loss)\n",
    "main_model_val_loss_std = stat.stdev(Main_val_loss)\n",
    "main_model_val_loss = \"{:.3f}  {:.3f}\".format(main_model_val_loss_mean,main_model_val_loss_std)\n",
    "print(\"main_model_val_loss - {}\".format(main_model_val_loss))\n",
    "\n",
    "\n",
    "Eva_final.update({'Ave of main loss validation':float(format(main_model_val_loss_mean, '.3f'))})\n",
    "Eva_final.update({'Std of main loss validation':float(format(main_model_val_loss_std, '.3f'))}) \n",
    "\n",
    "###########\n",
    "\n",
    "main_model_duration_mean =stat.mean(Main_duration)\n",
    "main_model_duration_std = stat.stdev(Main_duration)\n",
    "main_model_duration = \"{:.3f}  {:.3f}\".format(main_model_duration_mean,main_model_duration_std )\n",
    "print(\"main model duration - {}\".format(main_model_duration))\n",
    "\n",
    "Eva_final.update({'Ave of main model duration':float(format(main_model_duration_mean , '.3f'))})\n",
    "Eva_final.update({'Std of main model duration':float(format(main_model_duration_std, '.3f'))})                                         \n",
    "                                     \n",
    "#############\n",
    "main_model_accuracy_mean =stat.mean(Main_model_accuracy)\n",
    "main_model_accuracy_std = stat.stdev(Main_model_accuracy)\n",
    "main_model_accuracy = \"{:.3f}  {:.3f}\".format(main_model_accuracy_mean,main_model_accuracy_std )\n",
    "print(\"quant model accuracy - {}\".format(main_model_accuracy))\n",
    "\n",
    "Eva_final.update({'Ave of main model accuracy':float(format(main_model_accuracy_mean, '.3f'))})\n",
    "Eva_final.update({'Std of main model accuracy':float(format(main_model_accuracy_std, '.3f'))})\n",
    "                 \n",
    "######################\n",
    "t_main_model_mean = stat.mean(T_Main_model)\n",
    "t_main_model_std =stat.stdev(T_Main_model)\n",
    "t_main_model = \"{:.3f}  {:.3f}\".format(main_model_accuracy_mean,main_model_accuracy_std )\n",
    "print(\"time inference of main model- {}\".format(t_main_model))\n",
    "      \n",
    "Eva_final.update({'Ave of time inference of main model':float(format(t_main_model_mean, '.3f'))})\n",
    "Eva_final.update({'Std of time inference of main model':float(format(t_main_model_std, '.3f'))})\n",
    "\n",
    "##############\n",
    "num_parm_main_model_mean = stat.mean(Num_parm_Main_model)\n",
    "num_parm_main_model_std = stat.stdev(Num_parm_Main_model)\n",
    "num_parm_main_model= \"{:.3f}  {:.3f}\".format(num_parm_main_model_mean, num_parm_main_model_std)\n",
    "print(\"num parm main model: {}\".format(num_parm_main_model))\n",
    "\n",
    "\n",
    "Eva_final.update({'Ave of number parmameters of main model':num_parm_main_model_mean})\n",
    "Eva_final.update({'Std of number parmameters of main model':num_parm_main_model_std})\n",
    "##########################\n",
    "main_model_size_mean =stat.mean( Main_model_size)\n",
    "main_model_size_std = stat.stdev(Main_model_size)\n",
    "main_model_size= \"{:.3f}  {:.3f}\".format(main_model_size_mean, main_model_size_std)\n",
    "print(\"main modelsize: {}\".format(main_model_size))\n",
    "\n",
    "\n",
    "Eva_final.update({'Ave of main model size':main_model_size_mean})\n",
    "Eva_final.update({'Std of main_model_size':main_model_size_std })\n",
    "\n",
    "###########################\n",
    "main_energy_consumption_mean = stat.mean(Main_Energy_Consumption)\n",
    "main_energy_consumption_std = stat.stdev(Main_Energy_Consumption)\n",
    "main_energy_consumption= \"{:.3f}  {:.3f}\".format(main_energy_consumption_mean, main_energy_consumption_std)\n",
    "print(\"main energy consumption: {}\".format(main_energy_consumption))\n",
    "\n",
    "\n",
    "Eva_final.update({'Ave of energy consumption of main model':main_energy_consumption_mean })\n",
    "Eva_final.update({'Std of energy consumption of main model':main_energy_consumption_std})\n",
    "\n",
    "###############################\n",
    "main_cpu_usage_mean = stat.mean(Main_Cpu_Usage)\n",
    "main_cpu_usage_std = stat.stdev(Main_Cpu_Usage)\n",
    "main_cpu_usage= \"{:.3f}  {:.3f}\".format(main_cpu_usage_mean, main_cpu_usage_std )\n",
    "print(\"main cpu usage: {}\".format(main_cpu_usage))\n",
    "\n",
    "Eva_final.update({'Ave of cpu usage of main model':main_cpu_usage_mean})\n",
    "Eva_final.update({'Std of cpu usage of main model':main_cpu_usage_std})\n",
    "\n",
    "\n",
    "###############\n",
    "main_memory_usage_mean = stat.mean(Main_Memory_Usage)\n",
    "main_memory_usage_std = stat.stdev(Main_Memory_Usage)\n",
    "main_memory_usage= \"{:.3f}  {:.3f}\".format(main_memory_usage_mean, main_memory_usage_std)\n",
    "print(\"main memory usage: {}\".format(main_memory_usage))\n",
    "\n",
    "Eva_final.update({'Ave of memory usage of main model':main_memory_usage_mean})\n",
    "Eva_final.update({'Std of memory usage of main model':main_memory_usage_std})\n",
    "\n",
    "#################################\n",
    "\n",
    "\n",
    "# Determing Mainization Method \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64426391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the accuracy\n",
    "\n",
    "mode='FP32'\n",
    "file_name = pathresult+'/'+args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'_with_Mode_'+mode+'.txt'\n",
    "\n",
    "with open(file_name, 'w') as f:\n",
    "    for key, value in vars(args).items():\n",
    "        f.write('%s:%s\\n'%(key, value))\n",
    "\n",
    "    for key, value in Eva_final.items():\n",
    "        f.write('%s:%s\\n'%(key, value))\n",
    "\n",
    "    for key, value in Eva_measure.items():\n",
    "        f.write('%s:%s\\n' % (key, ','.join(map(str, value))))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b171f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561daff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
