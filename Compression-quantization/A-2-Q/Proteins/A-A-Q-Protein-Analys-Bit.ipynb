{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d7bb9e1",
   "metadata": {},
   "source": [
    "$A^2Q$ Quantization method of Proteins dataset Training by GIN Models\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c49f177",
   "metadata": {},
   "source": [
    "## Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8494b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import statistics as stat\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import argparse\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "\n",
    "# CPU and Enegusage \n",
    "import psutil\n",
    "import itertools\n",
    "import tracemalloc\n",
    "import gc\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import tensor\n",
    "from torch.nn import Linear, Sequential, ReLU, Identity, BatchNorm1d as BN\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree,remove_self_loops\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.data import Data\n",
    "from torch.autograd.function import InplaceFunction\n",
    "from torch_geometric.nn import GCNConv,GINConv,global_mean_pool,TopKPooling\n",
    "\n",
    "# For downloading Proteins dataset from TUDataset and Transformation\n",
    "from torch_geometric.datasets import TUDataset,Planetoid,GNNBenchmarkDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "# AAQ Quantization\n",
    "from quantize_function.u_quant_gc_bit_debug import *\n",
    "from quantize_function.MessagePassing_gc_bit import GINConvMultiQuant\n",
    "from quantize_function.get_scale_index import get_deg_index, get_scale_index\n",
    "from utils.quant_utils import analysis_bit\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1442c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For downloading BBBP dataset from MoleculeNet and Transformation\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch.utils.data import random_split, Subset\n",
    "from torch_geometric.data import Data, InMemoryDataset, DataLoader\n",
    "import torch_geometric.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a871dbe8",
   "metadata": {},
   "source": [
    "### Functions for Mmeasuring criterias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dca0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_parameters(model: nn.Module, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the total number of parameters of model\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    num_counted_elements = 0\n",
    "    for param in model.parameters():\n",
    "        if count_nonzero_only:\n",
    "            num_counted_elements += param.count_nonzero()\n",
    "        else:\n",
    "            num_counted_elements += param.numel()\n",
    "    return num_counted_elements\n",
    "\n",
    "# Function to get CPU usage\n",
    "def get_cpu_usage():\n",
    "    return psutil.cpu_percent(interval=1)\n",
    "\n",
    "\n",
    "\n",
    "# Function to approximate power consumption (Assume some average power usage per CPU percentage point)\n",
    "def estimate_power_usage(cpu_usage):\n",
    "    base_power_usage = 10  # Assumed base power usage in watts\n",
    "    power_per_percent = 0.5  # Assumed additional watts per CPU usage percent\n",
    "    return base_power_usage + (power_per_percent * cpu_usage)\n",
    "\n",
    "# The model size based on the number of parameters\n",
    "def calculate_model_size(model: nn.Module, \n",
    "                         qypte: str = 'fp32', \n",
    "                         include_metadata: bool = False,\n",
    "                         model_path: str = None) -> float:\n",
    "    \"\"\"\n",
    "    Calculate model size in KB/MB for different precisions.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        precision: 'fp32' (32-bit float) or 'int4' (4-bit integer)\n",
    "        include_metadata: Whether to include PyTorch metadata in size calculation\n",
    "        model_path: If provided, will check actual file size on disk\n",
    "        \n",
    "    Returns:\n",
    "        Size in KB (if include_metadata=False) or actual file size (if include_metadata=True)\n",
    "    \"\"\"\n",
    "    # Get total number of parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Calculate theoretical size\n",
    "    if qypte == 'FP32':\n",
    "        size_bits = total_params * 32\n",
    "    elif qypte== 'INT4':\n",
    "        size_bits = total_params * 4\n",
    "    elif qypte == 'INT8':\n",
    "        size_bits = total_params * 8    \n",
    "   \n",
    "    \n",
    "    size_bytes = size_bits / 8\n",
    "    size_kb = size_bytes / 1024\n",
    "    \n",
    "    # If checking actual file size\n",
    "    if include_metadata and model_path:\n",
    "        if not os.path.exists(model_path):\n",
    "            # Save model to temporary file if path doesn't exist\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        actual_size_kb = os.path.getsize(model_path) / 1024\n",
    "        return actual_size_kb\n",
    "    \n",
    "    return size_kb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc4e84f",
   "metadata": {
    "id": "aec599ef"
   },
   "source": [
    "## Function for Quatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28fc1163",
   "metadata": {
    "id": "28fc1163"
   },
   "outputs": [],
   "source": [
    "def paras_group(model):\n",
    "    all_params = model.parameters()\n",
    "    weight_paras=[]\n",
    "    quant_paras_bit_weight = []\n",
    "    quant_paras_bit_fea = []\n",
    "    quant_paras_scale_weight = []\n",
    "    quant_paras_scale_fea = []\n",
    "    quant_paras_scale_xw = []\n",
    "    quant_paras_bit_xw = []\n",
    "    other_paras = []\n",
    "    for name,para in model.named_parameters():        \n",
    "        if('quant' in name and 'bit' in name and 'weight' in name):\n",
    "            quant_paras_bit_weight+=[para]\n",
    "            # para.requires_grad = False\n",
    "        elif('quant' in name and 'bit' in name and 'fea' in name):\n",
    "            quant_paras_bit_fea+=[para]\n",
    "        elif('quant' in name and 'bit' not in name and 'weight' in name):\n",
    "            quant_paras_scale_weight+=[para]\n",
    "            # para.requires_grad = False\n",
    "        elif('quant' in name and 'bit' not in name and 'fea' in name):\n",
    "            quant_paras_scale_fea+=[para]\n",
    "        elif('xw'in name and 'q' in name and 'bit' not in name):\n",
    "            quant_paras_scale_xw+=[para]\n",
    "        elif('xw'in name and 'q' in name and 'bit' in name):\n",
    "            quant_paras_bit_xw+=[para]\n",
    "        elif('weight' in name and 'quant' not in name ):\n",
    "            weight_paras+=[para]\n",
    "    params_id = list(map(id,quant_paras_bit_fea))+list(map(id,quant_paras_bit_weight))+list(map(id,quant_paras_scale_weight))+list(map(id,quant_paras_scale_fea))+list(map(id,weight_paras))\\\n",
    "    +list(map(id,quant_paras_scale_xw))+list(map(id,quant_paras_bit_xw))\n",
    "    other_paras = list(filter(lambda p: id(p) not in params_id, all_params))\n",
    "    return weight_paras,quant_paras_bit_weight,quant_paras_bit_fea,quant_paras_scale_weight,quant_paras_scale_fea,quant_paras_scale_xw,quant_paras_bit_xw,other_paras\n",
    "\n",
    "def setup_seed(seed):\n",
    "      torch.manual_seed(seed)\n",
    "      torch.cuda.manual_seed_all(seed)\n",
    "      np.random.seed(seed)\n",
    "      random.seed(seed)\n",
    "    #  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def parameter_stastic(model,dataset,hidden_units):\n",
    "    w_Byte = 0\n",
    "    a_Byte = 0\n",
    "    for name, par in model.named_parameters():\n",
    "        if(('bit' in name)&('weight' in name)):\n",
    "            if('conv1' in name):\n",
    "                scale = dataset.num_node_features\n",
    "            else:\n",
    "                scale = hidden_units\n",
    "            par = torch.floor(par)\n",
    "            w_Byte = scale*par.sum()/8./1024.+w_Byte\n",
    "        elif(('bit' in name)&('fea' in name)):\n",
    "            if('conv1' in name):\n",
    "                a_scale = 0\n",
    "            else:\n",
    "                a_scale = hidden_units\n",
    "            # a_scale = dataset.data.num_nodes\n",
    "            par = torch.floor(par)\n",
    "            a_Byte = a_scale*par.sum()/8./1024.+a_Byte\n",
    "    return w_Byte, a_Byte\n",
    "\n",
    "class ResettableSequential(nn.Sequential):\n",
    "    def reset_parameters(self):\n",
    "        for child in self.children():\n",
    "            if hasattr(child, \"reset_parameters\"):\n",
    "                child.reset_parameters()\n",
    "    def forward(self,input,edge_index,bit_sum):\n",
    "        for model in self:\n",
    "            input,_,bit_sum = model(input,edge_index,bit_sum)\n",
    "        return input,bit_sum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56aeb4a8",
   "metadata": {
    "id": "56aeb4a8"
   },
   "outputs": [],
   "source": [
    "# Relu and Batch Normalization\n",
    "class relu(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "    def forward(self,x,edge_index,bit_sum):\n",
    "        x[x<0] = 0\n",
    "        return x,edge_index,bit_sum\n",
    "\n",
    "class bn(nn.Module):\n",
    "    def __init__(self,hidden_units):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(hidden_units)\n",
    "    def forward(self,x,edge_index,bit_sum):\n",
    "        x = self.bn(x)\n",
    "        return x,edge_index,bit_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9879a7b5",
   "metadata": {},
   "source": [
    "## qGIN Model with Quatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07e32186",
   "metadata": {
    "id": "07e32186"
   },
   "outputs": [],
   "source": [
    "\n",
    "class qGIN(nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden_units, bit, num_deg=1000, is_q=True,\n",
    "                    uniform=False,init='norm'):\n",
    "        super(qGIN, self).__init__()\n",
    "        gin_layer = GINConvMultiQuant\n",
    "        self.bit = bit\n",
    "        para_list=[[{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'gama_init':0.70,'gama_std':0.1}],\n",
    "                   [{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'gama_init':0.6,'gama_std':0.7}],\n",
    "                   [{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'gama_init':0.76,'gama_std':0.68}],\n",
    "                   [{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'gama_init':0.6,'gama_std':0.5}],\n",
    "                   [{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'gama_init':0.6,'gama_std':0.3}],\n",
    "                   [{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1}],\n",
    "                   [{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1}]]\n",
    "        if(is_q):\n",
    "            # As the DQ, we either don't quantize the input features of the REDDIT-BINARY dataset because the feature is only 1-dimension.\n",
    "            self.conv1 = gin_layer(\n",
    "                ResettableSequential(\n",
    "                    QLinear(dataset.num_features,hidden_units, num_deg, bit,para_dict=para_list[0][0], all_positive=True,\n",
    "                            quant_fea=True,\n",
    "                            uniform=uniform,init=init),\n",
    "                    relu(),\n",
    "                    QLinear(hidden_units, hidden_units, num_deg, bit, para_dict=para_list[0][1],all_positive=True,\n",
    "                            uniform=uniform,init=init),\n",
    "                    relu(),\n",
    "                ),\n",
    "                train_eps=True,\n",
    "                in_features=num_deg, out_features=1,\n",
    "                bit=bit, para_dict=para_list[0][2],quant_fea=True,uniform=uniform\n",
    "            )\n",
    "        else:\n",
    "            self.conv1 = GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(dataset.num_features, hidden_units),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_units, hidden_units),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm1d(hidden_units),\n",
    "                ),\n",
    "                train_eps=True,\n",
    "            )\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            if(is_q):\n",
    "                self.convs.append(\n",
    "                    gin_layer(\n",
    "                        ResettableSequential(\n",
    "                            QLinear(hidden_units, hidden_units, num_deg,bit, para_dict=para_list[0][0],all_positive=False,\n",
    "                                    uniform=uniform,init=init),\n",
    "                            relu(),\n",
    "                            QLinear(hidden_units, hidden_units, num_deg,bit, para_dict=para_list[0][1], all_positive=True,\n",
    "                                    uniform=uniform,init=init),\n",
    "                            relu(),\n",
    "                        ),\n",
    "                        train_eps=True,\n",
    "                        in_features=num_deg, out_features=hidden_units,\n",
    "                        bit=bit, para_dict=para_list[0][2], uniform=uniform,quant_fea=True\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                self.convs.append(\n",
    "                    GINConv(\n",
    "                        nn.Sequential(\n",
    "                            nn.Linear(hidden_units, hidden_units),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(hidden_units, hidden_units),\n",
    "                            nn.ReLU(),\n",
    "                            nn.BatchNorm1d(hidden_units),\n",
    "                        ),\n",
    "                        train_eps=True,\n",
    "                    )\n",
    "                )\n",
    "        self.bn_list = torch.nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.bn_list.append(nn.BatchNorm1d(hidden_units))\n",
    "        if(is_q):\n",
    "            self.lin1 = QLinear(hidden_units, hidden_units, num_deg, bit, para_dict=para_list[-1][0], all_positive=False,\n",
    "                                        uniform=uniform,init=init)\n",
    "            self.lin2 = QLinear(hidden_units, dataset.num_classes, num_deg, bit, para_dict=para_list[-1][0], all_positive=True,\n",
    "                                        uniform=uniform,init=init)\n",
    "        else:\n",
    "            self.lin1 = nn.Linear(hidden_units, hidden_units)\n",
    "            self.lin2 = nn.Linear(hidden_units, dataset.num_classes)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        bit_sum=x.new_zeros(1)\n",
    "        x,bit_sum = self.conv1(x, edge_index,bit_sum)\n",
    " \n",
    "        x = self.bn_list[0](x)\n",
    "        # x,_,bit_sum = self.embeding(x,edge_index,bit_sum)\n",
    "        # x = F.relu(x)\n",
    "        i = 1\n",
    "        for conv in self.convs:\n",
    "            x,bit_sum = conv(x,edge_index,bit_sum)\n",
    "            x = self.bn_list[i](x)\n",
    "            i=i+1\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        x,_,bit_sum = self.lin1(x,edge_index,bit_sum)\n",
    " \n",
    "        x = F.relu(x)\n",
    "        x,_,bit_sum = self.lin2(x,edge_index,bit_sum)\n",
    "    \n",
    "        return F.log_softmax(x, dim=-1),bit_sum\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35505c3",
   "metadata": {},
   "source": [
    "# Helpful Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c8795f2",
   "metadata": {
    "id": "2c8795f2"
   },
   "outputs": [],
   "source": [
    "class NormalizedDegree(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, data):\n",
    "        deg = degree(data.edge_index[0], dtype=torch.float)\n",
    "        deg = (deg - self.mean) / self.std\n",
    "        data.x = deg.view(-1, 1)\n",
    "        return data\n",
    "\n",
    "\n",
    "def num_graphs(data):\n",
    "    if data.batch is not None:\n",
    "        return data.num_graphs\n",
    "    else:\n",
    "        return data.x.size(0)\n",
    "\n",
    "\n",
    "def train(model, optimizer, loader,a_loss, a_storage=1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        out,bit_sum = model(data)\n",
    "        loss = F.cross_entropy(out, data.y.view(-1))\n",
    "        loss_store = a_loss*F.relu(bit_sum-a_storage)**2\n",
    "        loss_store.backward(retain_graph=True)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * num_graphs(data)\n",
    "        optimizer.step()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_acc(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(data)[0].max(1)[1]\n",
    "        correct += pred.eq(data.y.view(-1)).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_loss(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)[0]\n",
    "        loss += F.cross_entropy(out, data.y.view(-1), reduction=\"sum\").item()\n",
    "    return loss / len(loader.dataset)\n",
    "\n",
    "def k_fold(dataset, folds):\n",
    "    skf = StratifiedKFold(folds, shuffle=True, random_state=12345)\n",
    "\n",
    "    test_indices, train_indices = [], []\n",
    "    for _, idx in skf.split(torch.zeros(len(dataset)), dataset.data.y):\n",
    "        test_indices.append(torch.from_numpy(idx))\n",
    "\n",
    "    val_indices = [test_indices[i - 1] for i in range(folds)]\n",
    "\n",
    "    for i in range(folds):\n",
    "        train_mask = torch.ones(len(dataset), dtype=torch.bool)\n",
    "        train_mask[test_indices[i]] = 0\n",
    "        train_mask[val_indices[i]] = 0\n",
    "        train_indices.append(train_mask.nonzero().view(-1))\n",
    "\n",
    "    return train_indices, test_indices, val_indices\n",
    "\n",
    "\n",
    "\n",
    "def load_checkpoint(model, checkpoint):\n",
    "    if checkpoint != 'No':\n",
    "        print(\"loading checkpoint...\")\n",
    "        model_dict = model.state_dict()\n",
    "        modelCheckpoint = torch.load(checkpoint)\n",
    "        pretrained_dict = modelCheckpoint['state_dict']\n",
    "        new_dict = {k: v for k, v in pretrained_dict.items() if ((k in model_dict.keys()))}\n",
    "        model_dict.update(new_dict)\n",
    "        print('Total : {}, update: {}'.format(len(pretrained_dict), len(new_dict)))\n",
    "        model.load_state_dict(model_dict)\n",
    "        print(\"loaded finished!\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226056b6",
   "metadata": {
    "id": "33a70973"
   },
   "source": [
    "## Definition of Requirment Parameters as args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a25086e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a25086e1",
    "outputId": "ccea8abe-0fbd-4eb1-815a-2c21d54b55d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model='GIN', gpu_id=0, dataset_name='PROTEINS', num_deg=1000, num_layers=5, hidden_units=64, batch_size=128, bit=4, max_epoch=100, max_cycle=2000, folds=5, weight_decay=0, lr=0.01, a_loss=0.001, lr_quant_scale_fea=0.02, lr_quant_scale_xw=0.01, lr_quant_scale_weight=0.02, lr_quant_bit_fea=0.008, lr_quant_bit_weight=0.0001, lr_step_size=50, lr_decay_factor=0.5, lr_schedule_patience=10, is_naive=False, resume=True, store_ckpt=True, uniform=True, use_norm_quant=True, a_storage=1, result_folder='result', check_folder='checkpoint', pathdataset='/')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "\n",
    "# Clearing the arguments\n",
    "sys.argv = ['']\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model',type=str,default='GIN')\n",
    "parser.add_argument('--gpu_id',type=int,default=0)\n",
    "parser.add_argument('--dataset_name',type=str,default='PROTEINS')\n",
    "parser.add_argument('--num_deg',type=int,default=1000)\n",
    "parser.add_argument('--num_layers', type=int, default=5)\n",
    "parser.add_argument('--hidden_units',type=int,default=64)\n",
    "parser.add_argument('--batch-size',type=int,default=128)\n",
    "parser.add_argument('--bit',type=int,default=4)\n",
    "parser.add_argument('--max_epoch',type=int,default=100)\n",
    "parser.add_argument('--max_cycle',type=int,default=2000)\n",
    "parser.add_argument('--folds',type=int,default=5)\n",
    "parser.add_argument('--weight_decay',type=float,default=0)\n",
    "parser.add_argument('--lr',type=float,default=0.01)\n",
    "parser.add_argument('--a_loss',type=float,default=0.001)\n",
    "parser.add_argument('--lr_quant_scale_fea',type=float,default=0.02)\n",
    "parser.add_argument('--lr_quant_scale_xw',type=float,default=1e-2)\n",
    "parser.add_argument('--lr_quant_scale_weight',type=float,default=0.02)\n",
    "parser.add_argument('--lr_quant_bit_fea',type=float,default=0.008)\n",
    "parser.add_argument('--lr_quant_bit_weight',type=float,default=0.0001)\n",
    "parser.add_argument('--lr_step_size',type=int, default=50)\n",
    "parser.add_argument('--lr_decay_factor',type=float,default=0.5)\n",
    "parser.add_argument('--lr_schedule_patience',type=int,default=10)\n",
    "parser.add_argument('--is_naive',type=bool,default=False)\n",
    "###############################################################\n",
    "parser.add_argument('--resume',type=bool,default=True)\n",
    "parser.add_argument('--store_ckpt',type=bool,default=True)\n",
    "parser.add_argument('--uniform',type=bool,default=True)\n",
    "parser.add_argument('--use_norm_quant',type=bool,default=True)\n",
    "###############################################################\n",
    "# The target memory size of nodes features\n",
    "parser.add_argument('--a_storage',type=float,default=1)\n",
    "# Path to results\n",
    "parser.add_argument('--result_folder',type=str,default='result')\n",
    "# Path to checkpoint\n",
    "parser.add_argument('--check_folder',type=str,default='checkpoint')\n",
    "# Path to dataset\n",
    "parser.add_argument('--pathdataset',type=str,default='/')\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1fe2c50",
   "metadata": {
    "id": "b1fe2c50"
   },
   "outputs": [],
   "source": [
    "###############################################################\n",
    "model = args.model\n",
    "dataset_name = args.dataset_name\n",
    "num_layers = args.num_layers\n",
    "hidden_units=args.hidden_units\n",
    "bit=args.bit\n",
    "max_epoch = args.max_epoch\n",
    "resume = args.resume\n",
    "\n",
    "\n",
    "# Path direction\n",
    "pathresult = args.result_folder+'/'+args.model+'_'+dataset_name\n",
    "pathcheck = args.check_folder+'/'+args.model+'_'+dataset_name\n",
    "if not os.path.exists(pathresult):\n",
    "    os.makedirs(pathresult)\n",
    "if not os.path.exists(pathcheck):\n",
    "    os.makedirs(pathcheck)\n",
    "###############################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e65b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4829bb89",
   "metadata": {},
   "source": [
    "## Loading Dataset and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7603e335",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7603e335",
    "outputId": "c4821940-66ec-4243-c86e-6778f8416e61"
   },
   "outputs": [],
   "source": [
    "def get_dataset(dataset_dir, dataset_name):\n",
    "    dataset = TUDataset(dataset_dir, dataset_name)\n",
    "    \n",
    "    if dataset.data.x is None:\n",
    "        max_degree = 0\n",
    "        degs = []\n",
    "        for data in dataset:\n",
    "            degs += [degree(data.edge_index[0], dtype=torch.long)]\n",
    "            max_degree = max(max_degree, degs[-1].max().item())\n",
    "\n",
    "        if max_degree < 1000:\n",
    "            dataset.transform = T.OneHotDegree(max_degree)\n",
    "        else:\n",
    "            deg = torch.cat(degs, dim=0).to(torch.float)\n",
    "            mean, std = deg.mean().item(), deg.std().item()\n",
    "            dataset.transform = NormalizedDegree(mean, std)\n",
    "            \n",
    "    return dataset                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3de8fefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "dataset=get_dataset(args.pathdataset, args.dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc04ce93",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22387597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_bit_proteins(dataset, state_dict, all_positive=True):\n",
    "    # Step 1: Collect layer-wise bit parameters\n",
    "    layer_bits = {}\n",
    "  \n",
    "    for key, param in state_dict.items():\n",
    "        if'quant' in key and 'bit' in key and 'fea' in key:\n",
    "            layer_name = key.split('.quant_bit_fea')[0]\n",
    "            layer_bits[layer_name] = param.abs().round() - 1\n",
    "\n",
    "    # Step 2: Per-graph analysis\n",
    "    for i, data in enumerate(dataset):\n",
    "        #print(f\"\\n===== Analyzing Graph {i+1}/{len(dataset)} =====\")\n",
    "        edge_index = data.edge_index\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, data.x.size(0)).cpu()\n",
    "        \n",
    "        # Step 3: Per-layer analysis within current graph\n",
    "        for layer_name, bits in layer_bits.items():\n",
    "            # Skip if bits tensor doesn't match current graph size\n",
    "            if bits.size(0) != deg.size(0):\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nLayer {layer_name}:\")\n",
    "            print(f\"Avg bits: {bits.mean().item():.2f}\")\n",
    "            \n",
    "            # Bit-degree correlation\n",
    "            for bit_val in range(0, 9):\n",
    "                mask = (bits == bit_val)\n",
    "                if mask.sum() > 0:\n",
    "                    avg_deg = deg[mask].mean().item()\n",
    "                    print(f\"  {bit_val}-bit nodes: {mask.sum().item()} nodes, Avg Degree={avg_deg:.1f}\")\n",
    "    \n",
    "    # Step 4: Weight quantization analysis\n",
    "    weight_bits = []\n",
    "    for key, param in state_dict.items():\n",
    "        if'quant' in key and 'bit' in key and 'fea' in key:\n",
    "            bits = param.abs().round() - 1\n",
    "            weight_bits.append(bits.mean().item())\n",
    "    \n",
    "    print(\"\\n===== Weight Quantization Summary =====\")\n",
    "    if weight_bits:\n",
    "        print(f\"Avg weight bits: {sum(weight_bits)/len(weight_bits):.2f}\")\n",
    "    else:\n",
    "        print(\"No weight quantization parameters found\")\n",
    "    \n",
    "    print(\"Analysis complete\")\n",
    "    return {sum(weight_bits)/len(weight_bits)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "295c28c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(bit=32, max_epoch=5):\n",
    "        args.batch_size=16\n",
    "        args.max_epoch=5\n",
    "        args.batch_size=32\n",
    "        max_acc =0.5\n",
    "        \n",
    "        if bit== 32:\n",
    "            qypte='FP32'\n",
    "        elif bit== 4:\n",
    "            qypte= 'INT4'    \n",
    "        elif bit== 8:\n",
    "            qypte = 'INT8'\n",
    "        \n",
    "     \n",
    "      \n",
    "        val_losses, accu, durations = [], [], []\n",
    "        quant_model_accuracy=[]\n",
    "        quant_model_loss=[]\n",
    "        t_quant_model=[]\n",
    "        Num_parm_quant_model=[]\n",
    "        quant_model_size=[]\n",
    "        quant_energy_consumption=[]\n",
    "        quant_cpu_usage=[]\n",
    "        quant_memory_usage=[]\n",
    "       \n",
    " \n",
    "        #Eva= OrderedDict()\n",
    "        #Eva=dict()\n",
    "        # Initialize a dictionary to store all results per iteration\n",
    "        Eva_iter = {\n",
    "            \"val losses per iter\": [],\n",
    "            \"durations per iter\": [],\n",
    "            \"quant model accuracy per iter\": [],\n",
    "            \"time inference of quant model per iter\": [],\n",
    "            \"number parmameters of quant model per iter\": [],  # Store the best accuracy for each fold\n",
    "            \"size of quant model per iter\": [],\n",
    "            \"energy consumption of quant model per iter\": [],\n",
    "            \"cpu usage of quant model per iter\": [],\n",
    "            \"total memory usage of quant model per iter\": [],\n",
    "            \"final_metrics\": {}  # Store final metrics (mean, std, etc.)\n",
    "        }\n",
    "    \n",
    "        \n",
    "     \n",
    "\n",
    "\n",
    "        for fold, (train_idx, test_idx, val_idx) in enumerate(zip(*k_fold(dataset, args.folds))):\n",
    "            print_max_acc=0\n",
    "            train_dataset = dataset[train_idx.tolist()]\n",
    "            test_dataset = dataset[test_idx.tolist()]\n",
    "            val_dataset = dataset[val_idx.tolist()]\n",
    "            train_loader = DataLoader(train_dataset, args.batch_size, num_workers=0,shuffle=False, drop_last=True)\n",
    "            val_loader = DataLoader(val_dataset, args.batch_size,num_workers=0,shuffle=False,drop_last=True)\n",
    "            test_loader = DataLoader(test_dataset, args.batch_size,num_workers=0,shuffle=False, drop_last=True)\n",
    "            k=0\n",
    "\n",
    "\n",
    "            model=qGIN(train_dataset, args.num_layers,hidden_units=args.hidden_units,bit=args.bit, is_q=True,\n",
    "                    num_deg=args.num_deg,\n",
    "                    uniform=args.uniform).to(device)\n",
    "            weight_paras,quant_paras_bit_weight, quant_paras_bit_fea, quant_paras_scale_weight, quant_paras_scale_fea, quant_paras_scale_xw, quant_paras_bit_xw, other_paras = paras_group(model)\n",
    "            # quant_paras_bit.requires_grad = False\n",
    "            optimizer = torch.optim.Adam([{'params':weight_paras},\n",
    "                                        {'params':quant_paras_scale_weight,'lr':args.lr_quant_scale_weight,'weight_decay':0},\n",
    "                                        {'params':quant_paras_scale_fea,'lr':args.lr_quant_scale_fea,'weight_decay':0},\n",
    "                                        {'params':quant_paras_scale_xw,'lr':args.lr_quant_scale_xw,'weight_decay':0},\n",
    "                                        # {'params':quant_paras_bit_weight,'lr':args.lr_quant_bit_weight,'weight_decay':0},\n",
    "                                        {'params':quant_paras_bit_fea,'lr':args.lr_quant_bit_fea,'weight_decay':0},\n",
    "                                        {'params':other_paras}],\n",
    "                                        lr=args.lr, weight_decay=args.weight_decay)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size, gamma=args.lr_decay_factor)\n",
    "            \n",
    "            t_start = time.perf_counter()\n",
    "\n",
    "            Eva_fold= OrderedDict() #It is a dictionary to arrange output of this fold\n",
    "            \n",
    "            # Remove saved model from previous fold\n",
    "            files = glob.glob('*.pth.tar')\n",
    "            for f in files:\n",
    "                 #if f!='{}.pth'.format(best_model):\n",
    "                        os.remove(f)\n",
    "            for epoch in range(max_epoch):\n",
    "                #t = tqdm(epoch)\n",
    "                train_loss=0\n",
    "                train_loss = train(model,optimizer,train_loader,args.a_loss, args.a_storage)\n",
    "                start = time.process_time()\n",
    "                val_loss = eval_loss(model,val_loader)\n",
    "                val_losses.append(val_loss)\n",
    "                end = time.process_time()\n",
    "                acc = eval_acc(model,test_loader)\n",
    "                \n",
    "\n",
    "                if epoch % 50 == 0:\n",
    "                    print(f\"Eval Epoch: {epoch} |Val_loss:{val_loss:.03f}| Train_Loss: {train_loss:.3f} | Acc: {acc:.3f}|Fold: {fold}\")\n",
    "                accu.append(acc)\n",
    "                if(acc>max_acc):\n",
    "                    max_acc = acc\n",
    "                    path=pathcheck+'/'+args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'quantized.pth.tar'\n",
    "                    torch.save({'state_dict': model.state_dict(), 'best_accu': acc,}, path)\n",
    "                if(acc>print_max_acc):\n",
    "                    print_max_acc = acc\n",
    "\n",
    "            t_end = time.perf_counter()\n",
    "            durations.append(t_end - t_start)      \n",
    "                    \n",
    "            # Start monitoring CPU and memory usage, model size, number of parametes, time inference and  power consumption\n",
    "            quant_model_path=pathcheck+'/'+args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'quantized.pth.tar'\n",
    "            #state = torch.load(quant_model_path)\n",
    "            #dict=state['state_dict']\n",
    "            #recover_model = lambda: model.load_state_dict(state['state_dict'])\n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5)  # Add a 5-second delay to stabilize the initial state\n",
    "            tracemalloc.start()  # Start tracking memory allocations\n",
    "            snapshot_before = tracemalloc.take_snapshot()#take a snapshot of the current memory state before starting the measurement.\n",
    "\n",
    "            t0 = time.perf_counter()\n",
    "            initial_cpu_usage = get_cpu_usage()\n",
    "            power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "\n",
    "\n",
    "            fold_quant_model_accuracy= eval_acc(model, test_loader)\n",
    "\n",
    "            fold_quant_cpu_usage = get_cpu_usage()\n",
    "            t1 = time.perf_counter()\n",
    "            fold_t_quant_model=t1-t0\n",
    "\n",
    "            snapshot_after = tracemalloc.take_snapshot()\n",
    "            tracemalloc.stop()\n",
    "            top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "            folde_quant_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "            fold_quant_energy_consumption = power_usage * fold_t_quant_model\n",
    "            #fold_quant_model_size = os.path.getsize(main_model_path)\n",
    "            fold_quant_model_size =calculate_model_size(model, qypte )\n",
    "            fold_num_parm_quant_model=get_num_parameters(model, count_nonzero_only=True)\n",
    "            \n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5) \n",
    "            #Update Eva dictionary\n",
    "            Eva_fold.update({'quant model accuracy per fold': fold_quant_model_accuracy,\n",
    "                        'time inference of quant model per fold':fold_t_quant_model,\n",
    "                        'number parmameters of quant model per fold': fold_num_parm_quant_model,\n",
    "                        'size of quant model per fold': fold_quant_model_size, \n",
    "                        'energy consumption of quant model per fold':fold_quant_energy_consumption,\n",
    "                        'total memory usage of quant model per fold':folde_quant_total_memory_diff,\n",
    "                        'cpu usage of quant model per fold':fold_quant_cpu_usage\n",
    "                       })\n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5) \n",
    "\n",
    "\n",
    "            quant_model_accuracy.append(Eva_fold['quant model accuracy per fold'])\n",
    "            t_quant_model.append(Eva_fold['time inference of quant model per fold'])\n",
    "            Num_parm_quant_model.append(int(Eva_fold['number parmameters of quant model per fold']))\n",
    "            quant_model_size.append(int(Eva_fold['size of quant model per fold']))\n",
    "            quant_energy_consumption.append(Eva_fold['energy consumption of quant model per fold'])\n",
    "            quant_cpu_usage.append(Eva_fold['cpu usage of quant model per fold'])\n",
    "            quant_memory_usage.append(Eva_fold['total memory usage of quant model per fold'])\n",
    "           # analysis_bit_proteins(dataset, model.state_dict(), all_positive=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        Eva_iter[\"quant model accuracy per iter\"]= stat.mean(quant_model_accuracy)\n",
    "        Eva_iter[\"time inference of quant model per iter\"]= stat.mean(t_quant_model)\n",
    "        Eva_iter[\"number parmameters of quant model per iter\"]=  stat.mean(Num_parm_quant_model)\n",
    "        Eva_iter[\"size of quant model per iter\"]= stat.mean(quant_model_size)\n",
    "        Eva_iter[\"energy consumption of quant model per iter\"]= stat.mean(quant_energy_consumption)\n",
    "        Eva_iter[\"cpu usage of quant model per iter\"]= stat.mean(quant_cpu_usage)\n",
    "        Eva_iter[\"total memory usage of quant model per iter\"]= stat.mean(quant_memory_usage)\n",
    "\n",
    "\n",
    "        loss, acc, duration = tensor(val_losses), tensor(accu), tensor(durations)\n",
    "        loss, acc = loss.view(args.folds, max_epoch), acc.view(args.folds, max_epoch)\n",
    "        loss, argmin = loss.min(dim=1)\n",
    "        acc = acc[torch.arange(args.folds, dtype=torch.long), argmin]\n",
    "\n",
    "        Eva_iter[\"val losses per iter\"]= loss.mean().item()\n",
    "        Eva_iter[\"durations per iter\"]= duration.mean().item()\n",
    "\n",
    "\n",
    "        state = torch.load(quant_model_path)\n",
    "        dict=state['state_dict']\n",
    "     \n",
    "\n",
    "        return Eva_iter , model                                       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902aaa5b",
   "metadata": {},
   "source": [
    "### Manual Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e6eb2f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are all list of criteria for measurements. \n",
    "# We collect all desired datas of each list across iterations. \n",
    "# Then, we compute average and std of each list.\n",
    "\n",
    "\n",
    "\n",
    "#quant model\n",
    "Quant_val_loss=[]\n",
    "Quant_duration=[]\n",
    "Quant_model_accuracy=[]\n",
    "T_quant_model=[]\n",
    "Num_parm_quant_model=[]\n",
    "Quant_model_size=[]\n",
    "Quant_Energy_Consumption=[]\n",
    "Quant_Cpu_Usage=[]\n",
    "Quant_Memory_Usage=[]\n",
    "\n",
    "\n",
    "# Here is the dictionary to record the list of all measurements\n",
    "Eva_measure={'quant validation loss':Quant_val_loss,\n",
    "             'quant duration':Quant_duration,\n",
    "            'quant model accuracy': Quant_model_accuracy,\n",
    "            'time inference of quant model':T_quant_model,\n",
    "            'number parmameters of quant model':Num_parm_quant_model,\n",
    "            'quant model size':Quant_model_size,\n",
    "            'energy consumption of quant model':Quant_Energy_Consumption,\n",
    "            'cpu usage of quant model':Quant_Cpu_Usage,\n",
    "            'memory usage of quant model':Quant_Memory_Usage}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cdc0f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.max_epoch=50\n",
    "max_epoch = args.max_epoch\n",
    "iterations=1\n",
    "args.bit=8\n",
    "bit=args.bit\n",
    "folds=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5d83e16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************\n",
      "The iteration is :1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Epoch: 0 |Val_loss:0.903| Train_Loss: 1.642 | Acc: 0.596|Fold: 0\n",
      "Eval Epoch: 0 |Val_loss:1.950| Train_Loss: 1.832 | Acc: 0.596|Fold: 1\n",
      "Eval Epoch: 0 |Val_loss:0.795| Train_Loss: 2.060 | Acc: 0.596|Fold: 2\n",
      "Eval Epoch: 0 |Val_loss:0.829| Train_Loss: 1.781 | Acc: 0.595|Fold: 3\n",
      "Eval Epoch: 0 |Val_loss:1.949| Train_Loss: 1.866 | Acc: 0.595|Fold: 4\n"
     ]
    }
   ],
   "source": [
    "#### load the quantized  model\n",
    "\n",
    "for i in range(iterations):\n",
    "    print('********************************************')\n",
    "    print(f'The iteration is :{i+1} ')\n",
    "   \n",
    " \n",
    "\n",
    "    \n",
    "    Eva_iter,model=run(bit, max_epoch)\n",
    "\n",
    " \n",
    "    Quant_val_loss.append(Eva_iter[\"val losses per iter\"])\n",
    "    Quant_duration.append(Eva_iter[\"durations per iter\"])\n",
    "    Quant_model_accuracy.append(Eva_iter[\"quant model accuracy per iter\"])\n",
    "    T_quant_model.append(Eva_iter[\"time inference of quant model per iter\"])\n",
    "    Num_parm_quant_model.append(Eva_iter[\"number parmameters of quant model per iter\"])\n",
    "    Quant_model_size.append(Eva_iter[\"size of quant model per iter\"])\n",
    "    Quant_Energy_Consumption.append(Eva_iter[\"energy consumption of quant model per iter\"])\n",
    "    Quant_Cpu_Usage.append( Eva_iter[\"cpu usage of quant model per iter\"])\n",
    "    Quant_Memory_Usage.append(Eva_iter[\"total memory usage of quant model per iter\"])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e16f4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Weight Quantization Summary =====\n",
      "Avg weight bits: 6.08\n",
      "Analysis complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{6.084250013033549}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50 Epoch-bit =8\n",
    "quant_model_path=pathcheck+'/'+args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'quantized.pth.tar'\n",
    "state = torch.load(quant_model_path)\n",
    "dict=state['state_dict']\n",
    "analysis_bit_proteins(dataset, model.state_dict(), all_positive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e975996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIN_PROTEINS_4bitquantized.pth.tar\n",
      "\n",
      "===== Weight Quantization Summary =====\n",
      "Avg weight bits: 6.08\n",
      "Analysis complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{6.084250013033549}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save model by Danny bit=8\n",
    "bit=4\n",
    "quant_model_path= args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'quantized.pth.tar'\n",
    "print(quant_model_path)\n",
    "state = torch.load(quant_model_path,map_location=torch.device('cpu'))\n",
    "dict=state['state_dict']\n",
    "analysis_bit_proteins(dataset, model.state_dict(), all_positive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "41cf4946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIN_PROTEINS_4bitquantized.pth.tar\n",
      "\n",
      "===== Weight Quantization Summary =====\n",
      "Avg weight bits: 6.08\n",
      "Analysis complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{6.084250013033549}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save model by Danny bit=4\n",
    "\n",
    "\n",
    "args.bit=4\n",
    "bit=args.bit\n",
    "quant_model_path= args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'quantized.pth.tar'\n",
    "print(quant_model_path)\n",
    "state = torch.load(quant_model_path,map_location=torch.device('cpu'))\n",
    "dict=state['state_dict']\n",
    "analysis_bit_proteins(dataset, model.state_dict(), all_positive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a0e38bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint/GIN_PROTEINS/GIN_PROTEINS_8bitquantized.pth.tar\n",
      "\n",
      "===== Weight Quantization Summary =====\n",
      "Avg weight bits: 6.08\n",
      "Analysis complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{6.084250013033549}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20 Epoch-bit =8\n",
    "bit =8\n",
    "quant_model_path=pathcheck+'/'+args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'quantized.pth.tar'\n",
    "print(quant_model_path)\n",
    "state = torch.load(quant_model_path)\n",
    "dict=state['state_dict']\n",
    "analysis_bit_proteins(dataset, model.state_dict(), all_positive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a4b1614a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Weight Quantization Summary =====\n",
      "Avg weight bits: 3.58\n",
      "Analysis complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{3.5833333333333335}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epoch=2, bit=8\n",
    "bit=8\n",
    "model=qGIN(dataset, args.num_layers,hidden_units=args.hidden_units,bit=args.bit, is_q=True,\n",
    "                    num_deg=args.num_deg,\n",
    "                    uniform=args.uniform).to(device)\n",
    "quant_model_path=pathcheck+'/'+args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'quantized.pth.tar'\n",
    "state = torch.load(quant_model_path)\n",
    "dict=state['state_dict']\n",
    "analysis_bit_proteins(dataset, model.state_dict(), all_positive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d17e12fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Weight Quantization Summary =====\n",
      "Avg weight bits: 3.58\n",
      "Analysis complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{3.5833333333333335}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epoch=2, bit=4\n",
    "quant_model_path=pathcheck+'/'+args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'quantized.pth.tar'\n",
    "state = torch.load(quant_model_path)\n",
    "dict=state['state_dict']\n",
    "analysis_bit_proteins(dataset, model.state_dict(), all_positive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e814cce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a85384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daba64a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6556fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a dictionary to save all measurements. Aftre measuring, we can compute mean and std of each item.\n",
    "from collections import OrderedDict \n",
    "Eva_final = OrderedDict()\n",
    "\n",
    "\n",
    "\n",
    "quant_model_val_loss_mean =stat.mean(Quant_val_loss)\n",
    "quant_model_val_loss_std = stat.stdev(Quant_val_loss)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of quant loss validation':float(format(quant_model_val_loss_mean, '.3f'))})\n",
    "Eva_final.update({'Std of quant loss validation':float(format(quant_model_val_loss_std, '.3f'))})    \n",
    "\n",
    "quant_model_duration_mean =stat.mean(Quant_duration)\n",
    "quant_model_duration_std = stat.stdev(Quant_duration)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of quant model duration':float(format(quant_model_duration_mean , '.3f'))})\n",
    "Eva_final.update({'Std of quant model duration':float(format(quant_model_duration_std, '.3f'))})                                         \n",
    "                                     \n",
    "\n",
    "quant_model_accuracy_mean =stat.mean(Quant_model_accuracy)\n",
    "quant_model_accuracy_std = stat.stdev(Quant_model_accuracy)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of quant model accuracy':float(format(quant_model_accuracy_mean, '.3f'))})\n",
    "Eva_final.update({'Std of quant model accuracy':float(format(quant_model_accuracy_std, '.3f'))})\n",
    "                 \n",
    "\n",
    "t_quant_model_mean = stat.mean(T_quant_model)\n",
    "t_quant_model_std =stat.stdev(T_quant_model)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of time inference of quant model':float(format(t_quant_model_mean, '.3f'))})\n",
    "Eva_final.update({'Std of time inference of quant model':float(format(t_quant_model_std, '.3f'))})\n",
    "\n",
    "num_parm_quant_model_mean = stat.mean(Num_parm_quant_model)\n",
    "num_parm_quant_model_std = stat.stdev(Num_parm_quant_model)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of number parmameters of quant model':num_parm_quant_model_mean})\n",
    "Eva_final.update({'Std of number parmameters of quant model':num_parm_quant_model_std})\n",
    "\n",
    "quant_model_size_mean =stat.mean( Quant_model_size)\n",
    "quant_model_size_std = stat.stdev(Quant_model_size)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of quant model size':quant_model_size_mean})\n",
    "Eva_final.update({'Std of quant_model_size':quant_model_size_std })\n",
    "\n",
    "quant_energy_consumption_mean = stat.mean(Quant_Energy_Consumption)\n",
    "quant_energy_consumption_std = stat.stdev(Quant_Energy_Consumption)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of energy consumption of quant model':quant_energy_consumption_mean })\n",
    "Eva_final.update({'Std of energy consumption of quant model':quant_energy_consumption_std})\n",
    "\n",
    "\n",
    "quant_cpu_usage_mean = stat.mean(Quant_Cpu_Usage)\n",
    "quant_cpu_usage_std = stat.stdev(Quant_Cpu_Usage)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of cpu usage of quant model':quant_cpu_usage_mean})\n",
    "Eva_final.update({'Std of cpu usage of quant model':quant_cpu_usage_std})\n",
    "\n",
    "quant_memory_usage_mean = stat.mean(Quant_Memory_Usage)\n",
    "quant_memory_usage_std = stat.stdev(Quant_Memory_Usage)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of memory usage of quant model':quant_memory_usage_mean})\n",
    "Eva_final.update({'Std of memory usage of quant model':quant_memory_usage_std})\n",
    "\n",
    "#################################\n",
    "\n",
    "\n",
    "# Determing Quantization Method \n",
    "\n",
    "print(f\"All measurement about A-A-Q Quantization process of type:{ bit} \")   \n",
    "Eva_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "757883a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the accuracy\n",
    "Quantization_Method='AAQ'\n",
    "\n",
    "file_name = pathresult+'/'+Quantization_Method+'Method'+'_On'+args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'.txt'\n",
    "\n",
    "with open(file_name, 'w') as f:\n",
    "    for key, value in vars(args).items():\n",
    "        f.write('%s:%s\\n'%(key, value))\n",
    "\n",
    "    for key, value in Eva_final.items():\n",
    "        f.write('%s:%s\\n'%(key, value))\n",
    "\n",
    "    for key, value in Eva_measure.items():\n",
    "        f.write('%s:%s\\n' % (key, ','.join(map(str, value))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae6422",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de8649b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import Parameter, Module, ModuleDict\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import (\n",
    "    softmax,\n",
    "    add_self_loops,\n",
    "    remove_self_loops,\n",
    "    add_remaining_self_loops,\n",
    "    degree,\n",
    ")\n",
    "import torch_scatter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_sparse import SparseTensor, matmul, fill_diag, sum as sparsesum, mul\n",
    "\n",
    "\n",
    "def scatter_(name, src, index, dim=0, dim_size=None):\n",
    "    \"\"\"Taken from an earlier version of PyG\"\"\"\n",
    "    assert name in [\"add\", \"mean\", \"min\", \"max\"]\n",
    "\n",
    "    op = getattr(torch_scatter, \"scatter_{}\".format(name))\n",
    "    out = op(src, index, dim, None, dim_size)\n",
    "    out = out[0] if isinstance(out, tuple) else out\n",
    "\n",
    "    if name == \"max\":\n",
    "        out[out < -10000] = 0\n",
    "    elif name == \"min\":\n",
    "        out[out > 10000] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "def analysis_bit(data, state_dict,all_positive=True,name='plat'):\n",
    "    mean_all = []\n",
    "    if(name=='ogbn-arxiv'):\n",
    "        adj_t = data.adj_t\n",
    "        adj_t = adj_t.fill_value(1.,)\n",
    "        deg = sparsesum(adj_t, dim=1)\n",
    "    else:\n",
    "        edge_index = data.edge_index\n",
    "        row,col = edge_index\n",
    "        deg = degree(col,data.x.size(0))\n",
    "    for key in state_dict.keys():\n",
    "        if ('quant' in key and 'bit' in key and 'fea' in key):\n",
    "            if(all_positive):\n",
    "                bit=state_dict[key].abs().round()-1\n",
    "            else:\n",
    "                bit=state_dict[key].abs().round()-1\n",
    "            print(key+'\\n')\n",
    "            mean_all.append(bit.mean())\n",
    "            print('The average bits of current layer:',bit.mean())\n",
    "            print(\"0bit:{}\".format((bit==0).sum()))\n",
    "            print(\"1bit:{}\".format((bit==1).sum()))\n",
    "            print(\"2bit:{}\".format((bit==2).sum()))\n",
    "            print(\"3bit:{}\".format((bit==3).sum()))\n",
    "            print(\"4bit:{}\".format((bit==4).sum()))\n",
    "            print(\"5bit:{}\".format((bit==5).sum()))\n",
    "            print(\"6bit:{}\".format((bit==6).sum()))\n",
    "            print(\"7bit:{}\".format((bit==7).sum()))\n",
    "            print(\"8bit:{}\".format((bit==8).sum()))\n",
    "            print(\"9bit:{}\".format((bit==9).sum()))\n",
    "            print('\\n')\n",
    "            print('The average degree of the nodes using corresponding bitwidth:')\n",
    "            index_1_bit = torch.where(bit==1)[0]\n",
    "            index_2_bit = torch.where(bit==2)[0]\n",
    "            index_3_bit = torch.where(bit==3)[0]\n",
    "            index_4_bit = torch.where(bit==4)[0]\n",
    "            index_5_bit = torch.where(bit==5)[0]\n",
    "            index_6_bit = torch.where(bit==6)[0]\n",
    "            index_7_bit = torch.where(bit==7)[0]\n",
    "            index_8_bit = torch.where(bit==8)[0]\n",
    "            print('1bit_deg_mean:',deg[index_1_bit].mean())\n",
    "            print('2bit_deg_mean:',deg[index_2_bit].mean())\n",
    "            print('3bit_deg_mean:',deg[index_3_bit].mean())\n",
    "            print('4bit_deg_mean:',deg[index_4_bit].mean())\n",
    "            print('5bit_deg_mean:',deg[index_5_bit].mean())\n",
    "            print('6bit_deg_mean:',deg[index_6_bit].mean())\n",
    "            print('7bit_deg_mean:',deg[index_7_bit].mean())\n",
    "            print('8bit_deg_mean:',deg[index_8_bit].mean())\n",
    "            print('\\n')\n",
    "    print('The average bits: ',sum(mean_all)/len(mean_all))\n",
    "    print('Finish')\n",
    "\n",
    "msg_special_args = set(\n",
    "    [\n",
    "        \"edge_index\",\n",
    "        \"edge_index_i\",\n",
    "        \"edge_index_j\",\n",
    "        \"size\",\n",
    "        \"size_i\",\n",
    "        \"size_j\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "aggr_special_args = set(\n",
    "    [\n",
    "        \"index\",\n",
    "        \"dim_size\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "update_special_args = set([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924fae31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
