{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6284bda",
   "metadata": {
    "id": "6474ebc9"
   },
   "source": [
    "$A^2Q$ Quantization method of BBBP dataset Training by GIN Models\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a7ccc6",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "h63j2srEtN_2",
   "metadata": {
    "id": "h63j2srEtN_2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import statistics as stat\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import argparse\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "\n",
    "# CPU and Enegusage \n",
    "import psutil\n",
    "import itertools\n",
    "import tracemalloc\n",
    "import gc\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import tensor\n",
    "from torch.nn import Linear, Sequential, ReLU, Identity, BatchNorm1d as BN\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree,remove_self_loops\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import TUDataset,Planetoid,GNNBenchmarkDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_scatter import scatter_mean\n",
    "from torch.autograd.function import InplaceFunction\n",
    "from torch_geometric.nn import GCNConv,GINConv,global_mean_pool,TopKPooling\n",
    "\n",
    "# For downloading BBBP dataset from MoleculeNet and Transformation\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch.utils.data import random_split, Subset\n",
    "from torch_geometric.data import Data, InMemoryDataset, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "\n",
    "\n",
    "# AAQ Quantization\n",
    "from quantize_function.u_quant_gc_bit_debug import *\n",
    "from quantize_function.MessagePassing_gc_bit import GINConvMultiQuant\n",
    "from quantize_function.get_scale_index import get_deg_index, get_scale_index\n",
    "from utils.quant_utils import analysis_bit\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e81107",
   "metadata": {},
   "source": [
    "### Functions for Mmeasuring criterias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782501eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_parameters(model: nn.Module, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the total number of parameters of model\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    num_counted_elements = 0\n",
    "    for param in model.parameters():\n",
    "        if count_nonzero_only:\n",
    "            num_counted_elements += param.count_nonzero()\n",
    "        else:\n",
    "            num_counted_elements += param.numel()\n",
    "    return num_counted_elements\n",
    "\n",
    "# Function to get CPU usage\n",
    "def get_cpu_usage():\n",
    "    return psutil.cpu_percent(interval=1)\n",
    "\n",
    "\n",
    "\n",
    "# Function to approximate power consumption (Assume some average power usage per CPU percentage point)\n",
    "def estimate_power_usage(cpu_usage):\n",
    "    base_power_usage = 10  # Assumed base power usage in watts\n",
    "    power_per_percent = 0.5  # Assumed additional watts per CPU usage percent\n",
    "    return base_power_usage + (power_per_percent * cpu_usage)\n",
    "\n",
    "# The model size based on the number of parameters\n",
    "def calculate_model_size(model: nn.Module, \n",
    "                         qypte: str = 'fp32', \n",
    "                         include_metadata: bool = False,\n",
    "                         model_path: str = None) -> float:\n",
    "    \"\"\"\n",
    "    Calculate model size in KB/MB for different precisions.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        precision: 'fp32' (32-bit float) or 'int4' (4-bit integer)\n",
    "        include_metadata: Whether to include PyTorch metadata in size calculation\n",
    "        model_path: If provided, will check actual file size on disk\n",
    "        \n",
    "    Returns:\n",
    "        Size in KB (if include_metadata=False) or actual file size (if include_metadata=True)\n",
    "    \"\"\"\n",
    "    # Get total number of parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Calculate theoretical size\n",
    "    if qypte == 'FP32':\n",
    "        size_bits = total_params * 32\n",
    "    elif qypte== 'INT4':\n",
    "        size_bits = total_params * 4\n",
    "    elif qypte == 'INT8':\n",
    "        size_bits = total_params * 8    \n",
    "   \n",
    "    \n",
    "    size_bytes = size_bits / 8\n",
    "    size_kb = size_bytes / 1024\n",
    "    \n",
    "    # If checking actual file size\n",
    "    if include_metadata and model_path:\n",
    "        if not os.path.exists(model_path):\n",
    "            # Save model to temporary file if path doesn't exist\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        actual_size_kb = os.path.getsize(model_path) / 1024\n",
    "        return actual_size_kb\n",
    "    \n",
    "    return size_kb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8363e4e8",
   "metadata": {
    "id": "aec599ef"
   },
   "source": [
    "## Function for Quatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28fc1163",
   "metadata": {
    "id": "28fc1163"
   },
   "outputs": [],
   "source": [
    "def paras_group(model):\n",
    "    all_params = model.parameters()\n",
    "    weight_paras=[]\n",
    "    quant_paras_bit_weight = []\n",
    "    quant_paras_bit_fea = []\n",
    "    quant_paras_scale_weight = []\n",
    "    quant_paras_scale_fea = []\n",
    "    quant_paras_scale_xw = []\n",
    "    quant_paras_bit_xw = []\n",
    "    other_paras = []\n",
    "    for name,para in model.named_parameters():\n",
    "        if('quant' in name and 'bit' in name and 'weight' in name):\n",
    "            quant_paras_bit_weight+=[para]\n",
    "            # para.requires_grad = False\n",
    "        elif('quant' in name and 'bit' in name and 'fea' in name):\n",
    "            quant_paras_bit_fea+=[para]\n",
    "        elif('quant' in name and 'bit' not in name and 'weight' in name):\n",
    "            quant_paras_scale_weight+=[para]\n",
    "            # para.requires_grad = False\n",
    "        elif('quant' in name and 'bit' not in name and 'fea' in name):\n",
    "            quant_paras_scale_fea+=[para]\n",
    "        elif('xw'in name and 'q' in name and 'bit' not in name):\n",
    "            quant_paras_scale_xw+=[para]\n",
    "        elif('xw'in name and 'q' in name and 'bit' in name):\n",
    "            quant_paras_bit_xw+=[para]\n",
    "        elif('weight' in name and 'quant' not in name ):\n",
    "            weight_paras+=[para]\n",
    "    params_id = list(map(id,quant_paras_bit_fea))+list(map(id,quant_paras_bit_weight))+list(map(id,quant_paras_scale_weight))+list(map(id,quant_paras_scale_fea))+list(map(id,weight_paras))\\\n",
    "    +list(map(id,quant_paras_scale_xw))+list(map(id,quant_paras_bit_xw))\n",
    "    other_paras = list(filter(lambda p: id(p) not in params_id, all_params))\n",
    "    return weight_paras,quant_paras_bit_weight,quant_paras_bit_fea,quant_paras_scale_weight,quant_paras_scale_fea,quant_paras_scale_xw,quant_paras_bit_xw,other_paras\n",
    "\n",
    "def setup_seed(seed):\n",
    "      torch.manual_seed(seed)\n",
    "      torch.cuda.manual_seed_all(seed)\n",
    "      np.random.seed(seed)\n",
    "      random.seed(seed)\n",
    "    #  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def parameter_stastic(model,dataset,hidden_units):\n",
    "    w_Byte = 0\n",
    "    a_Byte = 0\n",
    "    for name, par in model.named_parameters():\n",
    "        if(('bit' in name)&('weight' in name)):\n",
    "            if('conv1' in name):\n",
    "                scale = dataset.num_node_features\n",
    "            else:\n",
    "                scale = hidden_units\n",
    "            par = torch.floor(par)\n",
    "            w_Byte = scale*par.sum()/8./1024.+w_Byte\n",
    "        elif(('bit' in name)&('fea' in name)):\n",
    "            if('conv1' in name):\n",
    "                a_scale = 0\n",
    "            else:\n",
    "                a_scale = hidden_units\n",
    "            # a_scale = dataset.data.num_nodes\n",
    "            par = torch.floor(par)\n",
    "            a_Byte = a_scale*par.sum()/8./1024.+a_Byte\n",
    "    return w_Byte, a_Byte\n",
    "\n",
    "class ResettableSequential(nn.Sequential):\n",
    "    def reset_parameters(self):\n",
    "        for child in self.children():\n",
    "            if hasattr(child, \"reset_parameters\"):\n",
    "                child.reset_parameters()\n",
    "    def forward(self,input,edge_index,bit_sum):\n",
    "        for model in self:\n",
    "            input,_,bit_sum = model(input,edge_index,bit_sum)\n",
    "        return input,bit_sum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56aeb4a8",
   "metadata": {
    "id": "56aeb4a8"
   },
   "outputs": [],
   "source": [
    "# Relu and Batch Normalization\n",
    "class relu(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "    def forward(self,x,edge_index,bit_sum):\n",
    "        x[x<0] = 0\n",
    "        return x,edge_index,bit_sum\n",
    "\n",
    "class bn(nn.Module):\n",
    "    def __init__(self,hidden_units):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(hidden_units)\n",
    "    def forward(self,x,edge_index,bit_sum):\n",
    "        x = self.bn(x)\n",
    "        return x,edge_index,bit_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e500ba11",
   "metadata": {},
   "source": [
    "## QGIN Model (GIN with Quatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e32186",
   "metadata": {
    "id": "07e32186"
   },
   "outputs": [],
   "source": [
    "\n",
    "class qGIN(nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden_units, bit, num_deg=1000, is_q=False,\n",
    "                    uniform=False,init='norm'):\n",
    "        super(qGIN, self).__init__()\n",
    "        gin_layer = GINConvMultiQuant\n",
    "        self.bit = bit\n",
    "        para_list=[[{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'gama_init':0.70,'gama_std':0.1}],\n",
    "                   [{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'gama_init':0.6,'gama_std':0.7}],\n",
    "                   [{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'gama_init':0.76,'gama_std':0.68}],\n",
    "                   [{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'gama_init':0.6,'gama_std':0.5}],\n",
    "                   [{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1},{'gama_init':0.6,'gama_std':0.3}],\n",
    "                   [{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1}],\n",
    "                   [{'alpha_init':0.01,'gama_init':0.01,'alpha_std':0.1,'gama_std':0.1}]]\n",
    "        if(is_q):\n",
    "            # As the DQ, we either don't quantize the input features of the REDDIT-BINARY dataset because the feature\n",
    "            # is only 1-dimension.\n",
    "            self.conv1 = gin_layer(\n",
    "                ResettableSequential(\n",
    "                    QLinear(9,hidden_units, num_deg, bit,para_dict=para_list[0][0], all_positive=True,\n",
    "                            quant_fea=True,\n",
    "                            uniform=uniform,init=init),\n",
    "                    relu(),\n",
    "                    QLinear(hidden_units, hidden_units, num_deg, bit, para_dict=para_list[0][1],all_positive=True,\n",
    "                            uniform=uniform,init=init),\n",
    "                    relu(),\n",
    "                ),\n",
    "                train_eps=True,\n",
    "                in_features=num_deg, out_features=1,\n",
    "                bit=bit, para_dict=para_list[0][2],quant_fea=True,uniform=uniform\n",
    "            )\n",
    "        else:\n",
    "            self.conv1 = GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(9, hidden_units),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_units, hidden_units),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm1d(hidden_units),\n",
    "                ),\n",
    "                train_eps=True,\n",
    "            )\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            if(is_q):\n",
    "                self.convs.append(\n",
    "                    gin_layer(\n",
    "                        ResettableSequential(\n",
    "                            QLinear(hidden_units, hidden_units, num_deg,bit, para_dict=para_list[0][0],all_positive=False,\n",
    "                                    uniform=uniform,init=init),\n",
    "                            relu(),\n",
    "                            QLinear(hidden_units, hidden_units, num_deg,bit, para_dict=para_list[0][1], all_positive=True,\n",
    "                                    uniform=uniform,init=init),\n",
    "                            relu(),\n",
    "                        ),\n",
    "                        train_eps=True,\n",
    "                        in_features=num_deg, out_features=hidden_units,\n",
    "                        bit=bit, para_dict=para_list[0][2], uniform=uniform,quant_fea=True\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                self.convs.append(\n",
    "                    GINConv(\n",
    "                        nn.Sequential(\n",
    "                            nn.Linear(hidden_units, hidden_units),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(hidden_units, hidden_units),\n",
    "                            nn.ReLU(),\n",
    "                            nn.BatchNorm1d(hidden_units),\n",
    "                        ),\n",
    "                        train_eps=True,\n",
    "                    )\n",
    "                )\n",
    "        self.bn_list = torch.nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.bn_list.append(nn.BatchNorm1d(hidden_units))\n",
    "        if(is_q):\n",
    "            self.lin1 = QLinear(hidden_units, hidden_units, num_deg, bit, para_dict=para_list[-1][0], all_positive=False,\n",
    "                                        uniform=uniform,init=init)\n",
    "            self.lin2 = QLinear(hidden_units, dataset.num_classes, num_deg, bit, para_dict=para_list[-1][0], all_positive=True,\n",
    "                                        uniform=uniform,init=init)\n",
    "        else:\n",
    "            self.lin1 = nn.Linear(hidden_units, hidden_units)\n",
    "            self.lin2 = nn.Linear(hidden_units, dataset.num_classes)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        bit_sum=x.new_zeros(1)\n",
    "        x,bit_sum = self.conv1(x, edge_index,bit_sum)\n",
    "        x = self.bn_list[0](x)\n",
    "        # x,_,bit_sum = self.embeding(x,edge_index,bit_sum)\n",
    "        # x = F.relu(x)\n",
    "        i = 1\n",
    "        for conv in self.convs:\n",
    "            x,bit_sum = conv(x,edge_index,bit_sum)\n",
    "            x = self.bn_list[i](x)\n",
    "            i=i+1\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x,_,bit_sum = self.lin1(x,edge_index,bit_sum)\n",
    "        x = F.relu(x)\n",
    "        x,_,bit_sum = self.lin2(x,edge_index,bit_sum)\n",
    "        return F.log_softmax(x, dim=-1),bit_sum\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7911a",
   "metadata": {},
   "source": [
    "# Helpful Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c8795f2",
   "metadata": {
    "id": "2c8795f2"
   },
   "outputs": [],
   "source": [
    "class NormalizedDegree(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, data):\n",
    "        deg = degree(data.edge_index[0], dtype=torch.float)\n",
    "        deg = (deg - self.mean) / self.std\n",
    "        data.x = deg.view(-1, 1)\n",
    "        return data\n",
    "\n",
    "\n",
    "def num_graphs(data):\n",
    "    if data.batch is not None:\n",
    "        return data.num_graphs\n",
    "    else:\n",
    "        return data.x.size(0)\n",
    "\n",
    "\n",
    "def train(model, optimizer, loader,a_loss, a_storage=1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        out,bit_sum = model(data)\n",
    "        target = data.y.view(-1)\n",
    "\n",
    "        # Check if the last batch is smaller\n",
    "        if out.size(0) != target.size(0):\n",
    "            target = target[:out.size(0)]  # Truncate target to match output size\n",
    "\n",
    "        loss = F.cross_entropy(out, target)\n",
    "        loss_store = a_loss*F.relu(bit_sum-a_storage)**2\n",
    "        loss_store.backward(retain_graph=True)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * num_graphs(data)\n",
    "        optimizer.step()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def eval_acc(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:        \n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(data)[0].max(1)[1] \n",
    "\n",
    "        target = data.y.view(-1)\n",
    "        # Check if the last batch is smaller\n",
    "        if pred.size(0) != target.size(0):\n",
    "            target = target[:pred.size(0)]  # Truncate target to match output size                 \n",
    "        correct+=pred.eq(target).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_loss(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    for data in loader:     \n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)[0]\n",
    "\n",
    "        target = data.y.view(-1)\n",
    "\n",
    "        # Check if the last batch is smaller\n",
    "        if out.size(0) != target.size(0):\n",
    "            target = target[:out.size(0)]  # Truncate target to match output size                \n",
    "        loss += F.cross_entropy(out, target, reduction=\"sum\").item()\n",
    "    return loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "# Real k_fold\n",
    "def k_fold(dataset, folds):\n",
    "    skf = StratifiedKFold(folds, shuffle=True, random_state=12345)\n",
    "\n",
    "    test_indices, train_indices = [], []\n",
    "    for _, idx in skf.split(torch.zeros(len(dataset)), dataset.data.y):\n",
    "        test_indices.append(torch.from_numpy(idx))\n",
    "\n",
    "    val_indices = [test_indices[i - 1] for i in range(folds)]\n",
    "\n",
    "    for i in range(folds):\n",
    "        train_mask = torch.ones(len(dataset), dtype=torch.bool)\n",
    "        train_mask[test_indices[i]] = 0\n",
    "        train_mask[val_indices[i]] = 0\n",
    "        train_indices.append(train_mask.nonzero().view(-1))\n",
    "\n",
    "    return train_indices, test_indices, val_indices\n",
    "\n",
    "\n",
    "def load_checkpoint(model, checkpoint):\n",
    "    if checkpoint != 'No':\n",
    "        print(\"loading checkpoint...\")\n",
    "        model_dict = model.state_dict()\n",
    "        modelCheckpoint = torch.load(checkpoint)\n",
    "        pretrained_dict = modelCheckpoint['state_dict']\n",
    "        new_dict = {k: v for k, v in pretrained_dict.items() if ((k in model_dict.keys()))}\n",
    "        model_dict.update(new_dict)\n",
    "        print('Total : {}, update: {}'.format(len(pretrained_dict), len(new_dict)))\n",
    "        model.load_state_dict(model_dict)\n",
    "        print(\"loaded finished!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c9e20",
   "metadata": {
    "id": "5e53efba"
   },
   "source": [
    "## Setting Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a25086e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a25086e1",
    "outputId": "ccea8abe-0fbd-4eb1-815a-2c21d54b55d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model='GIN', gpu_id=0, dataset_name='BBBP', num_deg=1000, num_layers=5, hidden_units=64, batch_size=64, bit=4, max_epoch=100, max_cycle=2000, folds=10, weight_decay=0, lr=0.01, a_loss=0.001, lr_quant_scale_fea=0.02, lr_quant_scale_xw=0.01, lr_quant_scale_weight=0.02, lr_quant_bit_fea=0.008, lr_quant_bit_weight=0.0001, lr_step_size=50, lr_decay_factor=0.5, lr_schedule_patience=10, is_naive=False, resume=True, store_ckpt=True, uniform=True, use_norm_quant=True, a_storage=1, result_folder='result', check_folder='checkpoint', pathdataset='/')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "\n",
    "# Clearing the arguments\n",
    "sys.argv = ['']\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model',type=str,default='GIN')\n",
    "parser.add_argument('--gpu_id',type=int,default=0)\n",
    "parser.add_argument('--dataset_name',type=str,default='BBBP')\n",
    "parser.add_argument('--num_deg',type=int,default=1000)\n",
    "parser.add_argument('--num_layers', type=int, default=5)\n",
    "parser.add_argument('--hidden_units',type=int,default=64)\n",
    "parser.add_argument('--batch-size',type=int,default=64)\n",
    "parser.add_argument('--bit',type=int,default=4)\n",
    "parser.add_argument('--max_epoch',type=int,default=100)\n",
    "parser.add_argument('--max_cycle',type=int,default=2000)\n",
    "parser.add_argument('--folds',type=int,default=10)\n",
    "parser.add_argument('--weight_decay',type=float,default=0)\n",
    "parser.add_argument('--lr',type=float,default=0.01)\n",
    "parser.add_argument('--a_loss',type=float,default=0.001)\n",
    "parser.add_argument('--lr_quant_scale_fea',type=float,default=0.02)\n",
    "parser.add_argument('--lr_quant_scale_xw',type=float,default=1e-2)\n",
    "parser.add_argument('--lr_quant_scale_weight',type=float,default=0.02)\n",
    "parser.add_argument('--lr_quant_bit_fea',type=float,default=0.008)\n",
    "parser.add_argument('--lr_quant_bit_weight',type=float,default=0.0001)\n",
    "parser.add_argument('--lr_step_size',type=int, default=50)\n",
    "parser.add_argument('--lr_decay_factor',type=float,default=0.5)\n",
    "parser.add_argument('--lr_schedule_patience',type=int,default=10)\n",
    "parser.add_argument('--is_naive',type=bool,default=False)\n",
    "###############################################################\n",
    "parser.add_argument('--resume',type=bool,default=True)\n",
    "parser.add_argument('--store_ckpt',type=bool,default=True)\n",
    "parser.add_argument('--uniform',type=bool,default=True)\n",
    "parser.add_argument('--use_norm_quant',type=bool,default=True)\n",
    "###############################################################\n",
    "# The target memory size of nodes features\n",
    "parser.add_argument('--a_storage',type=float,default=1)\n",
    "# Path to results\n",
    "parser.add_argument('--result_folder',type=str,default='result')\n",
    "# Path to checkpoint\n",
    "parser.add_argument('--check_folder',type=str,default='checkpoint')\n",
    "# Path to dataset\n",
    "parser.add_argument('--pathdataset',type=str,default='/')\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1fe2c50",
   "metadata": {
    "id": "b1fe2c50"
   },
   "outputs": [],
   "source": [
    "###############################################################\n",
    "model = args.model\n",
    "dataset_name = args.dataset_name\n",
    "num_layers = args.num_layers\n",
    "hidden_units=args.hidden_units\n",
    "bit=args.bit\n",
    "max_epoch = args.max_epoch\n",
    "resume = args.resume\n",
    "\n",
    "\n",
    "# Path direction\n",
    "pathresult = args.result_folder+'/'+args.model+'_'+dataset_name\n",
    "pathcheck = args.check_folder+'/'+args.model+'_'+dataset_name\n",
    "if not os.path.exists(pathresult):\n",
    "    os.makedirs(pathresult)\n",
    "if not os.path.exists(pathcheck):\n",
    "    os.makedirs(pathcheck)\n",
    "###############################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70672cef",
   "metadata": {},
   "source": [
    "## Loading Dataset and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d4766d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_MolecueNet(dataset_dir, dataset_name, task=None):\n",
    "    \"\"\" Attention the multi-task problems not solved yet \"\"\"\n",
    "    molecule_net_dataset_names = {name.lower(): name for name in MoleculeNet.names.keys()}\n",
    "    dataset = MoleculeNet(root=dataset_dir, name=molecule_net_dataset_names[dataset_name.lower()])\n",
    "    dataset.data.x = dataset.data.x.float()\n",
    "    if task is None:\n",
    "        dataset.data.y = dataset.data.y.squeeze().long()\n",
    "    else:\n",
    "        dataset.data.y = dataset.data.y[task].long()\n",
    "    dataset.node_type_dict = None\n",
    "    dataset.node_color = None\n",
    "    return dataset\n",
    "\n",
    "def get_dataset(dataset_dir, dataset_name, task=None):\n",
    "   \n",
    "    molecule_net_dataset_names = [name.lower() for name in MoleculeNet.names.keys()]\n",
    "    dataset=load_MolecueNet(dataset_dir, dataset_name, task)\n",
    "\n",
    "    return  dataset\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def get_dataloader(dataset, batch_size, random_split_flag=True, data_split_ratio=None, seed=2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset:\n",
    "        batch_size: int\n",
    "        random_split_flag: bool\n",
    "        data_split_ratio: list, training, validation and testing ratio\n",
    "        seed: random seed to split the dataset randomly\n",
    "    Returns:\n",
    "        a dictionary of training, validation, and testing dataLoader\n",
    "    \"\"\"\n",
    "\n",
    "    if not random_split_flag and hasattr(dataset, 'supplement'):\n",
    "        assert 'split_indices' in dataset.supplement.keys(), \"split idx\"\n",
    "        split_indices = dataset.supplement['split_indices']\n",
    "        train_indices = torch.where(split_indices == 0)[0].numpy().tolist()\n",
    "        dev_indices = torch.where(split_indices == 1)[0].numpy().tolist()\n",
    "        test_indices = torch.where(split_indices == 2)[0].numpy().tolist()\n",
    "\n",
    "        train = Subset(dataset, train_indices)\n",
    "        eval = Subset(dataset, dev_indices)\n",
    "        test = Subset(dataset, test_indices)\n",
    "    else:\n",
    "        num_train = int(0.8 * len(dataset))\n",
    "        num_eval = int(0.1 * len(dataset))\n",
    "        num_test = len(dataset) - num_train - num_eval\n",
    "\n",
    "        train, eval, test = random_split(dataset, lengths=[num_train, num_eval, num_test],\n",
    "                                         generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    dataloader = dict()\n",
    "    dataloader['train'] = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    dataloader['eval'] = DataLoader(eval, batch_size=batch_size, shuffle=False)\n",
    "    dataloader['test'] = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78acbfe9",
   "metadata": {},
   "source": [
    "### Loading dataset and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6859b681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args.dataset_name=='BBBP'\n",
    "dataset = get_dataset(args.pathdataset, args.dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7603e335",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7603e335",
    "outputId": "c4821940-66ec-4243-c86e-6778f8416e61"
   },
   "outputs": [],
   "source": [
    "\n",
    "if dataset.data.x is None:\n",
    "    max_degree = 0\n",
    "    degs = []\n",
    "    for data in dataset:\n",
    "        degs += [degree(data.edge_index[0], dtype=torch.long)]\n",
    "        max_degree = max(max_degree, degs[-1].max().item())\n",
    "\n",
    "    if max_degree < 1000:\n",
    "        dataset.transform = T.OneHotDegree(max_degree)\n",
    "    else:\n",
    "        deg = torch.cat(degs, dim=0).to(torch.float)\n",
    "        mean, std = deg.mean().item(), deg.std().item()\n",
    "        dataset.transform = NormalizedDegree(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edb3f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(bit=32, max_epoch=5):\n",
    "        args.batch_size=16\n",
    "        args.max_epoch=5\n",
    "        args.batch_size=32\n",
    "        max_acc =0.65\n",
    "        if bit== 32:\n",
    "            qypte='FP32'\n",
    "        elif bit== 4:\n",
    "            qypte= 'INT4'    \n",
    "        elif bit== 8:\n",
    "            qypte = 'INT8'\n",
    "            \n",
    "        \n",
    "     \n",
    "      \n",
    "        val_losses, accu, durations = [], [], []\n",
    "        quant_model_accuracy=[]\n",
    "        quant_model_loss=[]\n",
    "        t_quant_model=[]\n",
    "        Num_parm_quant_model=[]\n",
    "        quant_model_size=[]\n",
    "        quant_energy_consumption=[]\n",
    "        quant_cpu_usage=[]\n",
    "        quant_memory_usage=[]\n",
    "       \n",
    " \n",
    "        #Eva= OrderedDict()\n",
    "        #Eva=dict()\n",
    "        # Initialize a dictionary to store all results per iteration\n",
    "        Eva_iter = {\n",
    "            \"val losses per iter\": [],\n",
    "            \"durations per iter\": [],\n",
    "            \"quant model accuracy per iter\": [],\n",
    "            \"time inference of quant model per iter\": [],\n",
    "            \"number parmameters of quant model per iter\": [],  # Store the best accuracy for each fold\n",
    "            \"size of quant model per iter\": [],\n",
    "            \"energy consumption of quant model per iter\": [],\n",
    "            \"cpu usage of quant model per iter\": [],\n",
    "            \"total memory usage of quant model per iter\": [],\n",
    "            \"final_metrics\": {}  # Store final metrics (mean, std, etc.)\n",
    "        }\n",
    "    \n",
    "        \n",
    "     \n",
    "\n",
    "\n",
    "        for fold, (train_idx, test_idx, val_idx) in enumerate(zip(*k_fold(dataset, args.folds))):\n",
    "            print_max_acc=0\n",
    "            train_dataset = dataset[train_idx.tolist()]\n",
    "            test_dataset = dataset[test_idx.tolist()]\n",
    "            val_dataset = dataset[val_idx.tolist()]\n",
    "            train_loader = DataLoader(train_dataset, args.batch_size, num_workers=0,shuffle=False, drop_last=True)\n",
    "            val_loader = DataLoader(val_dataset, args.batch_size,num_workers=0,shuffle=False,drop_last=True)\n",
    "            test_loader = DataLoader(test_dataset, args.batch_size,num_workers=0,shuffle=False, drop_last=True)\n",
    "            k=0\n",
    "\n",
    "\n",
    "            model=qGIN(train_dataset, args.num_layers,hidden_units=args.hidden_units,bit=args.bit, is_q=True,\n",
    "                    num_deg=args.num_deg,\n",
    "                    uniform=args.uniform).to(device)\n",
    "            weight_paras,quant_paras_bit_weight, quant_paras_bit_fea, quant_paras_scale_weight, quant_paras_scale_fea, quant_paras_scale_xw, quant_paras_bit_xw, other_paras = paras_group(model)\n",
    "            # quant_paras_bit.requires_grad = False\n",
    "            optimizer = torch.optim.Adam([{'params':weight_paras},\n",
    "                                        {'params':quant_paras_scale_weight,'lr':args.lr_quant_scale_weight,'weight_decay':0},\n",
    "                                        {'params':quant_paras_scale_fea,'lr':args.lr_quant_scale_fea,'weight_decay':0},\n",
    "                                        {'params':quant_paras_scale_xw,'lr':args.lr_quant_scale_xw,'weight_decay':0},\n",
    "                                        # {'params':quant_paras_bit_weight,'lr':args.lr_quant_bit_weight,'weight_decay':0},\n",
    "                                        {'params':quant_paras_bit_fea,'lr':args.lr_quant_bit_fea,'weight_decay':0},\n",
    "                                        {'params':other_paras}],\n",
    "                                        lr=args.lr, weight_decay=args.weight_decay)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size, gamma=args.lr_decay_factor)\n",
    "            \n",
    "            t_start = time.perf_counter()\n",
    "\n",
    "            Eva_fold= OrderedDict() #It is a dictionary to arrange output of this fold\n",
    "        \n",
    "            for epoch in range(max_epoch):\n",
    "                #t = tqdm(epoch)\n",
    "                train_loss=0\n",
    "                train_loss = train(model,optimizer,train_loader,args.a_loss, args.a_storage)\n",
    "                start = time.process_time()\n",
    "                val_loss = eval_loss(model,val_loader)\n",
    "                val_losses.append(val_loss)\n",
    "                end = time.process_time()\n",
    "                acc = eval_acc(model,test_loader)\n",
    "                \n",
    "\n",
    "                if epoch % 50 == 0:\n",
    "                    print(f\"Eval Epoch: {epoch} |Val_loss:{val_loss:.03f}| Train_Loss: {train_loss:.3f} | Acc: {acc:.3f}|Fold: {fold}\")\n",
    "                accu.append(acc)\n",
    "                if(acc>max_acc):\n",
    "                    max_acc = acc\n",
    "                    path=pathcheck+'/'+args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'quantized.pth.tar'\n",
    "                    torch.save({'state_dict': model.state_dict(), 'best_accu': acc,}, path)\n",
    "                    path_q='Q-model.pth'\n",
    "                    #state_sparse_model(model,  path_q)\n",
    "                if(acc>print_max_acc):\n",
    "                    print_max_acc = acc\n",
    "\n",
    "            t_end = time.perf_counter()\n",
    "            durations.append(t_end - t_start)      \n",
    "                    \n",
    "            # Start monitoring CPU and memory usage, model size, number of parametes, time inference and  power consumption\n",
    "            quant_model_path=pathcheck+'/'+args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'quantized.pth.tar'\n",
    "            #state = torch.load(quant_model_path)\n",
    "            #dict=state['state_dict']\n",
    "            #recover_model = lambda: model.load_state_dict(state['state_dict'])\n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5)  # Add a 5-second delay to stabilize the initial state\n",
    "            tracemalloc.start()  # Start tracking memory allocations\n",
    "            snapshot_before = tracemalloc.take_snapshot()#take a snapshot of the current memory state before starting the measurement.\n",
    "\n",
    "            t0 = time.perf_counter()\n",
    "            initial_cpu_usage = get_cpu_usage()\n",
    "            power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "\n",
    "\n",
    "            fold_quant_model_accuracy= eval_acc(model, test_loader)\n",
    "\n",
    "            fold_quant_cpu_usage = get_cpu_usage()\n",
    "            t1 = time.perf_counter()\n",
    "            fold_t_quant_model=t1-t0\n",
    "\n",
    "            snapshot_after = tracemalloc.take_snapshot()\n",
    "            tracemalloc.stop()\n",
    "            top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "            folde_quant_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "            fold_quant_energy_consumption = power_usage * fold_t_quant_model\n",
    "            #fold_quant_model_size = os.path.getsize(main_model_path)\n",
    "            fold_quant_model_size =calculate_model_size(model, qypte )\n",
    "         \n",
    "            fold_num_parm_quant_model=get_num_parameters(model, count_nonzero_only=True)\n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5) \n",
    "            #Update Eva dictionary\n",
    "            Eva_fold.update({'quant model accuracy per fold': fold_quant_model_accuracy,\n",
    "                        'time inference of quant model per fold':fold_t_quant_model,\n",
    "                        'number parmameters of quant model per fold': fold_num_parm_quant_model,\n",
    "                        'size of quant model per fold': fold_quant_model_size, \n",
    "                        'energy consumption of quant model per fold':fold_quant_energy_consumption,\n",
    "                        'total memory usage of quant model per fold':folde_quant_total_memory_diff,\n",
    "                        'cpu usage of quant model per fold':fold_quant_cpu_usage\n",
    "                       })\n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5) \n",
    "\n",
    "\n",
    "            quant_model_accuracy.append(Eva_fold['quant model accuracy per fold'])\n",
    "            t_quant_model.append(Eva_fold['time inference of quant model per fold'])\n",
    "            Num_parm_quant_model.append(int(Eva_fold['number parmameters of quant model per fold']))\n",
    "            quant_model_size.append(int(Eva_fold['size of quant model per fold']))\n",
    "            quant_energy_consumption.append(Eva_fold['energy consumption of quant model per fold'])\n",
    "            quant_cpu_usage.append(Eva_fold['cpu usage of quant model per fold'])\n",
    "            quant_memory_usage.append(Eva_fold['total memory usage of quant model per fold'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        Eva_iter[\"quant model accuracy per iter\"]= stat.mean(quant_model_accuracy)\n",
    "        Eva_iter[\"time inference of quant model per iter\"]= stat.mean(t_quant_model)\n",
    "        Eva_iter[\"number parmameters of quant model per iter\"]=  stat.mean(Num_parm_quant_model)\n",
    "        Eva_iter[\"size of quant model per iter\"]= stat.mean(quant_model_size)\n",
    "        Eva_iter[\"energy consumption of quant model per iter\"]= stat.mean(quant_energy_consumption)\n",
    "        Eva_iter[\"cpu usage of quant model per iter\"]= stat.mean(quant_cpu_usage)\n",
    "        Eva_iter[\"total memory usage of quant model per iter\"]= stat.mean(quant_memory_usage)\n",
    "\n",
    "\n",
    "        loss, acc, duration = tensor(val_losses), tensor(accu), tensor(durations)\n",
    "        loss, acc = loss.view(args.folds, max_epoch), acc.view(args.folds, max_epoch)\n",
    "        loss, argmin = loss.min(dim=1)\n",
    "        acc = acc[torch.arange(args.folds, dtype=torch.long), argmin]\n",
    "\n",
    "        Eva_iter[\"val losses per iter\"]= loss.mean().item()\n",
    "        Eva_iter[\"durations per iter\"]= duration.mean().item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return Eva_iter    , model                                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b050379a",
   "metadata": {},
   "source": [
    "### Manual Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b289e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are all list of criteria for measurements. \n",
    "# We collect all desired datas of each list across iterations. \n",
    "# Then, we compute average and std of each list.\n",
    "\n",
    "\n",
    "\n",
    "#quant model\n",
    "Quant_val_loss=[]\n",
    "Quant_duration=[]\n",
    "Quant_model_accuracy=[]\n",
    "T_quant_model=[]\n",
    "Num_parm_quant_model=[]\n",
    "Quant_model_size=[]\n",
    "Quant_Energy_Consumption=[]\n",
    "Quant_Cpu_Usage=[]\n",
    "Quant_Memory_Usage=[]\n",
    "\n",
    "\n",
    "# Here is the dictionary to record the list of all measurements\n",
    "Eva_measure={'quant validation loss':Quant_val_loss,\n",
    "             'quant duration':Quant_duration,\n",
    "            'quant model accuracy': Quant_model_accuracy,\n",
    "            'time inference of quant model':T_quant_model,\n",
    "            'number parmameters of quant model':Num_parm_quant_model,\n",
    "            'quant model size':Quant_model_size,\n",
    "            'energy consumption of quant model':Quant_Energy_Consumption,\n",
    "            'cpu usage of quant model':Quant_Cpu_Usage,\n",
    "            'memory usage of quant model':Quant_Memory_Usage}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "884e3da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.max_epoch=1\n",
    "max_epoch = args.max_epoch\n",
    "iterations=1\n",
    "args.bit=4\n",
    "bit=args.bit\n",
    "folds=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dded791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_sparse_model(model, path, bit_width=4):\n",
    "    state_dict = model.state_dict()\n",
    "    compressed_state = {}\n",
    "    \n",
    "    for k, v in state_dict.items():\n",
    "        if torch.is_tensor(v):\n",
    "            mask = v != 0\n",
    "            if mask.any():  # Only compress if there are non-zeros\n",
    "                if bit_width == 4:\n",
    "                    # For 4-bit quantization, we'll pack two values into each byte\n",
    "                    values = v[mask]\n",
    "                    # Scale to 4-bit range (0-15)\n",
    "                    v_min = values.min()\n",
    "                    v_max = values.max()\n",
    "                    scaled_values = (values - v_min) / (v_max - v_min) * 15\n",
    "                    quantized_values = scaled_values.round().to(torch.uint8)\n",
    "                    \n",
    "                    # Pack two 4-bit values into each byte\n",
    "                    packed = torch.zeros((quantized_values.numel() + 1) // 2, dtype=torch.uint8)\n",
    "                    packed[:] = (quantized_values[::2] << 4) | (quantized_values[1::2] & 0x0F)\n",
    "                    \n",
    "                    compressed_state[k] = {\n",
    "                        'shape': v.shape,\n",
    "                        'values': packed,  # Packed 4-bit values\n",
    "                        'min': v_min,\n",
    "                        'max': v_max,\n",
    "                        'dtype': 'uint4'\n",
    "                    }\n",
    "                else:\n",
    "                    compressed_state[k] = {\n",
    "                        'shape': v.shape,\n",
    "                        'values': v[mask].to(torch.float16)  # Default to float16 for other bit widths\n",
    "                    }\n",
    "            else:\n",
    "                compressed_state[k] = v  # Keep original if all zeros\n",
    "        else:\n",
    "            compressed_state[k] = v\n",
    "    \n",
    "    state = {'net': compressed_state, 'bit_width': bit_width}\n",
    "    torch.save(state, path)\n",
    "    \n",
    "def load_sparse_model(state_path, original_model):\n",
    "    \"\"\"\n",
    "    Loads a model saved in the custom compressed format (non-zero values only).\n",
    "    Handles both 4-bit packed and float16 formats.\n",
    "    \"\"\"\n",
    "    compressed_state = torch.load(state_path)\n",
    "    compressed_weights = compressed_state['net']\n",
    "    bit_width = compressed_state.get('bit_width', 16)  # Default to 16-bit if not specified\n",
    "    \n",
    "    new_state_dict = original_model.state_dict()\n",
    "    \n",
    "    for k, v in compressed_weights.items():\n",
    "        if isinstance(v, dict) and 'shape' in v:\n",
    "            # Reconstruct dense tensor from compressed format\n",
    "            dense_tensor = torch.zeros(v['shape'])\n",
    "            \n",
    "            if v.get('dtype') == 'uint4':\n",
    "                # Unpack 4-bit values\n",
    "                packed = v['values']\n",
    "                quantized_values = torch.zeros(packed.numel() * 2, dtype=torch.uint8)\n",
    "                \n",
    "                # Unpack two 4-bit values from each byte\n",
    "                quantized_values[::2] = (packed >> 4) & 0x0F\n",
    "                quantized_values[1::2] = packed & 0x0F\n",
    "                \n",
    "                # Trim to correct length (in case odd number of values)\n",
    "                quantized_values = quantized_values[:v['values'].numel() * 2][:torch.prod(torch.tensor(v['shape']))]\n",
    "                \n",
    "                # Dequantize\n",
    "                values = quantized_values.float() / 15 * (v['max'] - v['min']) + v['min']\n",
    "                \n",
    "                # Reshape and fill the tensor\n",
    "                flat_tensor = dense_tensor.view(-1)\n",
    "                flat_tensor[:len(values)] = values\n",
    "                dense_tensor = flat_tensor.reshape(v['shape'])\n",
    "            else:\n",
    "                # Handle float16 or other formats\n",
    "                if 'values' in v:\n",
    "                    flat_tensor = dense_tensor.view(-1)\n",
    "                    flat_tensor[:len(v['values'])] = v['values'].float()\n",
    "                    dense_tensor = flat_tensor.reshape(v['shape'])\n",
    "            \n",
    "            new_state_dict[k] = dense_tensor\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    \n",
    "    original_model.load_state_dict(new_state_dict)\n",
    "    return original_model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c47b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(bit=32, max_epoch=5):\n",
    "        args.batch_size=16\n",
    "        args.max_epoch=5\n",
    "        args.batch_size=32\n",
    "        max_acc =0.65\n",
    "        if bit== 32:\n",
    "            qypte='FP32'\n",
    "        elif bit== 4:\n",
    "            qypte= 'INT4'    \n",
    "        elif bit== 8:\n",
    "            qypte = 'INT8'\n",
    "            \n",
    "        \n",
    "     \n",
    "      \n",
    "        val_losses, accu, durations = [], [], []\n",
    "        quant_model_accuracy=[]\n",
    "        quant_model_loss=[]\n",
    "        t_quant_model=[]\n",
    "        Num_parm_quant_model=[]\n",
    "        quant_model_size=[]\n",
    "        quant_energy_consumption=[]\n",
    "        quant_cpu_usage=[]\n",
    "        quant_memory_usage=[]\n",
    "       \n",
    " \n",
    "        #Eva= OrderedDict()\n",
    "        #Eva=dict()\n",
    "        # Initialize a dictionary to store all results per iteration\n",
    "        Eva_iter = {\n",
    "            \"val losses per iter\": [],\n",
    "            \"durations per iter\": [],\n",
    "            \"quant model accuracy per iter\": [],\n",
    "            \"time inference of quant model per iter\": [],\n",
    "            \"number parmameters of quant model per iter\": [],  # Store the best accuracy for each fold\n",
    "            \"size of quant model per iter\": [],\n",
    "            \"energy consumption of quant model per iter\": [],\n",
    "            \"cpu usage of quant model per iter\": [],\n",
    "            \"total memory usage of quant model per iter\": [],\n",
    "            \"final_metrics\": {}  # Store final metrics (mean, std, etc.)\n",
    "        }\n",
    "    \n",
    "        \n",
    "     \n",
    "\n",
    "\n",
    "        for fold, (train_idx, test_idx, val_idx) in enumerate(zip(*k_fold(dataset, args.folds))):\n",
    "            print_max_acc=0\n",
    "            train_dataset = dataset[train_idx.tolist()]\n",
    "            test_dataset = dataset[test_idx.tolist()]\n",
    "            val_dataset = dataset[val_idx.tolist()]\n",
    "            train_loader = DataLoader(train_dataset, args.batch_size, num_workers=0,shuffle=False, drop_last=True)\n",
    "            val_loader = DataLoader(val_dataset, args.batch_size,num_workers=0,shuffle=False,drop_last=True)\n",
    "            test_loader = DataLoader(test_dataset, args.batch_size,num_workers=0,shuffle=False, drop_last=True)\n",
    "            k=0\n",
    "\n",
    "\n",
    "            model=qGIN(train_dataset, args.num_layers,hidden_units=args.hidden_units,bit=args.bit, is_q=True,\n",
    "                    num_deg=args.num_deg,\n",
    "                    uniform=args.uniform).to(device)\n",
    "            weight_paras,quant_paras_bit_weight, quant_paras_bit_fea, quant_paras_scale_weight, quant_paras_scale_fea, quant_paras_scale_xw, quant_paras_bit_xw, other_paras = paras_group(model)\n",
    "            # quant_paras_bit.requires_grad = False\n",
    "            optimizer = torch.optim.Adam([{'params':weight_paras},\n",
    "                                        {'params':quant_paras_scale_weight,'lr':args.lr_quant_scale_weight,'weight_decay':0},\n",
    "                                        {'params':quant_paras_scale_fea,'lr':args.lr_quant_scale_fea,'weight_decay':0},\n",
    "                                        {'params':quant_paras_scale_xw,'lr':args.lr_quant_scale_xw,'weight_decay':0},\n",
    "                                        # {'params':quant_paras_bit_weight,'lr':args.lr_quant_bit_weight,'weight_decay':0},\n",
    "                                        {'params':quant_paras_bit_fea,'lr':args.lr_quant_bit_fea,'weight_decay':0},\n",
    "                                        {'params':other_paras}],\n",
    "                                        lr=args.lr, weight_decay=args.weight_decay)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size, gamma=args.lr_decay_factor)\n",
    "            \n",
    "            t_start = time.perf_counter()\n",
    "\n",
    "            Eva_fold= OrderedDict() #It is a dictionary to arrange output of this fold\n",
    "        \n",
    "            for epoch in range(max_epoch):\n",
    "                #t = tqdm(epoch)\n",
    "                train_loss=0\n",
    "                train_loss = train(model,optimizer,train_loader,args.a_loss, args.a_storage)\n",
    "                start = time.process_time()\n",
    "                val_loss = eval_loss(model,val_loader)\n",
    "                val_losses.append(val_loss)\n",
    "                end = time.process_time()\n",
    "                acc = eval_acc(model,test_loader)\n",
    "                \n",
    "\n",
    "                if epoch % 50 == 0:\n",
    "                    print(f\"Eval Epoch: {epoch} |Val_loss:{val_loss:.03f}| Train_Loss: {train_loss:.3f} | Acc: {acc:.3f}|Fold: {fold}\")\n",
    "                accu.append(acc)\n",
    "                if(acc>max_acc):\n",
    "                    max_acc = acc\n",
    "                    path=pathcheck+'/'+args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'quantized.pth.tar'\n",
    "                    torch.save({'state_dict': model.state_dict(), 'best_accu': acc,}, path)\n",
    "                    path_q='Q-model.pth'\n",
    "                    state_sparse_model(model, path_q)\n",
    "                    \n",
    "                if(acc>print_max_acc):\n",
    "                    print_max_acc = acc\n",
    "\n",
    "            t_end = time.perf_counter()\n",
    "            durations.append(t_end - t_start)      \n",
    "                    \n",
    "            # Start monitoring CPU and memory usage, model size, number of parametes, time inference and  power consumption\n",
    "            path_q='Q-model.pth'\n",
    "              # Assuming `original_model` is the model you used to create the sparse version\n",
    "            original_model =model # Define your original model here\n",
    "\n",
    "            # Load your sparse model\n",
    "            q_model = load_sparse_model(path_q, original_model,4)\n",
    "            # Save 4-bit quantized model\n",
    "\n",
    "            \n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5)  # Add a 5-second delay to stabilize the initial state\n",
    "            tracemalloc.start()  # Start tracking memory allocations\n",
    "            snapshot_before = tracemalloc.take_snapshot()#take a snapshot of the current memory state before starting the measurement.\n",
    "\n",
    "            t0 = time.perf_counter()\n",
    "            initial_cpu_usage = get_cpu_usage()\n",
    "            power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "\n",
    "\n",
    "            fold_quant_model_accuracy= eval_acc(q_model, test_loader)\n",
    "\n",
    "            fold_quant_cpu_usage = get_cpu_usage()\n",
    "            t1 = time.perf_counter()\n",
    "            fold_t_quant_model=t1-t0\n",
    "\n",
    "            snapshot_after = tracemalloc.take_snapshot()\n",
    "            tracemalloc.stop()\n",
    "            top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "            folde_quant_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "            fold_quant_energy_consumption = power_usage * fold_t_quant_model\n",
    "            fold_quant_model_size = os.path.getsize(path_q)\n",
    "            #fold_quant_model_size =calculate_model_size(model, qypte )\n",
    "         \n",
    "            fold_num_parm_quant_model=get_num_parameters(q_model, count_nonzero_only=True)\n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5) \n",
    "            #Update Eva dictionary\n",
    "            Eva_fold.update({'quant model accuracy per fold': fold_quant_model_accuracy,\n",
    "                        'time inference of quant model per fold':fold_t_quant_model,\n",
    "                        'number parmameters of quant model per fold': fold_num_parm_quant_model,\n",
    "                        'size of quant model per fold': fold_quant_model_size, \n",
    "                        'energy consumption of quant model per fold':fold_quant_energy_consumption,\n",
    "                        'total memory usage of quant model per fold':folde_quant_total_memory_diff,\n",
    "                        'cpu usage of quant model per fold':fold_quant_cpu_usage\n",
    "                       })\n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5) \n",
    "\n",
    "\n",
    "            quant_model_accuracy.append(Eva_fold['quant model accuracy per fold'])\n",
    "            t_quant_model.append(Eva_fold['time inference of quant model per fold'])\n",
    "            Num_parm_quant_model.append(int(Eva_fold['number parmameters of quant model per fold']))\n",
    "            quant_model_size.append(int(Eva_fold['size of quant model per fold']))\n",
    "            quant_energy_consumption.append(Eva_fold['energy consumption of quant model per fold'])\n",
    "            quant_cpu_usage.append(Eva_fold['cpu usage of quant model per fold'])\n",
    "            quant_memory_usage.append(Eva_fold['total memory usage of quant model per fold'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        Eva_iter[\"quant model accuracy per iter\"]= stat.mean(quant_model_accuracy)\n",
    "        Eva_iter[\"time inference of quant model per iter\"]= stat.mean(t_quant_model)\n",
    "        Eva_iter[\"number parmameters of quant model per iter\"]=  stat.mean(Num_parm_quant_model)\n",
    "        Eva_iter[\"size of quant model per iter\"]= stat.mean(quant_model_size)\n",
    "        Eva_iter[\"energy consumption of quant model per iter\"]= stat.mean(quant_energy_consumption)\n",
    "        Eva_iter[\"cpu usage of quant model per iter\"]= stat.mean(quant_cpu_usage)\n",
    "        Eva_iter[\"total memory usage of quant model per iter\"]= stat.mean(quant_memory_usage)\n",
    "\n",
    "\n",
    "        loss, acc, duration = tensor(val_losses), tensor(accu), tensor(durations)\n",
    "        loss, acc = loss.view(args.folds, max_epoch), acc.view(args.folds, max_epoch)\n",
    "        loss, argmin = loss.min(dim=1)\n",
    "        acc = acc[torch.arange(args.folds, dtype=torch.long), argmin]\n",
    "\n",
    "        Eva_iter[\"val losses per iter\"]= loss.mean().item()\n",
    "        Eva_iter[\"durations per iter\"]= duration.mean().item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return Eva_iter                                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cd05357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************\n",
      "The iteration is :1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Epoch: 0 |Val_loss:2.757| Train_Loss: 0.520 | Acc: 0.702|Fold: 0\n",
      "Eval Epoch: 0 |Val_loss:2.852| Train_Loss: 0.481 | Acc: 0.698|Fold: 1\n",
      "Eval Epoch: 0 |Val_loss:2.353| Train_Loss: 0.506 | Acc: 0.702|Fold: 2\n",
      "Eval Epoch: 0 |Val_loss:1.716| Train_Loss: 0.519 | Acc: 0.702|Fold: 3\n",
      "Eval Epoch: 0 |Val_loss:3.090| Train_Loss: 0.540 | Acc: 0.702|Fold: 4\n",
      "Eval Epoch: 0 |Val_loss:2.984| Train_Loss: 0.507 | Acc: 0.702|Fold: 5\n",
      "Eval Epoch: 0 |Val_loss:3.462| Train_Loss: 0.465 | Acc: 0.702|Fold: 6\n",
      "Eval Epoch: 0 |Val_loss:3.175| Train_Loss: 0.502 | Acc: 0.698|Fold: 7\n",
      "Eval Epoch: 0 |Val_loss:2.678| Train_Loss: 0.504 | Acc: 0.698|Fold: 8\n",
      "Eval Epoch: 0 |Val_loss:3.779| Train_Loss: 0.497 | Acc: 0.698|Fold: 9\n"
     ]
    }
   ],
   "source": [
    "#### load the quantized  model\n",
    "\n",
    "for i in range(iterations):\n",
    "    print('********************************************')\n",
    "    print(f'The iteration is :{i+1} ')\n",
    "   \n",
    " \n",
    "\n",
    "    \n",
    "    Eva_iter, model=run(bit, max_epoch)\n",
    "\n",
    " \n",
    "    Quant_val_loss.append(Eva_iter[\"val losses per iter\"])\n",
    "    Quant_duration.append(Eva_iter[\"durations per iter\"])\n",
    "    Quant_model_accuracy.append(Eva_iter[\"quant model accuracy per iter\"])\n",
    "    T_quant_model.append(Eva_iter[\"time inference of quant model per iter\"])\n",
    "    Num_parm_quant_model.append(Eva_iter[\"number parmameters of quant model per iter\"])\n",
    "    Quant_model_size.append(Eva_iter[\"size of quant model per iter\"])\n",
    "    Quant_Energy_Consumption.append(Eva_iter[\"energy consumption of quant model per iter\"])\n",
    "    Quant_Cpu_Usage.append( Eva_iter[\"cpu usage of quant model per iter\"])\n",
    "    Quant_Memory_Usage.append(Eva_iter[\"total memory usage of quant model per iter\"])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04357fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val losses per iter': 0.5526806116104126,\n",
       " 'durations per iter': 90.92674255371094,\n",
       " 'quant model accuracy per iter': 0.23463414634146343,\n",
       " 'time inference of quant model per iter': 3.352182789996732,\n",
       " 'number parmameters of quant model per iter': 13125,\n",
       " 'size of quant model per iter': 104673,\n",
       " 'energy consumption of quant model per iter': 45.24753060845862,\n",
       " 'cpu usage of quant model per iter': 7.95,\n",
       " 'total memory usage of quant model per iter': 85547.9,\n",
       " 'final_metrics': {}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eva_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74319013",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ave of quant loss validation:0.503\n",
    "Std of quant loss validation:0.005\n",
    "Ave of quant model duration:252.819\n",
    "Std of quant model duration:99.332\n",
    "Ave of quant model accuracy:0.7\n",
    "Std of quant model accuracy:0.0\n",
    "Ave of time inference of quant model:2.15\n",
    "Std of time inference of quant model:0.061\n",
    "Ave of number parmameters of quant model:68941\n",
    "Std of number parmameters of quant model:0.0\n",
    "Ave of quant model size:33\n",
    "Std of quant_model_size:0.0\n",
    "Ave of energy consumption of quant model:29.577216650601017\n",
    "Std of energy consumption of quant model:3.5693353199278217\n",
    "Ave of cpu usage of quant model:7.2315\n",
    "Std of cpu usage of quant model:2.402536543344401\n",
    "Ave of memory usage of quant model:79201.26\n",
    "Std of memory usage of quant model:802.032075352811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a65bd4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## New version\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.prune import global_unstructured, L1Unstructured, remove\n",
    "\n",
    "\n",
    "def state_sparse_model(model,  path):\n",
    "   \n",
    "    state_dict = model.state_dict()\n",
    "    compressed_state = {}\n",
    "    \n",
    "    for k, v in state_dict.items():\n",
    "        if torch.is_tensor(v):\n",
    "            mask = v != 0\n",
    "            if mask.any():  # Only compress if there are non-zeros\n",
    "                compressed_state[k] = {\n",
    "                    'shape': v.shape,\n",
    "                    'values': v[mask].to(torch.uint8)  # Store only non-zero values\n",
    "                }\n",
    "            else:\n",
    "                compressed_state[k] = v  # Keep original if all zeros\n",
    "        else:\n",
    "            compressed_state[k] = v\n",
    "    \n",
    "    state={'net': compressed_state}\n",
    "    torch.save(state, path)\n",
    "    #pruned_model_size = os.path.getsize(path)\n",
    "    #print(f\"pruned_model_size:{pruned_model_size}\")\n",
    "   \n",
    "\n",
    "def load_sparse_model(state_path, original_model):\n",
    "    \"\"\"\n",
    "    Loads a model saved in the custom compressed format (non-zero values only).\n",
    "    Reconstructs dense tensors before loading into the model.\n",
    "    \"\"\"\n",
    "    # Load the compressed state_dict\n",
    "    compressed_state = torch.load(state_path)\n",
    "    compressed_weights = compressed_state['net']\n",
    "    \n",
    "    # Initialize a new state_dict for the original model\n",
    "    new_state_dict = original_model.state_dict()\n",
    "    \n",
    "    for k, v in compressed_weights.items():\n",
    "        if isinstance(v, dict) and 'shape' in v and 'values' in v:\n",
    "            # Reconstruct dense tensor from compressed format\n",
    "            dense_tensor = torch.zeros(v['shape'], dtype=v['values'].dtype)\n",
    "            mask = (dense_tensor != 0)  # All False initially\n",
    "            # We need to know the positions of non-zero values (if available)\n",
    "            # If indices were saved, use them; otherwise, assume sequential filling (simpler but may not match original positions)\n",
    "            if 'indices' in v:\n",
    "                # If you saved indices (advanced version)\n",
    "                dense_tensor[v['indices']] = v['values']\n",
    "            else:\n",
    "                # If only values were saved (simpler version)\n",
    "                # Flatten and fill non-zeros sequentially (may not match original positions)\n",
    "                flat_tensor = dense_tensor.view(-1)\n",
    "                flat_tensor[:len(v['values'])] = v['values']\n",
    "                dense_tensor = flat_tensor.reshape(v['shape'])\n",
    "            \n",
    "            new_state_dict[k] = dense_tensor\n",
    "        else:\n",
    "            # If it's a normal tensor (e.g., biases, batch norm stats)\n",
    "            new_state_dict[k] = v\n",
    "    \n",
    "    # Load the reconstructed state_dict\n",
    "    original_model.load_state_dict(new_state_dict, strict=False)\n",
    "    return original_model\n",
    "    \n",
    "def load_and_evaluate_pruned_model(model_args, model_path):\n",
    "    \"\"\"\n",
    "    This function loads the pruned model from disk and evaluates it.\n",
    "    \"\"\"\n",
    "    # Instantiate the model\n",
    "    model = GnnNets(input_dim, output_dim, model_args)\n",
    "\n",
    "    # Load the pruned model from disk and change arcithechture to compute accuracy\n",
    "    sparse_model= load_sparse_model(model_path, model)\n",
    "    \n",
    "    print(\"Pruned model loaded.\")\n",
    "    \n",
    "    return sparse_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bba3c956",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ-model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mstate_sparse_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36mstate_sparse_model\u001b[1;34m(model, path)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstate_sparse_model\u001b[39m(model,  path):\n\u001b[1;32m----> 9\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m()\n\u001b[0;32m     10\u001b[0m     compressed_state \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m state_dict\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "path='Q-model.pth'\n",
    "state_sparse_model(model,  path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b473a4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c3cb4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All measurement about A-A-Q Quantization process of type:4 \n",
      "quant_model_val_loss - 0.520 ± 0.004\n",
      "quant_model_duration - 167.588 ± 1.902\n",
      "quant_model_accuracy - 0.700 ± 0.000\n",
      "t_quant_model- 0.700 ± 0.000\n",
      "num_parm_quant_model- 66939.000 ± 0.000\n",
      "quant_model_size- 306003.000 ± 0.000\n",
      "quant_energy_consumption- 35.067 ± 0.087\n",
      "quant_cpu_usage- 5.680 ± 0.184\n",
      "quant_memory_usage- 78838.200 ± 867.620\n"
     ]
    }
   ],
   "source": [
    "# This is a dictionary to save all measurements. Aftre measuring, we can compute mean and std of each item.\n",
    "from collections import OrderedDict \n",
    "Eva_final = OrderedDict()\n",
    "\n",
    "print(f\"All measurement about A-A-Q Quantization process of type:{ bit} \")   \n",
    "\n",
    "quant_model_val_loss_mean =stat.mean(Quant_val_loss)\n",
    "quant_model_val_loss_std = stat.stdev(Quant_val_loss)\n",
    "quant_model_val_loss = \"{:.3f} ± {:.3f}\".format(quant_model_val_loss_mean,quant_model_val_loss_std)\n",
    "print(\"quant_model_val_loss - {}\".format(quant_model_val_loss))\n",
    "\n",
    "\n",
    "Eva_final.update({'Ave of quant loss validation':float(format(quant_model_val_loss_mean, '.3f'))})\n",
    "Eva_final.update({'Std of quant loss validation':float(format(quant_model_val_loss_std, '.3f'))})    \n",
    "\n",
    "quant_model_duration_mean =stat.mean(Quant_duration)\n",
    "quant_model_duration_std = stat.stdev(Quant_duration)\n",
    "quant_model_duration = \"{:.3f} ± {:.3f}\".format(quant_model_duration_mean,quant_model_duration_std )\n",
    "print(\"quant_model_duration - {}\".format(quant_model_duration))\n",
    "\n",
    "Eva_final.update({'Ave of quant model duration':float(format(quant_model_duration_mean , '.3f'))})\n",
    "Eva_final.update({'Std of quant model duration':float(format(quant_model_duration_std, '.3f'))})                                         \n",
    "                                     \n",
    "\n",
    "quant_model_accuracy_mean =stat.mean(Quant_model_accuracy)\n",
    "quant_model_accuracy_std = stat.stdev(Quant_model_accuracy)\n",
    "quant_model_accuracy = \"{:.3f} ± {:.3f}\".format(quant_model_accuracy_mean,quant_model_accuracy_std )\n",
    "print(\"quant_model_accuracy - {}\".format(quant_model_accuracy))\n",
    "\n",
    "Eva_final.update({'Ave of quant model accuracy':float(format(quant_model_accuracy_mean, '.3f'))})\n",
    "Eva_final.update({'Std of quant model accuracy':float(format(quant_model_accuracy_std, '.3f'))})\n",
    "                 \n",
    "\n",
    "t_quant_model_mean = stat.mean(T_quant_model)\n",
    "t_quant_model_std =stat.stdev(T_quant_model)\n",
    "t_quant_model = \"{:.3f} ± {:.3f}\".format(quant_model_accuracy_mean,quant_model_accuracy_std )\n",
    "print(\"t_quant_model- {}\".format(t_quant_model))\n",
    "      \n",
    "Eva_final.update({'Ave of time inference of quant model':float(format(t_quant_model_mean, '.3f'))})\n",
    "Eva_final.update({'Std of time inference of quant model':float(format(t_quant_model_std, '.3f'))})\n",
    "\n",
    "num_parm_quant_model_mean = stat.mean(Num_parm_quant_model)\n",
    "num_parm_quant_model_std = stat.stdev(Num_parm_quant_model)\n",
    "num_parm_quant_model= \"{:.3f} ± {:.3f}\".format(num_parm_quant_model_mean, num_parm_quant_model_std)\n",
    "print(\"num_parm_quant_model- {}\".format(num_parm_quant_model))\n",
    "\n",
    "\n",
    "\n",
    "Eva_final.update({'Ave of number parmameters of quant model':num_parm_quant_model_mean})\n",
    "Eva_final.update({'Std of number parmameters of quant model':num_parm_quant_model_std})\n",
    "\n",
    "quant_model_size_mean =stat.mean( Quant_model_size)\n",
    "quant_model_size_std = stat.stdev(Quant_model_size)\n",
    "quant_model_size= \"{:.3f} ± {:.3f}\".format(quant_model_size_mean, quant_model_size_std)\n",
    "print(\"quant_model_size- {}\".format(quant_model_size))\n",
    "\n",
    "\n",
    "Eva_final.update({'Ave of quant model size':quant_model_size_mean})\n",
    "Eva_final.update({'Std of quant_model_size':quant_model_size_std })\n",
    "\n",
    "quant_energy_consumption_mean = stat.mean(Quant_Energy_Consumption)\n",
    "quant_energy_consumption_std = stat.stdev(Quant_Energy_Consumption)\n",
    "quant_energy_consumption= \"{:.3f} ± {:.3f}\".format(quant_energy_consumption_mean, quant_energy_consumption_std)\n",
    "print(\"quant_energy_consumption- {}\".format(quant_energy_consumption))\n",
    "\n",
    "\n",
    "Eva_final.update({'Ave of energy consumption of quant model':quant_energy_consumption_mean })\n",
    "Eva_final.update({'Std of energy consumption of quant model':quant_energy_consumption_std})\n",
    "\n",
    "\n",
    "quant_cpu_usage_mean = stat.mean(Quant_Cpu_Usage)\n",
    "quant_cpu_usage_std = stat.stdev(Quant_Cpu_Usage)\n",
    "quant_cpu_usage= \"{:.3f} ± {:.3f}\".format(quant_cpu_usage_mean, quant_cpu_usage_std )\n",
    "print(\"quant_cpu_usage- {}\".format(quant_cpu_usage))\n",
    "\n",
    "Eva_final.update({'Ave of cpu usage of quant model':quant_cpu_usage_mean})\n",
    "Eva_final.update({'Std of cpu usage of quant model':quant_cpu_usage_std})\n",
    "\n",
    "quant_memory_usage_mean = stat.mean(Quant_Memory_Usage)\n",
    "quant_memory_usage_std = stat.stdev(Quant_Memory_Usage)\n",
    "quant_memory_usage= \"{:.3f} ± {:.3f}\".format(quant_memory_usage_mean, quant_memory_usage_std)\n",
    "print(\"quant_memory_usage- {}\".format(quant_memory_usage))\n",
    "\n",
    "Eva_final.update({'Ave of memory usage of quant model':quant_memory_usage_mean})\n",
    "Eva_final.update({'Std of memory usage of quant model':quant_memory_usage_std})\n",
    "\n",
    "#################################\n",
    "\n",
    "\n",
    "# Determing Quantization Method \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf5d5e0",
   "metadata": {},
   "source": [
    "### Recording the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f3ae59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the accuracy\n",
    "Quantization_Method='AAQ'\n",
    "\n",
    "file_name = pathresult+'/'+Quantization_Method+'Method'+'_On'+args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'.txt'\n",
    "\n",
    "with open(file_name, 'w') as f:\n",
    "    for key, value in vars(args).items():\n",
    "        f.write('%s:%s\\n'%(key, value))\n",
    "\n",
    "    for key, value in Eva_final.items():\n",
    "        f.write('%s:%s\\n'%(key, value))\n",
    "\n",
    "    for key, value in Eva_measure.items():\n",
    "        f.write('%s:%s\\n' % (key, ','.join(map(str, value))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56ba1ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_bit_proteins(dataset, state_dict, all_positive=True):\n",
    "    # Step 1: Collect layer-wise bit parameters\n",
    "    layer_bits = {}\n",
    "  \n",
    "    for key, param in state_dict.items():\n",
    "        if'quant' in key and 'bit' in key and 'fea' in key:\n",
    "            layer_name = key.split('.quant_bit_fea')[0]\n",
    "            layer_bits[layer_name] = param.abs().round() - 1\n",
    "\n",
    "    # Step 2: Per-graph analysis\n",
    "    for i, data in enumerate(dataset):\n",
    "        #print(f\"\\n===== Analyzing Graph {i+1}/{len(dataset)} =====\")\n",
    "        edge_index = data.edge_index\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, data.x.size(0)).cpu()\n",
    "        \n",
    "        # Step 3: Per-layer analysis within current graph\n",
    "        for layer_name, bits in layer_bits.items():\n",
    "            # Skip if bits tensor doesn't match current graph size\n",
    "            if bits.size(0) != deg.size(0):\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nLayer {layer_name}:\")\n",
    "            print(f\"Avg bits: {bits.mean().item():.2f}\")\n",
    "            \n",
    "            # Bit-degree correlation\n",
    "            for bit_val in range(0, 9):\n",
    "                mask = (bits == bit_val)\n",
    "                if mask.sum() > 0:\n",
    "                    avg_deg = deg[mask].mean().item()\n",
    "                    print(f\"  {bit_val}-bit nodes: {mask.sum().item()} nodes, Avg Degree={avg_deg:.1f}\")\n",
    "    \n",
    "    # Step 4: Weight quantization analysis\n",
    "    weight_bits = []\n",
    "    for key, param in state_dict.items():\n",
    "        if'quant' in key and 'bit' in key and 'fea' in key:\n",
    "            bits = param.abs().round() - 1\n",
    "            weight_bits.append(bits.mean().item())\n",
    "    \n",
    "    print(\"\\n===== Weight Quantization Summary =====\")\n",
    "    if weight_bits:\n",
    "        print(f\"Avg weight bits: {sum(weight_bits)/len(weight_bits):.2f}\")\n",
    "    else:\n",
    "        print(\"No weight quantization parameters found\")\n",
    "    \n",
    "    print(\"Analysis complete\")\n",
    "    return {sum(weight_bits)/len(weight_bits)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30546f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Weight Quantization Summary =====\n",
      "Avg weight bits: 3.58\n",
      "Analysis complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{3.5833333333333335}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50 Epoch-bit =4\n",
    "model=qGIN(dataset, args.num_layers,hidden_units=args.hidden_units,bit=args.bit, is_q=True,\n",
    "                    num_deg=args.num_deg,\n",
    "                    uniform=args.uniform).to(device)\n",
    "quant_model_path=pathcheck+'/'+args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'quantized.pth.tar'\n",
    "state = torch.load(quant_model_path)\n",
    "dict=state['state_dict']\n",
    "analysis_bit_proteins(dataset, model.state_dict(), all_positive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b38f143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Weight Quantization Summary =====\n",
      "Avg weight bits: 3.58\n",
      "Analysis complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{3.5833333333333335}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save model by Danny bit=8\n",
    "model=qGIN(dataset, args.num_layers,hidden_units=args.hidden_units,bit=args.bit, is_q=True,\n",
    "                    num_deg=args.num_deg,\n",
    "                    uniform=args.uniform).to(device)\n",
    "quant_model_path= args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'quantized.pth.tar'\n",
    "state = torch.load(quant_model_path,map_location=torch.device('cpu'))\n",
    "dict=state['state_dict']\n",
    "analysis_bit_proteins(dataset, model.state_dict(), all_positive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a60eb3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIN_BBBP_8bitquantized.pth.tar\n",
      "\n",
      "===== Weight Quantization Summary =====\n",
      "Avg weight bits: 7.58\n",
      "Analysis complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{7.583333333333333}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save model by Danny bit=8\n",
    "bit=8\n",
    "model=qGIN(dataset, args.num_layers,hidden_units=args.hidden_units,bit=bit, is_q=True,\n",
    "                    num_deg=args.num_deg,\n",
    "                    uniform=args.uniform).to(device)\n",
    "quant_model_path= args.model+'_'+dataset_name+'_'+str(bit)+'bit'+'quantized.pth.tar'\n",
    "print(quant_model_path)\n",
    "state = torch.load(quant_model_path,map_location=torch.device('cpu'))\n",
    "dict=state['state_dict']\n",
    "analysis_bit_proteins(dataset, model.state_dict(), all_positive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3886e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
