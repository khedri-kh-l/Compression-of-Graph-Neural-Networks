{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efd0e7ff",
   "metadata": {},
   "source": [
    "Grain Pruning Method on Graph Classification Task of Proteins Dataset\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8afdc3",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09f0b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "import statistics as stat\n",
    "import psutil\n",
    "import itertools\n",
    "import tracemalloc\n",
    "import gc\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sparse_softmax import Sparsemax\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.pool.topk_pool import topk, filter_adj\n",
    "from torch_geometric.utils import softmax, dense_to_sparse, add_remaining_self_loops\n",
    "from torch_scatter import scatter_add\n",
    "from torch_sparse import spspmm, coalesce\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7b2210",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2fc88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da55036",
   "metadata": {},
   "source": [
    "### Sparsity\n",
    "- The sparsity is the parameter that is determines the rate of pruning across the layer. It is a value in range(0,0.1,1). This parameter is fixed for this notebook and change for remaining experiment. We determine it before training process. Here is all values of sparsities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "366057a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1, 0.2, 0.3, 0.4,  0.5, 0.6 , 0.7,  0.8, 0.9\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e0c25",
   "metadata": {},
   "source": [
    "### Functions for pruning and loading pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "124e8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## New \n",
    "\n",
    "\n",
    "def fine_grained_prune(tensor: torch.Tensor, sparsity : float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    magnitude-based pruning for single tensor\n",
    "    :param tensor: torch.(cuda.)Tensor, weight of conv/fc layer\n",
    "    :param sparsity: float, pruning sparsity\n",
    "        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n",
    "    :return:\n",
    "        torch.(cuda.)Tensor, mask for zeros\n",
    "    \"\"\"\n",
    "    sparsity = min(max(0.0, sparsity), 1.0)\n",
    "    if sparsity == 1.0:\n",
    "        tensor.zero_()\n",
    "        return torch.zeros_like(tensor)\n",
    "    elif sparsity == 0.0:\n",
    "        return torch.ones_like(tensor)\n",
    "\n",
    "    num_elements = tensor.numel()\n",
    "\n",
    "    num_zeros = round(num_elements * sparsity)\n",
    "    importance = tensor.abs()\n",
    "    threshold = importance.view(-1).kthvalue(num_zeros).values\n",
    "    mask = torch.gt(importance, threshold)\n",
    "    tensor.mul_(mask)\n",
    "\n",
    "    return mask\n",
    "\n",
    "class FineGrainedPruner:\n",
    "    def __init__(self, model, sparsity_dict):\n",
    "        self.masks = FineGrainedPruner.prune(model, sparsity_dict)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def apply(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in self.masks:\n",
    "                param *= self.masks[name]\n",
    "\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def prune(model, sparsity_dict):\n",
    "        masks = dict()\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.dim() > 1: # we only prune conv and fc weights\n",
    "                if isinstance(sparsity_dict, dict):\n",
    "                    masks[name] = fine_grained_prune(param, sparsity_dict[name])\n",
    "                else:\n",
    "                    assert(sparsity_dict < 1 and sparsity_dict >= 0)\n",
    "                    if sparsity_dict > 0:\n",
    "                        masks[name] = fine_grained_prune(param, sparsity_dict)\n",
    "        return masks\n",
    "    \n",
    "\n",
    "\n",
    "def state_sparse_model(model, eval_acc=None, epoch=None):\n",
    "    state_dict = model.state_dict()\n",
    "    compressed_state = {}\n",
    "    \n",
    "    for k, v in state_dict.items():\n",
    "        if torch.is_tensor(v):\n",
    "            mask = v != 0\n",
    "            if mask.any():  # Only compress if there are non-zeros\n",
    "                compressed_state[k] = {\n",
    "                    'shape': v.shape,\n",
    "                    'values': v[mask]  # Store only non-zero values\n",
    "                }\n",
    "            else:\n",
    "                compressed_state[k] = v  # Keep original if all zeros\n",
    "        else:\n",
    "            compressed_state[k] = v\n",
    "    \n",
    "    return {'net': compressed_state, 'epoch': epoch, 'acc': eval_acc}\n",
    "\n",
    "\n",
    "def load_sparse_model(state_path, original_model):\n",
    "    \"\"\"\n",
    "    Loads a pruned model, converting any sparse tensors to dense format.\n",
    "    \"\"\"\n",
    "    # Load the saved state_dict\n",
    "    saved_state = torch.load(state_path)\n",
    "    saved_weights = saved_state['net']\n",
    "    \n",
    "    # Get the model's current state_dict\n",
    "    model_state = original_model.state_dict()\n",
    "    \n",
    "    for key in saved_weights:\n",
    "        if key in model_state:\n",
    "            # If the saved tensor is sparse, convert it to dense\n",
    "            if isinstance(saved_weights[key], torch.Tensor) and saved_weights[key].is_sparse:\n",
    "                model_state[key] = saved_weights[key].to_dense()\n",
    "            # If it's a custom compressed format (shape + values), reconstruct it\n",
    "            elif isinstance(saved_weights[key], dict) and 'shape' in saved_weights[key]:\n",
    "                dense_tensor = torch.zeros(saved_weights[key]['shape'], dtype=model_state[key].dtype)\n",
    "                values = saved_weights[key]['values']\n",
    "                if 'indices' in saved_weights[key]:  # If indices were saved\n",
    "                    dense_tensor[saved_weights[key]['indices']] = values\n",
    "                else:  # If not, assume sequential filling\n",
    "                    dense_tensor.view(-1)[:len(values)] = values\n",
    "                model_state[key] = dense_tensor\n",
    "            else:  # Normal dense tensor\n",
    "                model_state[key] = saved_weights[key]\n",
    "    \n",
    "    # Load the reconstructed state_dict\n",
    "    original_model.load_state_dict(model_state, strict=False)\n",
    "    return original_model\n",
    "\n",
    "def load_and_evaluate_pruned_model(args,model_path):\n",
    "    \"\"\"\n",
    "    This function loads the pruned model from disk and evaluates it.\n",
    "    \"\"\"\n",
    "    # Instantiate the model\n",
    "    model = Model(args)\n",
    "\n",
    "\n",
    "    # Load the pruned model\n",
    "    model = load_sparse_model(model_path, model)\n",
    "    print(\"Pruned model loaded.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7ee43e",
   "metadata": {},
   "source": [
    "### Functions for Mmeasuring criterias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c59894ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_parameters(model: nn.Module, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the total number of parameters of model\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    num_counted_elements = 0\n",
    "    for param in model.parameters():\n",
    "        if count_nonzero_only:\n",
    "            num_counted_elements += param.count_nonzero()\n",
    "        else:\n",
    "            num_counted_elements += param.numel()\n",
    "    return num_counted_elements\n",
    "\n",
    "# Function to get CPU usage\n",
    "def get_cpu_usage():\n",
    "    return psutil.cpu_percent(interval=1)\n",
    "\n",
    "\n",
    "\n",
    "# Function to approximate power consumption (Assume some average power usage per CPU percentage point)\n",
    "def estimate_power_usage(cpu_usage):\n",
    "    base_power_usage = 10  # Assumed base power usage in watts\n",
    "    power_per_percent = 0.5  # Assumed additional watts per CPU usage percent\n",
    "    return base_power_usage + (power_per_percent * cpu_usage)\n",
    "\n",
    "# The model size based on the number of parameters\n",
    "def get_model_size_param(model: nn.Module, data_width=32, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the model size in bits\n",
    "    :param data_width: #bits per element\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    return get_num_parameters(model, count_nonzero_only) * data_width\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dbcaf5",
   "metadata": {},
   "source": [
    "### Setting Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a2ea1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1cb10726a10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=777, help='random seed')\n",
    "parser.add_argument('--batch_size', type=int, default=512, help='batch size')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.001, help='weight decay')\n",
    "parser.add_argument('--nhid', type=int, default=128, help='hidden size')\n",
    "parser.add_argument('--sample_neighbor', type=bool, default=True, help='whether sample neighbors')\n",
    "parser.add_argument('--sparse_attention', type=bool, default=True, help='whether use sparse attention')\n",
    "parser.add_argument('--structure_learning', type=bool, default=True, help='whether perform structure learning')\n",
    "parser.add_argument('--pooling_ratio', type=float, default=0.5, help='pooling ratio')\n",
    "parser.add_argument('--dropout_ratio', type=float, default=0.0, help='dropout ratio')\n",
    "parser.add_argument('--lamb', type=float, default=1.0, help='trade-off parameter')\n",
    "parser.add_argument('--dataset', type=str, default='PROTEINS', help='DD/PROTEINS/NCI1/NCI109/Mutagenicity/ENZYMES')\n",
    "parser.add_argument('--device', type=str, default='cpu', help='specify cuda devices')\n",
    "parser.add_argument('--epochs', type=int, default=2, help='maximum number of epochs')\n",
    "parser.add_argument('--patience', type=int, default=100, help='patience for early stopping')\n",
    "parser.add_argument('--model_name', type=str, default='HGPSL', help='-')\n",
    "\n",
    "args = parser.parse_args()\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04e4dac",
   "metadata": {},
   "source": [
    "### save path for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "276fb64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.isdir('checkpoint'):\n",
    "    os.mkdir('checkpoint')\n",
    "if not os.path.isdir(os.path.join('checkpoint', args.dataset)):\n",
    "    os.mkdir(os.path.join('checkpoint', f\"{args.dataset}\"))\n",
    "ckpt_dir = f\"./checkpoint/{args.dataset}/\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_best(ckpt_dir, epoch, state, model_name, eval_acc, is_best, is_pruned):\n",
    "    print('saving....')\n",
    "            \n",
    "    model.to(device)\n",
    "    state_save = {\n",
    "        'net':state,\n",
    "        'epoch':epoch,\n",
    "        'acc': eval_acc \n",
    "        }\n",
    "    best_pth_name = f'{args.model_name}_best.pth'\n",
    "    fine_tuned_pth_name = f'{args.model_name}_fine_tuned_best.pth'\n",
    "  \n",
    "    if is_pruned & is_best:\n",
    "        ckpt_path = os.path.join(ckpt_dir, fine_tuned_pth_name) \n",
    "        torch.save(state_save, ckpt_path)\n",
    "    \n",
    "     \n",
    "    if is_pruned== False & is_best:\n",
    "        ckpt_path = os.path.join(ckpt_dir, best_pth_name)  \n",
    "        torch.save(state_save, ckpt_path)\n",
    "                   \n",
    "            \n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b2c39",
   "metadata": {},
   "source": [
    "### Start loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df6ad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(seed=777, batch_size=512, lr=0.001, weight_decay=0.001, nhid=128, sample_neighbor=True, sparse_attention=True, structure_learning=True, pooling_ratio=0.5, dropout_ratio=0.0, lamb=1.0, dataset='PROTEINS', device='cpu', epochs=2, patience=100, model_name='HGPSL', num_classes=2, num_features=4)\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(os.path.join('data', args.dataset), name=args.dataset, use_node_attr=True)\n",
    "\n",
    "args.num_classes = dataset.num_classes\n",
    "args.num_features = dataset.num_features\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc12665",
   "metadata": {},
   "source": [
    "### Preprocessing  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "418e883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training = int(len(dataset) * 0.8)\n",
    "num_val = int(len(dataset) * 0.1)\n",
    "num_test = len(dataset) - (num_training + num_val)\n",
    "training_set, validation_set, test_set = random_split(dataset, [num_training, num_val, num_test])\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=args.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_set, batch_size=args.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9335564f",
   "metadata": {},
   "source": [
    "### Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e979cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TwoHopNeighborhood(object):\n",
    "    def __call__(self, data):\n",
    "        edge_index, edge_attr = data.edge_index, data.edge_attr\n",
    "        n = data.num_nodes\n",
    "\n",
    "        fill = 1e16\n",
    "        value = edge_index.new_full((edge_index.size(1),), fill, dtype=torch.float)\n",
    "\n",
    "        index, value = spspmm(edge_index, value, edge_index, value, n, n, n, True)\n",
    "\n",
    "        edge_index = torch.cat([edge_index, index], dim=1)\n",
    "        if edge_attr is None:\n",
    "            data.edge_index, _ = coalesce(edge_index, None, n, n)\n",
    "        else:\n",
    "            value = value.view(-1, *[1 for _ in range(edge_attr.dim() - 1)])\n",
    "            value = value.expand(-1, *list(edge_attr.size())[1:])\n",
    "            edge_attr = torch.cat([edge_attr, value], dim=0)\n",
    "            #, fill_value=fill\n",
    "            data.edge_index, edge_attr = coalesce(edge_index, edge_attr, n, n, op='min')\n",
    "            edge_attr[edge_attr >= fill] = 0\n",
    "            data.edge_attr = edge_attr\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}()'.format(self.__class__.__name__)\n",
    "\n",
    "\n",
    "class GCN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, cached=False, bias=True, **kwargs):\n",
    "        super(GCN, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "            nn.init.zeros_(self.bias.data)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = torch.matmul(x, self.weight)\n",
    "\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\n",
    "\n",
    "\n",
    "class NodeInformationScore(MessagePassing):\n",
    "    def __init__(self, improved=False, cached=False, **kwargs):\n",
    "        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes)\n",
    "\n",
    "        row, col = edge_index\n",
    "        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n",
    "        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "\n",
    "class HGPSLPool(torch.nn.Module):\n",
    "    def __init__(self, in_channels, ratio=0.8, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2):\n",
    "        super(HGPSLPool, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = ratio\n",
    "        self.sample = sample\n",
    "        self.sparse = sparse\n",
    "        self.sl = sl\n",
    "        self.negative_slop = negative_slop\n",
    "        self.lamb = lamb\n",
    "\n",
    "        self.att = Parameter(torch.Tensor(1, self.in_channels * 2))\n",
    "        nn.init.xavier_uniform_(self.att.data)\n",
    "        self.sparse_attention = Sparsemax()\n",
    "        self.neighbor_augment = TwoHopNeighborhood()\n",
    "        self.calc_information_score = NodeInformationScore()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch=None):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "\n",
    "        x_information_score = self.calc_information_score(x, edge_index, edge_attr)\n",
    "        score = torch.sum(torch.abs(x_information_score), dim=1)\n",
    "\n",
    "        # Graph Pooling\n",
    "        original_x = x\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "        x = x[perm]\n",
    "        batch = batch[perm]\n",
    "        induced_edge_index, induced_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "        # Discard structure learning layer, directly return\n",
    "        if self.sl is False:\n",
    "            return x, induced_edge_index, induced_edge_attr, batch\n",
    "\n",
    "        # Structure Learning\n",
    "        if self.sample:\n",
    "            # A fast mode for large graphs.\n",
    "            # In large graphs, learning the possible edge weights between each pair of nodes is time consuming.\n",
    "            # To accelerate this process, we sample it's K-Hop neighbors for each node and then learn the\n",
    "            # edge weights between them.\n",
    "            k_hop = 3\n",
    "            if edge_attr is None:\n",
    "                edge_attr = torch.ones((edge_index.size(1),), dtype=torch.float, device=edge_index.device)\n",
    "\n",
    "            hop_data = Data(x=original_x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "            for _ in range(k_hop - 1):\n",
    "                hop_data = self.neighbor_augment(hop_data)\n",
    "            hop_edge_index = hop_data.edge_index\n",
    "            hop_edge_attr = hop_data.edge_attr\n",
    "            new_edge_index, new_edge_attr = filter_adj(hop_edge_index, hop_edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "            new_edge_index, new_edge_attr = add_remaining_self_loops(new_edge_index, new_edge_attr, 0, x.size(0))\n",
    "            row, col = new_edge_index\n",
    "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
    "            weights = F.leaky_relu(weights, self.negative_slop) + new_edge_attr * self.lamb\n",
    "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
    "            adj[row, col] = weights\n",
    "            new_edge_index, weights = dense_to_sparse(adj)\n",
    "            row, col = new_edge_index\n",
    "            if self.sparse:\n",
    "                new_edge_attr = self.sparse_attention(weights, row)\n",
    "            else:\n",
    "                new_edge_attr = softmax(weights, row, x.size(0))\n",
    "            # filter out zero weight edges\n",
    "            adj[row, col] = new_edge_attr\n",
    "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
    "            # release gpu memory\n",
    "            del adj\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            # Learning the possible edge weights between each pair of nodes in the pooled subgraph, relative slower.\n",
    "            if edge_attr is None:\n",
    "                induced_edge_attr = torch.ones((induced_edge_index.size(1),), dtype=x.dtype,\n",
    "                                               device=induced_edge_index.device)\n",
    "            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "            shift_cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "            cum_num_nodes = num_nodes.cumsum(dim=0)\n",
    "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
    "            # Construct batch fully connected graph in block diagonal matirx format\n",
    "            for idx_i, idx_j in zip(shift_cum_num_nodes, cum_num_nodes):\n",
    "                adj[idx_i:idx_j, idx_i:idx_j] = 1.0\n",
    "            new_edge_index, _ = dense_to_sparse(adj)\n",
    "            row, col = new_edge_index\n",
    "\n",
    "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
    "            weights = F.leaky_relu(weights, self.negative_slop)\n",
    "            adj[row, col] = weights\n",
    "            induced_row, induced_col = induced_edge_index\n",
    "\n",
    "            adj[induced_row, induced_col] += induced_edge_attr * self.lamb\n",
    "            weights = adj[row, col]\n",
    "            if self.sparse:\n",
    "                new_edge_attr = self.sparse_attention(weights, row)\n",
    "            else:\n",
    "                new_edge_attr = softmax(weights, row, x.size(0))\n",
    "            # filter out zero weight edges\n",
    "            adj[row, col] = new_edge_attr\n",
    "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
    "            # release gpu memory\n",
    "            del adj\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return x, new_edge_index, new_edge_attr, batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4e02a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Model, self).__init__()\n",
    "        self.args = args\n",
    "        self.num_features = args.num_features\n",
    "        self.nhid = args.nhid\n",
    "        self.num_classes = args.num_classes\n",
    "        self.pooling_ratio = args.pooling_ratio\n",
    "        self.dropout_ratio = args.dropout_ratio\n",
    "        self.sample = args.sample_neighbor\n",
    "        self.sparse = args.sparse_attention\n",
    "        self.sl = args.structure_learning\n",
    "        self.lamb = args.lamb\n",
    "\n",
    "        self.conv1 = GCNConv(self.num_features, self.nhid)\n",
    "        self.conv2 = GCN(self.nhid, self.nhid)\n",
    "        self.conv3 = GCN(self.nhid, self.nhid)\n",
    "\n",
    "        self.pool1 = HGPSLPool(self.nhid, self.pooling_ratio, self.sample, self.sparse, self.sl, self.lamb)\n",
    "        self.pool2 = HGPSLPool(self.nhid, self.pooling_ratio, self.sample, self.sparse, self.sl, self.lamb)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(self.nhid * 2, self.nhid)\n",
    "        self.lin2 = torch.nn.Linear(self.nhid, self.nhid // 2)\n",
    "        self.lin3 = torch.nn.Linear(self.nhid // 2, self.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        edge_attr = None\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x, edge_index, edge_attr, batch = self.pool1(x, edge_index, edge_attr, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x, edge_index, edge_attr, batch = self.pool2(x, edge_index, edge_attr, batch)\n",
    "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv3(x, edge_index, edge_attr))\n",
    "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(x1) + F.relu(x2) + F.relu(x3)\n",
    "\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n",
    "        x = F.log_softmax(self.lin3(x), dim=-1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08d0aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(args)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c056424",
   "metadata": {},
   "source": [
    "### Required functions  for training with global pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21600921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader):\n",
    "    loss_train = 0.0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for i, data in enumerate(train_loader):\n",
    "            #data = data.to(args.device)\n",
    "            out = model(data)\n",
    "            loss = F.nll_loss(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "            pred = out.max(dim=1)[1]\n",
    "            correct += pred.eq(data.y).sum().item()\n",
    "    acc_train = correct / len(train_loader.dataset)\n",
    "    \n",
    "    return   loss_train,acc_train      \n",
    "\n",
    "\n",
    "def compute_test(loader, model):\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    loss_test = 0.0\n",
    "    for data in loader:\n",
    "        #data = data.to(args.device)\n",
    "        out = model(data)\n",
    "        pred = out.max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "        loss_test += F.nll_loss(out, data.y).item()\n",
    "    return correct / len(loader.dataset), loss_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3761e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, train_loader, callbacks = None):\n",
    "    min_loss = 1e10\n",
    "    patience_cnt = 0\n",
    "    val_loss_values = []\n",
    "    best_epoch = 0\n",
    "    if callbacks == None:\n",
    "        is_prune=False\n",
    "    \n",
    "    else:\n",
    "        is_prune= True\n",
    "\n",
    "    #model.train()\n",
    "    t = time.time()\n",
    "    for epoch in range(args.epochs):\n",
    "        #loss_train = 0.0\n",
    "        #correct = 0\n",
    "        loss_train,acc_train =train(model, train_loader)\n",
    "        \n",
    "        acc_val, loss_val = compute_test(val_loader, model)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch: {:04d}'.format(epoch ), 'loss_train: {:.6f}'.format(loss_train),\n",
    "                  'acc_train: {:.6f}'.format(acc_train), 'loss_val: {:.6f}'.format(loss_val),\n",
    "                  'acc_val: {:.6f}'.format(acc_val), 'time: {:.6f}s'.format(time.time() - t))\n",
    "\n",
    "        val_loss_values.append(loss_val)\n",
    "        if val_loss_values[-1] < min_loss:\n",
    "            min_loss = val_loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            patience_cnt = 0\n",
    "            is_best=True\n",
    "            if is_prune:\n",
    "                non_zero_state_dict=state_sparse_model(model,acc_val, epoch)\n",
    "                save_best(ckpt_dir, epoch, non_zero_state_dict, args.model_name, acc_val, is_best, is_prune)\n",
    "            else:\n",
    "                save_best(ckpt_dir, epoch, model.state_dict(), args.model_name, acc_val, is_best, is_prune)\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "\n",
    "        if patience_cnt == args.patience:\n",
    "            break\n",
    "        \n",
    "        if callbacks is not None:\n",
    "            for callback in callbacks:\n",
    "                callback()\n",
    "\n",
    "     \n",
    "    \n",
    "\n",
    "    print('Optimization Finished! Total time elapsed: {:.6f}'.format(time.time() - t))\n",
    "    \n",
    "\n",
    "    return best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9a62110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## New Version\n",
    "def run(model, train_loader, callbacks=None, is_prune=False):  # Add is_prune parameter\n",
    "    min_loss = 1e10\n",
    "    patience_cnt = 0\n",
    "    val_loss_values = []\n",
    "    best_epoch = 0\n",
    "    \n",
    "    t = time.time()\n",
    "    for epoch in range(args.epochs):\n",
    "        loss_train, acc_train = train(model, train_loader)\n",
    "        acc_val, loss_val = compute_test(val_loader, model)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch: {:04d}'.format(epoch), 'loss_train: {:.6f}'.format(loss_train),\n",
    "                  'acc_train: {:.6f}'.format(acc_train), 'loss_val: {:.6f}'.format(loss_val),\n",
    "                  'acc_val: {:.6f}'.format(acc_val), 'time: {:.6f}s'.format(time.time() - t))\n",
    "        if callbacks is not None:\n",
    "            for callback in callbacks:\n",
    "                callback()\n",
    "        val_loss_values.append(loss_val)\n",
    "        if val_loss_values[-1] < min_loss:\n",
    "            min_loss = val_loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            patience_cnt = 0\n",
    "            if is_prune:\n",
    "                # Use sparse state dict for pruned models\n",
    "                state_dict = state_sparse_model(model, acc_val, epoch)\n",
    "                \n",
    "            else:\n",
    "                state_dict = model.state_dict()\n",
    "            save_best(ckpt_dir, epoch, state_dict, args.model_name, acc_val, True, is_prune)\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "\n",
    "        if patience_cnt == args.patience:\n",
    "            break\n",
    "\n",
    "    print('Optimization Finished! Total time elapsed: {:.6f}'.format(time.time() - t))\n",
    "    return best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1f0c75",
   "metadata": {},
   "source": [
    "###  Pruning the Model and Re-Evaluate the Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2dae8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting Sparsity\n",
    "sparsity=0.9\n",
    "# The number of epochs  \n",
    "args.epochs=5\n",
    "# The number of iterations\n",
    "num_iterations=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "78f08adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are all list of criteria for measurements. \n",
    "# We collect all desired datas of each list across iterations. \n",
    "# Then, we compute average and std of each list.\n",
    "Eva_final=dict()\n",
    "\n",
    "#Base model\n",
    "Base_model_accuracy=[]\n",
    "T_base_model=[]\n",
    "Num_parm_base_model=[]\n",
    "Base_model_size=[]\n",
    "Base_Energy_Consumption=[]\n",
    "Base_Cpu_Usage=[]\n",
    "Base_Memory_Usage=[]\n",
    "\n",
    "#Pruned model\n",
    "Pruned_model_accuracy=[]\n",
    "T_pruned_model=[]\n",
    "Num_parm_pruned_model=[]\n",
    "Pruned_model_size=[]\n",
    "Pruned_Energy_Consumption=[]\n",
    "Pruned_Cpu_Usage=[]\n",
    "Pruned_Memory_Usage=[]\n",
    "\n",
    "#Pruned and finetune model\n",
    "Pruned_finetune_model_accuracy=[]\n",
    "T_pruned_finetune_model=[]\n",
    "Num_parm_pruned_finetune_model=[]\n",
    "Pruned_finetune_model_size=[]\n",
    "Pruned_finetune_Energy_Consumption=[]\n",
    "Pruned_finetune_Cpu_Usage=[]\n",
    "Pruned_finetune_Memory_Usage=[]\n",
    "\n",
    "Eva_measure={'base model accuracy':Base_model_accuracy,\n",
    "            'time inference of base model':T_base_model,\n",
    "            'number parmameters of base model':Num_parm_base_model,\n",
    "            'base model size':Base_model_size,\n",
    "            'energy consumption of base model':Base_Energy_Consumption,\n",
    "            'cpu usage of base model':Base_Cpu_Usage,\n",
    "            'memory usage of base model':Base_Memory_Usage,\n",
    "            'pruned model accuracy': Pruned_model_accuracy,\n",
    "            'time inference of pruned model':T_pruned_model,\n",
    "            'number parmameters of pruned model':Num_parm_pruned_model,\n",
    "            'pruned model size':Pruned_model_size,\n",
    "            'energy consumption of pruned model':Pruned_Energy_Consumption,\n",
    "            'cpu usage of pruned model':Pruned_Cpu_Usage,\n",
    "            'memory usage of pruned model':Pruned_Memory_Usage,\n",
    "            'pruned finetune model accuracy':Pruned_finetune_model_accuracy,\n",
    "            'time inference of pruned finetune model':T_pruned_finetune_model,\n",
    "            'number parmameters of pruned finetune model':Num_parm_pruned_finetune_model,\n",
    "            'pruned finetune model size':Pruned_finetune_model_size,\n",
    "            'energy consumption of pruned_finetune model':Pruned_finetune_Energy_Consumption,\n",
    "            'cpu usage of pruned_finetune model':Pruned_finetune_Cpu_Usage,\n",
    "            'memory usage of pruned_finetune model':Pruned_finetune_Memory_Usage}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb05927",
   "metadata": {},
   "source": [
    "### Training, Pruning, Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4d378af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________\n",
      " This is iteration:1\n",
      "Training and evaluation before pruning \n",
      "Starting training...\n",
      "Epoch: 0000 loss_train: 1.348095 acc_train: 0.579775 loss_val: 0.649441 acc_val: 0.621622 time: 8.404190s\n",
      "saving....\n",
      "saving....\n",
      "Optimization Finished! Total time elapsed: 41.447165\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.70%\n",
      "base model has size=306667.00 bit\n",
      "The time inference of base model is =2.5716869000007136\n",
      "The number of parametrs of base model is:75412\n",
      "Energy Consumption : 44.233\n",
      "total memory usage of base model':37843 \n",
      "cpu usage of base model':0.800 %\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.70%\n",
      "pruned model has size=36299.00\n",
      "The time inference of pruned model is =2.5561433999992005\n",
      "The number of parametrs of pruned model is:8020\n",
      "Energy Consumption : 36.042\n",
      "total memory usage of pruned model':39557 \n",
      "cpu usage of pruned model':1.600 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Epoch: 0000 loss_train: 1.361547 acc_train: 0.579775 loss_val: 0.671353 acc_val: 0.621622 time: 8.996532s\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "Optimization Finished! Total time elapsed: 41.866561\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.70%\n",
      "pruned_finetune model has size=36827.00 \n",
      "The time inference of pruned_finetune model is =2.5783651000001555\n",
      "The number of parametrs of pruned_finetune model is:8058\n",
      "Energy Consumption of pruned_finetune model: 25.784\n",
      "total memory usage of pruned_finetune model':38113 \n",
      "cpu usage of pruned_finetune model':11.300 %\n",
      "_________________________________________\n",
      " This is iteration:2\n",
      "Training and evaluation before pruning \n",
      "Starting training...\n",
      "Epoch: 0000 loss_train: 1.418565 acc_train: 0.498876 loss_val: 0.651320 acc_val: 0.621622 time: 7.426072s\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "Optimization Finished! Total time elapsed: 37.930261\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.70%\n",
      "base model has size=306667.00 bit\n",
      "The time inference of base model is =2.5558590999989974\n",
      "The number of parametrs of base model is:75416\n",
      "Energy Consumption : 25.559\n",
      "total memory usage of base model':38741 \n",
      "cpu usage of base model':1.600 %\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.30%\n",
      "pruned model has size=36363.00\n",
      "The time inference of pruned model is =2.5557279999993625\n",
      "The number of parametrs of pruned model is:8024\n",
      "Energy Consumption : 25.557\n",
      "total memory usage of pruned model':39626 \n",
      "cpu usage of pruned model':1.500 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Epoch: 0000 loss_train: 1.391156 acc_train: 0.421348 loss_val: 0.695135 acc_val: 0.405405 time: 7.454203s\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "Optimization Finished! Total time elapsed: 38.054498\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.31%\n",
      "pruned_finetune model has size=36891.00 \n",
      "The time inference of pruned_finetune model is =2.605309299999135\n",
      "The number of parametrs of pruned_finetune model is:8061\n",
      "Energy Consumption of pruned_finetune model: 26.053\n",
      "total memory usage of pruned_finetune model':38153 \n",
      "cpu usage of pruned_finetune model':1.500 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_iterations):\n",
    "    \n",
    "        print('_________________________________________')\n",
    "        print(f' This is iteration:{i+1}')\n",
    "        print(f'Training and evaluation before pruning ')\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "        Eva=dict() # It is a dictionary to arrange output of this iteration\n",
    "\n",
    "        model = Model(args)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        best_model = run(model, train_loader)\n",
    "\n",
    "        #### load the best model from disk\n",
    "        base_model_path = os.path.join(ckpt_dir, f'{args.model_name}_best.pth') \n",
    "        checkpoint = torch.load(base_model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "     \n",
    "        recover_model = lambda: model.model.load_state_dict(checkpoint['net'])\n",
    "\n",
    "        # Start monitoring CPU and memory usage, model size, number of parametes, time inference and  power consumption\n",
    "        gc.collect()\n",
    "        time.sleep(5)  # Add a 5-second delay to stabilize the initial state\n",
    "        tracemalloc.start()  # Start tracking memory allocations\n",
    "        snapshot_before = tracemalloc.take_snapshot()#take a snapshot of the current memory state before starting the measurement.\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        initial_cpu_usage = get_cpu_usage()\n",
    "        power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "\n",
    "        base_model_accuracy, test_loss = compute_test(test_loader, model)\n",
    "\n",
    "        base_cpu_usage = get_cpu_usage()\n",
    "        t1 = time.perf_counter()\n",
    "        t_base_model=t1-t0\n",
    "\n",
    "        snapshot_after = tracemalloc.take_snapshot()\n",
    "        tracemalloc.stop()\n",
    "        top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "        base_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "        base_energy_consumption = power_usage * t_base_model\n",
    "        base_model_size = os.path.getsize(base_model_path)\n",
    "        num_parm_base_model=get_num_parameters(model, count_nonzero_only=True)\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5) \n",
    "\n",
    "        print(f'*****Results of base model*********')\n",
    "\n",
    "        print(f\"base model has accuracy on test set={base_model_accuracy:.2f}%\")\n",
    "        print(f\"base model has size={base_model_size:.2f} bit\")\n",
    "        print(f\"The time inference of base model is ={t_base_model}\") \n",
    "        print(f\"The number of parametrs of base model is:{num_parm_base_model}\") \n",
    "\n",
    "        print(f\"Energy Consumption : {base_energy_consumption:.3f}\")\n",
    "        print(f\"total memory usage of base model':{base_total_memory_diff} \")\n",
    "        print(f\"cpu usage of base model':{base_cpu_usage:.3f} %\")\n",
    "\n",
    "\n",
    "        #Update Eva dictionary\n",
    "        Eva.update({'base model accuracy': base_model_accuracy,\n",
    "                'time inference of base model': t_base_model,\n",
    "                'number parmameters of base model': num_parm_base_model,\n",
    "                'size of base model': base_model_size, \n",
    "                'energy consumption of base model':base_energy_consumption,\n",
    "                'total memory usage of base model':base_total_memory_diff,\n",
    "                'cpu usage of base model':base_cpu_usage\n",
    "               })\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5)  \n",
    "\n",
    "        #### Pruning of the Model\n",
    "\n",
    "        # Pruned model path\n",
    "        pth_name = f\"pruned_model.pth\"   \n",
    "        ckpt_pruned_path = os.path.join(ckpt_dir, pth_name)  \n",
    "        # Apply pruning method\n",
    "        pruner = FineGrainedPruner(model, sparsity)\n",
    "        pruner.apply(model)\n",
    "        # Remove non-zero wieghts \n",
    "        non_zero_state = state_sparse_model(model)\n",
    "        # Save non-zero weights \n",
    "        torch.save(non_zero_state , ckpt_pruned_path)\n",
    "        # Loading ptuned model from disk\n",
    "        pruned_model=load_and_evaluate_pruned_model(args,ckpt_pruned_path)\n",
    "\n",
    "\n",
    "        print('****************Result of pruning ******************')\n",
    "\n",
    "\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5)  \n",
    "        tracemalloc.start()  \n",
    "        snapshot_before = tracemalloc.take_snapshot()\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        initial_cpu_usage = get_cpu_usage()\n",
    "        power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "\n",
    "        pruned_model_accuracy, test_loss = compute_test(test_loader, pruned_model)\n",
    "\n",
    "        pruned_cpu_usage = get_cpu_usage()\n",
    "        t1 = time.perf_counter()\n",
    "        t_pruned_model=t1-t0\n",
    "\n",
    "        snapshot_after = tracemalloc.take_snapshot()\n",
    "        tracemalloc.stop()\n",
    "        top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "        pruned_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "        pruned_energy_consumption = power_usage * t_pruned_model\n",
    "        pruned_model_size = os.path.getsize(ckpt_pruned_path)\n",
    "        num_parm_pruned_model=get_num_parameters(pruned_model, count_nonzero_only=True)\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5)  \n",
    "\n",
    "        ###### Report of pruning \n",
    "        print(f\"pruned model has accuracy on test set={pruned_model_accuracy:.2f}%\")\n",
    "        print(f\"pruned model has size={pruned_model_size:.2f}\")\n",
    "        print(f\"The time inference of pruned model is ={t_pruned_model}\") \n",
    "        print(f\"The number of parametrs of pruned model is:{num_parm_pruned_model}\") \n",
    "\n",
    "        print(f\"Energy Consumption : {pruned_energy_consumption:.3f}\")\n",
    "        print(f\"total memory usage of pruned model':{pruned_total_memory_diff} \")\n",
    "        print(f\"cpu usage of pruned model':{pruned_cpu_usage:.3f} %\")\n",
    "\n",
    "\n",
    "        #Update Eva dictionary\n",
    "        Eva.update({'pruned model accuracy': pruned_model_accuracy,\n",
    "                'time inference of pruned model': t_pruned_model,\n",
    "                'number parmameters of pruned model': num_parm_pruned_model,\n",
    "                'size of pruned model': pruned_model_size, \n",
    "                'energy consumption of pruned model':pruned_energy_consumption,\n",
    "                'total memory usage of pruned model':pruned_total_memory_diff,\n",
    "                'cpu usage of pruned model':pruned_cpu_usage\n",
    "               })\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5)   \n",
    "\n",
    "        print('________*******************************_____________')\n",
    "        print(f'Finetuning Pruned Sparse Model')\n",
    "        \n",
    "        # Fine-tuning model\n",
    "        #best_model_pruned=run(model, train_loader, callbacks=[lambda:pruner.apply(model)])\n",
    "        best_model_pruned = run(model, train_loader, callbacks=[lambda:pruner.apply(model)], \n",
    "                       is_prune=True)\n",
    "\n",
    "        #### load the best fine-tune model\n",
    "        fine_tuned_pth_name=f'{args.model_name}_fine_tuned_best.pth'\n",
    "        fine_tuned_model_path = os.path.join(ckpt_dir, fine_tuned_pth_name)\n",
    "\n",
    "        # Assuming `original_model` is the model you used to create the sparse version\n",
    "        original_model =model # Define your original model here\n",
    "\n",
    "        # Load your sparse model\n",
    "        sparse_model = load_sparse_model(fine_tuned_model_path, original_model)\n",
    "\n",
    "        # Now you can use the sparse model for evaluation or further training\n",
    "        print('****************Result of fine-tuning of pruned model ******************')\n",
    "\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5)  \n",
    "        tracemalloc.start() \n",
    "        snapshot_before = tracemalloc.take_snapshot()\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        initial_cpu_usage = get_cpu_usage()\n",
    "        power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "\n",
    "        pruned_finetune_model_accuracy, test_loss = compute_test(test_loader,sparse_model)\n",
    "\n",
    "\n",
    "        pruned_finetune_cpu_usage = get_cpu_usage()\n",
    "        t1 = time.perf_counter()\n",
    "        t_pruned_finetune_model=t1-t0\n",
    "\n",
    "        snapshot_after = tracemalloc.take_snapshot()\n",
    "        tracemalloc.stop()\n",
    "        top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "        pruned_finetune_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "        pruned_finetune_energy_consumption = power_usage * t_pruned_finetune_model\n",
    "        pruned_finetune_model_size = os.path.getsize( fine_tuned_model_path)\n",
    "        num_parm_pruned_finetune_model=get_num_parameters(sparse_model, count_nonzero_only=True)\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5)  # Add a 5-second delay to stabilize the initial state    \n",
    "\n",
    "        ###### Report  \n",
    "\n",
    "        print(f\"pruned_finetune model has accuracy on test set={pruned_finetune_model_accuracy:.2f}%\")\n",
    "        print(f\"pruned_finetune model has size={pruned_finetune_model_size:.2f} \")\n",
    "        print(f\"The time inference of pruned_finetune model is ={t_pruned_finetune_model}\") \n",
    "        print(f\"The number of parametrs of pruned_finetune model is:{num_parm_pruned_finetune_model}\") \n",
    "\n",
    "        print(f\"Energy Consumption of pruned_finetune model: {pruned_finetune_energy_consumption:.3f}\")\n",
    "        print(f\"total memory usage of pruned_finetune model':{pruned_finetune_total_memory_diff} \")\n",
    "        print(f\"cpu usage of pruned_finetune model':{pruned_finetune_cpu_usage:.3f} %\")\n",
    "\n",
    "\n",
    "        #Update my Eva dictionary\n",
    "        Eva.update({'pruned and finetune model accuracy': pruned_finetune_model_accuracy,\n",
    "                'time inference of pruned and finetune model': t_pruned_finetune_model,\n",
    "                'number parmameters of pruned and finetune model': num_parm_pruned_finetune_model,\n",
    "                'size of pruned and finetune model': pruned_finetune_model_size, \n",
    "                'energy consumption of pruned and finetune model':pruned_finetune_energy_consumption,\n",
    "                'total memory usage of pruned and finetune model':pruned_finetune_total_memory_diff,\n",
    "                'cpu usage of pruned and finetune model':pruned_finetune_cpu_usage\n",
    "               })\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5) \n",
    "\n",
    "\n",
    "        Base_model_accuracy.append(Eva['base model accuracy'])\n",
    "        T_base_model.append(Eva['time inference of base model'])\n",
    "        Num_parm_base_model.append(int(Eva['number parmameters of base model']))\n",
    "        Base_model_size.append(int(Eva['size of base model']))\n",
    "        Base_Energy_Consumption.append(Eva['energy consumption of base model'])\n",
    "        Base_Cpu_Usage.append(Eva['cpu usage of base model'])\n",
    "        Base_Memory_Usage.append(Eva['total memory usage of base model'])\n",
    "\n",
    "        Pruned_model_accuracy.append(Eva['pruned model accuracy'])\n",
    "        T_pruned_model.append(Eva['time inference of pruned model'])\n",
    "        Num_parm_pruned_model.append(int(Eva['number parmameters of pruned model']))\n",
    "        Pruned_model_size.append(int(Eva['size of pruned model']))\n",
    "        Pruned_Energy_Consumption.append(Eva['energy consumption of pruned model'])\n",
    "        Pruned_Cpu_Usage.append(Eva['cpu usage of pruned model'])\n",
    "        Pruned_Memory_Usage.append(Eva['total memory usage of pruned model'])\n",
    "\n",
    "\n",
    "        Pruned_finetune_model_accuracy.append(Eva['pruned and finetune model accuracy'])\n",
    "        T_pruned_finetune_model.append(Eva['time inference of pruned and finetune model'])\n",
    "        Num_parm_pruned_finetune_model.append(int(Eva['number parmameters of pruned and finetune model']))\n",
    "        Pruned_finetune_model_size.append(int(Eva['size of pruned and finetune model']))\n",
    "        Pruned_finetune_Energy_Consumption.append(Eva['energy consumption of pruned and finetune model'])\n",
    "        Pruned_finetune_Cpu_Usage.append(Eva['cpu usage of pruned and finetune model'])\n",
    "        Pruned_finetune_Memory_Usage.append(Eva['total memory usage of pruned and finetune model'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bd73e0",
   "metadata": {},
   "source": [
    "### Computing the mean and std "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7ca4aa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All measurement about pruning process of sparsity:90.0% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Ave of base model accuracy': 0.696,\n",
       " 'Std of base model accuracy': 0.0,\n",
       " 'Ave of time inference of base model': 2.564,\n",
       " 'Std of time inference of base model': 0.011,\n",
       " 'Ave of number parmameters of base model': 75414,\n",
       " 'Std of number parmameters of base model': 2.8284271247461903,\n",
       " 'Ave of base model size': 306667,\n",
       " 'Std of base model size': 0.0,\n",
       " 'Ave of energy consumption of base model': 34.895802840001124,\n",
       " 'Std of energy consumption of base model': 13.20481161889441,\n",
       " 'Ave of cpu usage of base model': 1.2000000000000002,\n",
       " 'Std of cpu usage of base model': 0.5656854249492381,\n",
       " 'Ave of memory usage of base model': 38292,\n",
       " 'Std of memory usage of base model': 634.9818895055197,\n",
       " 'Ave of pruned model accuracy': 0.5,\n",
       " 'Std of pruned model accuracy': 0.278,\n",
       " 'Ave of time inference of pruned model': 2.556,\n",
       " 'Std of time inference of pruned model': 0.0,\n",
       " 'Ave of number parmameters of pruned model': 8022,\n",
       " 'Std of number parmameters of pruned model': 2.8284271247461903,\n",
       " 'Ave of pruned model size': 36331,\n",
       " 'Std of pruned model size': 45.254833995939045,\n",
       " 'Ave of energy consumption of pruned model': 30.799450969991174,\n",
       " 'Std of energy consumption of pruned model': 7.413549282049057,\n",
       " 'Ave of cpu usage of pruned model': 1.55,\n",
       " 'Std of cpu usage of pruned model': 0.07071067811865482,\n",
       " 'Ave of memory usage of pruned model': 39591.5,\n",
       " 'Std of memory usage of pruned model': 48.79036790187178,\n",
       " 'Ave of pruned finetune model accuracy': 0.504,\n",
       " 'Std of pruned finetune model accuracy': 0.271,\n",
       " 'Ave of time inference of pruned finetune model': 2.592,\n",
       " 'Std of time inference of pruned finetune model': 0.019,\n",
       " 'Ave of number parmameters of pruned finetune model': 8059.5,\n",
       " 'Std of number parmameters of pruned finetune model': 2.1213203435596424,\n",
       " 'Ave of pruned finetune model size': 36859,\n",
       " 'Std of pruned finetune model size': 45.254833995939045,\n",
       " 'Ave of energy consumption of pruned_finetune model': 25.918371999996452,\n",
       " 'Std of energy consumption of pruned_finetune model': 0.1905242653292494,\n",
       " 'Ave of cpu usage of pruned_finetune model': 6.4,\n",
       " 'Std of cpu usage of pruned_finetune model': 6.929646455628166,\n",
       " 'Ave of memory usage of pruned_finetune model': 38133,\n",
       " 'Std of memory usage of pruned_finetune model': 28.284271247461902}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eva_final=dict()\n",
    "base_model_accuracy_mean = stat.mean(Base_model_accuracy)\n",
    "base_model_accuracy_std =  stat.stdev(Base_model_accuracy)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(base_model_accuracy_mean,base_model_accuracy_std)\n",
    "\n",
    "\n",
    "Eva_final.update({'Ave of base model accuracy':float(format(base_model_accuracy_mean, '.3f'))})\n",
    "Eva_final.update({'Std of base model accuracy':float(format(base_model_accuracy_std, '.3f'))})\n",
    "                 \n",
    "t_base_model_mean =stat.mean(T_base_model)\n",
    "t_base_model_std =stat.stdev(T_base_model)  \n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of time inference of base model':float(format(t_base_model_mean, '.3f'))})\n",
    "Eva_final.update({'Std of time inference of base model':float(format(t_base_model_std, '.3f'))})\n",
    "\n",
    "\n",
    "num_parm_base_model_mean = stat.mean(Num_parm_base_model)\n",
    "num_parm_base_model_std = stat.stdev(Num_parm_base_model)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of number parmameters of base model':num_parm_base_model_mean})\n",
    "Eva_final.update({'Std of number parmameters of base model':num_parm_base_model_std})\n",
    "\n",
    "base_model_size_mean = stat.mean(Base_model_size)\n",
    "base_model_size_std = stat.stdev(Base_model_size)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of base model size':base_model_size_mean})\n",
    "Eva_final.update({'Std of base model size':base_model_size_std})\n",
    "\n",
    "\n",
    "base_energy_consumption_mean = stat.mean(Base_Energy_Consumption)\n",
    "base_energy_consumption_std = stat.stdev(Base_Energy_Consumption)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of energy consumption of base model':base_energy_consumption_mean })\n",
    "Eva_final.update({'Std of energy consumption of base model':base_energy_consumption_std})\n",
    "\n",
    "\n",
    "base_cpu_usage_mean = stat.mean(Base_Cpu_Usage)\n",
    "base_cpu_usage_std = stat.stdev(Base_Cpu_Usage)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of cpu usage of base model':base_cpu_usage_mean})\n",
    "Eva_final.update({'Std of cpu usage of base model':base_cpu_usage_std})\n",
    "\n",
    "base_memory_usage_mean = stat.mean(Base_Memory_Usage)\n",
    "base_memory_usage_std = stat.stdev(Base_Memory_Usage)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of memory usage of base model':base_memory_usage_mean})\n",
    "Eva_final.update({'Std of memory usage of base model':base_memory_usage_std})\n",
    "\n",
    "#################################\n",
    "\n",
    "pruned_model_accuracy_mean =stat.mean(Pruned_model_accuracy)\n",
    "pruned_model_accuracy_std = stat.stdev(Pruned_model_accuracy)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of pruned model accuracy':float(format(pruned_model_accuracy_mean, '.3f'))})\n",
    "Eva_final.update({'Std of pruned model accuracy':float(format(pruned_model_accuracy_std, '.3f'))})\n",
    "                 \n",
    "\n",
    "t_pruned_model_mean = stat.mean(T_pruned_model)\n",
    "t_pruned_model_std =stat.stdev(T_pruned_model)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of time inference of pruned model':float(format(t_pruned_model_mean, '.3f'))})\n",
    "Eva_final.update({'Std of time inference of pruned model':float(format(t_pruned_model_std, '.3f'))})\n",
    "\n",
    "num_parm_pruned_model_mean = stat.mean(Num_parm_pruned_model)\n",
    "num_parm_pruned_model_std = stat.stdev(Num_parm_pruned_model)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of number parmameters of pruned model':num_parm_pruned_model_mean})\n",
    "Eva_final.update({'Std of number parmameters of pruned model':num_parm_pruned_model_std})\n",
    "\n",
    "pruned_model_size_mean =stat.mean( Pruned_model_size)\n",
    "pruned_model_size_std = stat.stdev(Pruned_model_size)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of pruned model size':pruned_model_size_mean})\n",
    "Eva_final.update({'Std of pruned model size':pruned_model_size_std })\n",
    "\n",
    "pruned_energy_consumption_mean = stat.mean(Pruned_Energy_Consumption)\n",
    "pruned_energy_consumption_std = stat.stdev(Pruned_Energy_Consumption)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of energy consumption of pruned model':pruned_energy_consumption_mean })\n",
    "Eva_final.update({'Std of energy consumption of pruned model':pruned_energy_consumption_std})\n",
    "\n",
    "\n",
    "pruned_cpu_usage_mean = stat.mean(Pruned_Cpu_Usage)\n",
    "pruned_cpu_usage_std = stat.stdev(Pruned_Cpu_Usage)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of cpu usage of pruned model':pruned_cpu_usage_mean})\n",
    "Eva_final.update({'Std of cpu usage of pruned model':pruned_cpu_usage_std})\n",
    "\n",
    "pruned_memory_usage_mean = stat.mean(Pruned_Memory_Usage)\n",
    "pruned_memory_usage_std = stat.stdev(Pruned_Memory_Usage)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of memory usage of pruned model':pruned_memory_usage_mean})\n",
    "Eva_final.update({'Std of memory usage of pruned model':pruned_memory_usage_std})\n",
    "\n",
    "\n",
    "#################################\n",
    "pruned_finetune_model_accuracy_mean =stat.mean(Pruned_finetune_model_accuracy)\n",
    "pruned_finetune_model_accuracy_std = stat.stdev(Pruned_finetune_model_accuracy)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of pruned finetune model accuracy':float(format(pruned_finetune_model_accuracy_mean, '.3f'))})\n",
    "Eva_final.update({'Std of pruned finetune model accuracy':float(format(pruned_finetune_model_accuracy_std, '.3f'))})                 \n",
    "\n",
    "t_pruned_finetune_model_mean =stat.mean(T_pruned_finetune_model)\n",
    "t_pruned_finetune_model_std =stat.stdev(T_pruned_finetune_model)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of time inference of pruned finetune model':float(format(t_pruned_finetune_model_mean,'.3f'))})\n",
    "Eva_final.update({'Std of time inference of pruned finetune model':float(format(t_pruned_finetune_model_std,'.3f'))})\n",
    "\n",
    "num_parm_pruned_finetune_model_mean =stat.mean(Num_parm_pruned_finetune_model)\n",
    "num_parm_pruned_finetune_model_std = stat.stdev(Num_parm_pruned_finetune_model)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of number parmameters of pruned finetune model':num_parm_pruned_finetune_model_mean})\n",
    "Eva_final.update({'Std of number parmameters of pruned finetune model':num_parm_pruned_finetune_model_std })\n",
    "\n",
    "pruned_finetune_model_size_mean = stat.mean(Pruned_finetune_model_size)\n",
    "pruned_finetune_model_size_std = stat.stdev(Pruned_finetune_model_size)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of pruned finetune model size':pruned_finetune_model_size_mean})\n",
    "Eva_final.update({'Std of pruned finetune model size':pruned_finetune_model_size_std})\n",
    "\n",
    "\n",
    "pruned_finetune_energy_consumption_mean = stat.mean(Pruned_finetune_Energy_Consumption)\n",
    "pruned_finetune_energy_consumption_std = stat.stdev(Pruned_finetune_Energy_Consumption)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of energy consumption of pruned_finetune model':pruned_finetune_energy_consumption_mean })\n",
    "Eva_final.update({'Std of energy consumption of pruned_finetune model':pruned_finetune_energy_consumption_std})\n",
    "\n",
    "\n",
    "pruned_finetune_cpu_usage_mean = stat.mean(Pruned_finetune_Cpu_Usage)\n",
    "pruned_finetune_cpu_usage_std = stat.stdev(Pruned_finetune_Cpu_Usage)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of cpu usage of pruned_finetune model':pruned_finetune_cpu_usage_mean})\n",
    "Eva_final.update({'Std of cpu usage of pruned_finetune model':pruned_finetune_cpu_usage_std})\n",
    "\n",
    "pruned_finetune_memory_usage_mean = stat.mean(Pruned_finetune_Memory_Usage)\n",
    "pruned_finetune_memory_usage_std = stat.stdev(Pruned_finetune_Memory_Usage)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of memory usage of pruned_finetune model':pruned_finetune_memory_usage_mean})\n",
    "Eva_final.update({'Std of memory usage of pruned_finetune model':pruned_finetune_memory_usage_std})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################\n",
    "\n",
    "\n",
    "print(f\"All measurement about pruning process of sparsity:{sparsity*100}% \")   \n",
    "Eva_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b6dd7",
   "metadata": {},
   "source": [
    "### Recording the results on txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "960b0d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The sparsity changes across range(0, 1, .01)\n",
    "dataset_name = 'Proteins'\n",
    "Pruning_Method='Grained_Pruning'\n",
    "max_epoch = 100\n",
    "resume = True\n",
    "result_folder ='pathresult/'\n",
    "if not os.path.exists(result_folder):\n",
    "    os.makedirs(result_folder)\n",
    "\n",
    "\n",
    "\n",
    "file_name = result_folder+Pruning_Method+'_'+'with_sparsity_of'+'_'+str(sparsity)+'_on_'+dataset_name+'_'+str(max_epoch)+'.txt'\n",
    "\n",
    "with open(file_name, 'w') as f:\n",
    "        f.write('%s:%s\\n'%('dataset_name', 'Proteins'))\n",
    "        f.write('%s:%s\\n'%('max_epoch', max_epoch))\n",
    "        f.write('%s:%s\\n'%('sparsity', sparsity))\n",
    "        for key, value in Eva_final.items():\n",
    "            f.write('%s:%s\\n'%(key, value))\n",
    "            \n",
    "        for key, value in Eva_measure.items():\n",
    "            f.write('%s:%s\\n' % (key, ','.join(map(str, value))))                \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c759446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55824b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a1b1063f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torchinfo\n",
      "Version: 1.8.0\n",
      "Summary: Model summary in PyTorch, based off of the original torchsummary.\n",
      "Home-page: https://github.com/tyleryep/torchinfo\n",
      "Author: Tyler Yep @tyleryep\n",
      "Author-email: tyep@cs.stanford.edu\n",
      "License: MIT\n",
      "Location: C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python313\\site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf22c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torchinfo 1.8.0\n",
      "Uninstalling torchinfo-1.8.0:\n",
      "  Successfully uninstalled torchinfo-1.8.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchinfo\n",
      "  Using cached torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Using cached torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torchinfo -y\n",
    "!pip install torchinfo --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8871b5f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchinfo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchinfo\u001b[39;00m \u001b[38;5;66;03m#import summary\u001b[39;00m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(args)  \u001b[38;5;66;03m# Initialize your model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Adjust based on your batch size\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchinfo'"
     ]
    }
   ],
   "source": [
    "import torchinfo #import summary\n",
    "\n",
    "model = Model(args)  # Initialize your model\n",
    "batch_size = 1  # Adjust based on your batch size\n",
    "input_size = (batch_size, args.num_features)  # (batch_size, num_features)\n",
    "\n",
    "# Example input (adjust for your data)\n",
    "dummy_data = Data(\n",
    "    x=torch.randn(batch_size, args.num_features),  # Node features\n",
    "    edge_index=torch.randint(0, batch_size, (2, 10)),  # Random edges\n",
    "    batch=torch.zeros(batch_size, dtype=torch.long)  # Batch indices\n",
    ")\n",
    "\n",
    "summary(model, input_data=dummy_data, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb58b9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.13.1\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0cb9468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc37cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
