{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66806e82",
   "metadata": {},
   "source": [
    "Grain Pruning Method on Node Classification Task of Cora Dataset\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c9530",
   "metadata": {},
   "source": [
    "### All libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2bb697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "import copy\n",
    "import psutil\n",
    "import itertools\n",
    "import tracemalloc\n",
    "import gc\n",
    "import statistics as stat\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SplineConv\n",
    "from torch_geometric.typing import WITH_TORCH_SPLINE_CONV\n",
    "from torch.optim import Adam\n",
    "\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e344ac",
   "metadata": {},
   "source": [
    "### Sparsity\n",
    "- The sparsity is the parameter that is determines the rate of pruning across the layer. It is a value in range(0,0.1,1). This parameter is fixed for this notebook and change for remaining experiment. We determine it before training process. Here is all values of sparsities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "452b9d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1, 0.2, 0.3, 0.4,  0.5, 0.6 , 0.7,  0.8, 0.9\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49825c49",
   "metadata": {},
   "source": [
    "### Pruning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86b9d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.prune import global_unstructured, L1Unstructured, remove\n",
    "\n",
    "def fine_grained_prune(tensor: torch.Tensor, sparsity : float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    magnitude-based pruning for single tensor\n",
    "    :param tensor: torch.(cuda.)Tensor, weight of conv/fc layer\n",
    "    :param sparsity: float, pruning sparsity\n",
    "        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n",
    "    :return:\n",
    "        torch.(cuda.)Tensor, mask for zeros\n",
    "    \"\"\"\n",
    "    sparsity = min(max(0.0, sparsity), 1.0)\n",
    "    if sparsity == 1.0:\n",
    "        tensor.zero_()\n",
    "        return torch.zeros_like(tensor)\n",
    "    elif sparsity == 0.0:\n",
    "        return torch.ones_like(tensor)\n",
    "\n",
    "    num_elements = tensor.numel()\n",
    "\n",
    "    num_zeros = round(num_elements * sparsity)\n",
    "    importance = tensor.abs()\n",
    "    threshold = importance.view(-1).kthvalue(num_zeros).values\n",
    "    mask = torch.gt(importance, threshold)\n",
    "    tensor.mul_(mask)\n",
    "\n",
    "    return mask\n",
    "    \n",
    "\n",
    "class FineGrainedPruner:\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, model, sparsity_dict):\n",
    "        self.masks = FineGrainedPruner.prune(model, sparsity_dict)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def apply(self, model, path):\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in self.masks:\n",
    "                param *= self.masks[name]\n",
    "        # building non-zero weights as state dictinary                \n",
    "        #sparse_model = SparseModel(model)\n",
    "       # state_model=state_sparse_model(sparse_model, None)\n",
    "        # Saving model\n",
    "        #pth_name = f\"pruned_model.pth\"  \n",
    "        #ckpt_pruned_path = os.path.join(ckpt_dir, pth_name)\n",
    "        #torch.save(state_model, ckpt_pruned_path)\n",
    "       # return    sparse_model\n",
    "       \n",
    " \n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def prune(model, sparsity_dict):\n",
    "        masks = dict()\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.dim() > 1: # we only prune conv and fc weights\n",
    "                if isinstance(sparsity_dict, dict):\n",
    "                    masks[name] = fine_grained_prune(param, sparsity_dict[name])\n",
    "                else:\n",
    "                    assert(sparsity_dict < 1 and sparsity_dict >= 0)\n",
    "                    if sparsity_dict > 0:\n",
    "                        masks[name] = fine_grained_prune(param, sparsity_dict)\n",
    "        return masks\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "##### Old version\n",
    "def state_sparse_model(model, eval_acc):\n",
    "    '''\n",
    "    This funcrion Removes Zeroed Weights\n",
    "    and saves non-zero weights as state dictionary\n",
    "    '''\n",
    "    state_dict = model.state_dict()\n",
    "    non_zero_state = {\n",
    "        k: v.to_sparse() if torch.count_nonzero(v) < v.numel() else v\n",
    "        for k, v in state_dict.items()\n",
    "    }\n",
    "    non_zero_state_dict = {'net': non_zero_state, 'epoch': epoch, 'acc': eval_acc}\n",
    "    return non_zero_state_dict\n",
    "\n",
    "    \n",
    "    \n",
    "def load_sparse_model(state_path, original_model):\n",
    "    '''\n",
    "    This function provides a practical approach to loading sparse models into dense environments,\n",
    "    offering a good balance between memory efficiency and model functionality\n",
    "    '''\n",
    "    sparse_model = SparseModel(original_model)\n",
    "    non_zero_state_dict = torch.load(state_path)\n",
    "    \n",
    "    sparse_model_state = sparse_model.state_dict()\n",
    "    \n",
    "    for k, v in non_zero_state_dict['net'].items():\n",
    "        if isinstance(v, torch.Tensor) and v.is_sparse:\n",
    "            sparse_model_state[k] = v.to_dense()\n",
    "        else:\n",
    "            sparse_model_state[k] = v\n",
    "    \n",
    "    sparse_model.load_state_dict(sparse_model_state, strict=False)\n",
    "    return sparse_model\n",
    "    \n",
    "class SparseModel(nn.Module):\n",
    "    '''\n",
    "    This class simply creates a structurally similar \n",
    "    model without modifying the weights\n",
    "    or applying any sparse optimization techniques.\n",
    "    '''\n",
    "    def __init__(self, original_model):\n",
    "        super(SparseModel, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(layer.in_features, layer.out_features, bias=layer.bias is not None) if isinstance(layer, nn.Linear) \n",
    "            else layer for layer in original_model.children()\n",
    "        ])\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                x = layer(x, edge_index, edge_attr)\n",
    "        return x\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406f5c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d9b7d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## New version\n",
    "def state_sparse_model(model, eval_acc=None, epoch=None):\n",
    "    state_dict = model.state_dict()\n",
    "    compressed_state = {}\n",
    "    \n",
    "    for k, v in state_dict.items():\n",
    "        if torch.is_tensor(v):\n",
    "            mask = v != 0\n",
    "            if mask.any():  # Only compress if there are non-zeros\n",
    "                compressed_state[k] = {\n",
    "                    'shape': v.shape,\n",
    "                    'values': v[mask]  # Store only non-zero values\n",
    "                }\n",
    "            else:\n",
    "                compressed_state[k] = v  # Keep original if all zeros\n",
    "        else:\n",
    "            compressed_state[k] = v\n",
    "    \n",
    "    return {'net': compressed_state, 'epoch': epoch, 'acc': eval_acc}\n",
    "\n",
    "def load_sparse_model(state_path, original_model):\n",
    "    \"\"\"\n",
    "    Loads a model saved in the custom compressed format (non-zero values only).\n",
    "    Reconstructs dense tensors before loading into the model.\n",
    "    \"\"\"\n",
    "    # Load the compressed state_dict\n",
    "    compressed_state = torch.load(state_path)\n",
    "    compressed_weights = compressed_state['net']\n",
    "    \n",
    "    # Initialize a new state_dict for the original model\n",
    "    new_state_dict = original_model.state_dict()\n",
    "    \n",
    "    for k, v in compressed_weights.items():\n",
    "        if isinstance(v, dict) and 'shape' in v and 'values' in v:\n",
    "            # Reconstruct dense tensor from compressed format\n",
    "            dense_tensor = torch.zeros(v['shape'], dtype=v['values'].dtype)\n",
    "            mask = (dense_tensor != 0)  # All False initially\n",
    "            # We need to know the positions of non-zero values (if available)\n",
    "            # If indices were saved, use them; otherwise, assume sequential filling (simpler but may not match original positions)\n",
    "            if 'indices' in v:\n",
    "                # If you saved indices (advanced version)\n",
    "                dense_tensor[v['indices']] = v['values']\n",
    "            else:\n",
    "                # If only values were saved (simpler version)\n",
    "                # Flatten and fill non-zeros sequentially (may not match original positions)\n",
    "                flat_tensor = dense_tensor.view(-1)\n",
    "                flat_tensor[:len(v['values'])] = v['values']\n",
    "                dense_tensor = flat_tensor.reshape(v['shape'])\n",
    "            \n",
    "            new_state_dict[k] = dense_tensor\n",
    "        else:\n",
    "            # If it's a normal tensor (e.g., biases, batch norm stats)\n",
    "            new_state_dict[k] = v\n",
    "    \n",
    "    # Load the reconstructed state_dict\n",
    "    original_model.load_state_dict(new_state_dict, strict=False)\n",
    "    return original_model\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def load_and_evaluate_pruned_model(model_path):\n",
    "    \"\"\"\n",
    "    This function loads the pruned model from disk and evaluates it.\n",
    "    \"\"\"\n",
    "    # Instantiate the model\n",
    "    model = Net()\n",
    "    \n",
    "    # Load the pruned model\n",
    "    sparse_model = load_sparse_model(model_path, model)\n",
    "    print(\"Pruned model loaded.\")\n",
    "    \n",
    "\n",
    "    return sparse_model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb57c7",
   "metadata": {},
   "source": [
    "### Functions for Mmeasuring criterias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e26f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_parameters(model: nn.Module, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the total number of parameters of model\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    num_counted_elements = 0\n",
    "    for param in model.parameters():\n",
    "        if count_nonzero_only:\n",
    "            num_counted_elements += param.count_nonzero()\n",
    "        else:\n",
    "            num_counted_elements += param.numel()\n",
    "    return num_counted_elements\n",
    "\n",
    "# Function to get CPU usage\n",
    "def get_cpu_usage():\n",
    "    return psutil.cpu_percent(interval=1)\n",
    "\n",
    "\n",
    "\n",
    "# Function to approximate power consumption (Assume some average power usage per CPU percentage point)\n",
    "def estimate_power_usage(cpu_usage):\n",
    "    base_power_usage = 10  # Assumed base power usage in watts\n",
    "    power_per_percent = 0.5  # Assumed additional watts per CPU usage percent\n",
    "    return base_power_usage + (power_per_percent * cpu_usage)\n",
    "\n",
    "# The model size based on the number of parameters\n",
    "def get_model_size_param(model: nn.Module, data_width=32, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the model size in bits\n",
    "    :param data_width: #bits per element\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    return get_num_parameters(model, count_nonzero_only) * data_width\n",
    "\n",
    "\n",
    "def get_model_sparsity(model: nn.Module) -> float:\n",
    "    ''' \n",
    "    The input is layers of pruned model and the output is the sparsity after pruning.\n",
    "    '''\n",
    "    Sparsity=dict()\n",
    "    global_zero=0\n",
    "    global_nzero=0\n",
    "    layyers=[]\n",
    "    spars=[]\n",
    "    for name, param in model.named_parameters(): \n",
    "        if 'weight' in name:\n",
    "                    zero=float(torch.sum(param == 0))\n",
    "                    nzero=float(param.nelement())\n",
    "                    sparsity=  float(zero)/ float(nzero)\n",
    "                    print( f'Sparsity in {name}: {sparsity:.3f}' )\n",
    "                    layyers.append(name)\n",
    "                    spars.append(sparsity)\n",
    "                    global_zero +=zero\n",
    "                    global_nzero +=nzero\n",
    "\n",
    "\n",
    "\n",
    "    Sparsity={key: value for key, value in zip(layyers,spars)}\n",
    "    global_sparsity= float(global_zero) /float(global_nzero)\n",
    "    Sparsity.update({'Global sparsity':  global_sparsity})\n",
    "    print(\"Global sparsity: {:.3f}\".format(global_sparsity))\n",
    "    return   Sparsity   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa82adb",
   "metadata": {},
   "source": [
    "### Clearing the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e150145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_name='Spline', dataset_name='Cora', num_deg=1000, num_layers=2, hidden_units=16, max_epoch=50, max_cycle=10, weight_decay=0, lr=0.001)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sys.argv = ['']\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_name',type=str,default='Spline')\n",
    "parser.add_argument('--dataset_name',type=str,default='Cora')\n",
    "parser.add_argument('--num_deg',type=int,default=1000)\n",
    "parser.add_argument('--num_layers', type=int, default=2)\n",
    "parser.add_argument('--hidden_units',type=int,default=16)\n",
    "parser.add_argument('--max_epoch',type=int,default=50)\n",
    "parser.add_argument('--max_cycle',type=int,default=10)\n",
    "parser.add_argument('--weight_decay',type=float,default=0)\n",
    "parser.add_argument('--lr',type=float,default=0.001)\n",
    "###############################################################\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab547fe",
   "metadata": {},
   "source": [
    "### start loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f5e686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not WITH_TORCH_SPLINE_CONV:\n",
    "    quit(\"This example requires 'torch-spline-conv'\")\n",
    "\n",
    "dataset = 'Cora'\n",
    "transform = T.Compose([\n",
    "    T.RandomNodeSplit(num_val=500, num_test=500),\n",
    "    T.TargetIndegree(),\n",
    "])\n",
    "path =  'data'\n",
    "dataset = Planetoid(path, dataset, transform=transform)\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94571a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = SplineConv(dataset.num_features, 16, dim=1, kernel_size=2)\n",
    "        self.conv2 = SplineConv(16, dataset.num_classes, dim=1, kernel_size=2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, data = Net().to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc040b52",
   "metadata": {},
   "source": [
    "### Function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a90184a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(callbacks = None):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    F.nll_loss(model(data)[data.train_mask], data.y[data.train_mask]).backward()\n",
    "    optimizer.step()\n",
    "    if callbacks is not None:\n",
    "        for callback in callbacks:\n",
    "                callback()\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "def evaluate_Spline(model, mask):\n",
    "    model.eval()\n",
    "    for _, mask in mask:\n",
    "        log_probs, accs = model(data), []\n",
    "        # calculate acc\n",
    "        pred = log_probs[mask].max(1)[1]\n",
    "        acc=pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        # Calculate loss\n",
    "        loss=F.nll_loss(model(data)[mask], data.y[mask])\n",
    "    \n",
    "    eval_state = {'loss': loss / (mask.sum().item()), 'acc': acc}\n",
    "    return eval_state\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    log_probs, accs = model(data), []\n",
    "    for _, mask in data('train_mask', 'test_mask'):\n",
    "        pred = log_probs[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6280ae5",
   "metadata": {},
   "source": [
    "### save path for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "873fcb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.isdir('checkpoint'):\n",
    "    os.mkdir('checkpoint')\n",
    "if not os.path.isdir(os.path.join('checkpoint', f\"Cora\")):\n",
    "    os.mkdir(os.path.join('checkpoint', f\"Cora\"))\n",
    "ckpt_dir = f\"./checkpoint/Cora/\"\n",
    "\n",
    "\n",
    "def save_best(ckpt_dir, epoch, state,  model_name, eval_acc, is_best, is_pruned):\n",
    "    print('saving....')\n",
    "    #model.to_device()\n",
    "    state_save = {\n",
    "        'net':state,\n",
    "        'epoch':epoch,\n",
    "        'acc': eval_acc \n",
    "        }\n",
    "    best_pth_name = f'{args.model_name}_best.pth'\n",
    "    fine_tuned_pth_name = f'{args.model_name}_fine_tuned_best.pth'\n",
    "  \n",
    "    if is_pruned & is_best:\n",
    "        ckpt_path = os.path.join(ckpt_dir, fine_tuned_pth_name) \n",
    "        torch.save(state_save, ckpt_path)\n",
    "    \n",
    "     \n",
    "    if is_pruned== False & is_best:\n",
    "        ckpt_path = os.path.join(ckpt_dir, best_pth_name)  \n",
    "        torch.save(state_save, ckpt_path)\n",
    "           \n",
    "        \n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b51dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c45ab052",
   "metadata": {},
   "source": [
    "###  Pruning the Model and Re-Evaluate the Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34aeae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting Sparsity\n",
    "sparsity=0.1\n",
    "\n",
    "# The number of epochs  \n",
    "num_epochs=2\n",
    "# The number of iterations\n",
    "num_iterations=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76999fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is a dictionary to save all measurements. Aftre measuring, we can compute mean and std of each item.\n",
    "Eva_final=dict()\n",
    "\n",
    "# The following are all list of criteria for measurements. \n",
    "# We collect all desired datas of each list across iterations. \n",
    "# Then, we compute average and std of each list.\n",
    "\n",
    "#Base model\n",
    "Base_model_accuracy=[]\n",
    "T_base_model=[]\n",
    "Num_parm_base_model=[]\n",
    "Base_model_size=[]\n",
    "Base_Energy_Consumption=[]\n",
    "Base_Cpu_Usage=[]\n",
    "Base_Memory_Usage=[]\n",
    "\n",
    "#Pruned model\n",
    "Pruned_model_accuracy=[]\n",
    "T_pruned_model=[]\n",
    "Num_parm_pruned_model=[]\n",
    "Pruned_model_size=[]\n",
    "Pruned_Energy_Consumption=[]\n",
    "Pruned_Cpu_Usage=[]\n",
    "Pruned_Memory_Usage=[]\n",
    "\n",
    "#Prined and finetune model\n",
    "Pruned_finetune_model_accuracy=[]\n",
    "T_pruned_finetune_model=[]\n",
    "Num_parm_pruned_finetune_model=[]\n",
    "Pruned_finetune_model_size=[]\n",
    "Pruned_finetune_Energy_Consumption=[]\n",
    "Pruned_finetune_Cpu_Usage=[]\n",
    "Pruned_finetune_Memory_Usage=[]\n",
    "\n",
    "\n",
    "Eva_measure={'base model accuracy':Base_model_accuracy,\n",
    "            'time inference of base model':T_base_model,\n",
    "            'number parmameters of base model':Num_parm_base_model,\n",
    "            'base model size':Base_model_size,\n",
    "            'energy consumption of base model':Base_Energy_Consumption,\n",
    "            'cpu usage of base model':Base_Cpu_Usage,\n",
    "            'memory usage of base model':Base_Memory_Usage,\n",
    "            'pruned model accuracy': Pruned_model_accuracy,\n",
    "            'time inference of pruned model':T_pruned_model,\n",
    "            'number parmameters of pruned model':Num_parm_pruned_model,\n",
    "            'pruned model size':Pruned_model_size,\n",
    "            'energy consumption of pruned model':Pruned_Energy_Consumption,\n",
    "            'cpu usage of pruned model':Pruned_Cpu_Usage,\n",
    "            'memory usage of pruned model':Pruned_Memory_Usage,\n",
    "            'pruned finetune model accuracy':Pruned_finetune_model_accuracy,\n",
    "            'time inference of pruned finetune model':T_pruned_finetune_model,\n",
    "            'number parmameters of pruned finetune model':Num_parm_pruned_finetune_model,\n",
    "            'pruned finetune model size':Pruned_finetune_model_size,\n",
    "            'energy consumption of pruned_finetune model':Pruned_finetune_Energy_Consumption,\n",
    "            'cpu usage of pruned_finetune model':Pruned_finetune_Cpu_Usage,\n",
    "            'memory usage of pruned_finetune model':Pruned_finetune_Memory_Usage}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f0da2",
   "metadata": {},
   "source": [
    "### Training, Pruning, Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "304f39fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________\n",
      " This is iteration:1\n",
      "Training and evaluation before pruning \n",
      "Starting training...\n",
      "Training and evaluation before pruning \n",
      "Eval Epoch: 0 | Loss: 0.004 | Acc: 0.584\n",
      "saving....\n",
      "Eval Epoch: 1 | Loss: 0.003 | Acc: 0.612\n",
      "saving....\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.61%\n",
      "base model has size=279991.00 bit\n",
      "The time inference of base model is =18.29284920000009\n",
      "The number of parametrs of base model is:69143\n",
      "Energy Consumption : 182.928\n",
      "total memory usage of base model':7656 \n",
      "cpu usage of base model':0.000 %\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.59%\n",
      "pruned model has size=252291.00\n",
      "The time inference of pruned model is =18.34988929999963\n",
      "The number of parametrs of pruned model is:62231\n",
      "Energy Consumption : 183.499\n",
      "total memory usage of pruned model':7624 \n",
      "cpu usage of pruned model':0.000 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Eval Epoch: 0 | Loss: 0.003 | Acc: 0.628\n",
      "saving....\n",
      "saving....\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.66%\n",
      "pruned_finetune model has size=252475.00 \n",
      "The time inference of pruned_finetune model is =18.428580699999657\n",
      "The number of parametrs of pruned_finetune model is:62231\n",
      "Energy Consumption of pruned_finetune model: 184.286\n",
      "total memory usage of pruned_finetune model':5352 \n",
      "cpu usage of pruned_finetune model':0.400 %\n",
      "_________________________________________\n",
      " This is iteration:2\n",
      "Training and evaluation before pruning \n",
      "Starting training...\n",
      "Training and evaluation before pruning \n",
      "Eval Epoch: 0 | Loss: 0.004 | Acc: 0.418\n",
      "saving....\n",
      "Eval Epoch: 1 | Loss: 0.003 | Acc: 0.506\n",
      "saving....\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.49%\n",
      "base model has size=279991.00 bit\n",
      "The time inference of base model is =19.384872100000393\n",
      "The number of parametrs of base model is:69143\n",
      "Energy Consumption : 243.280\n",
      "total memory usage of base model':5352 \n",
      "cpu usage of base model':6.500 %\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.47%\n",
      "pruned model has size=252291.00\n",
      "The time inference of pruned model is =19.895917900000313\n",
      "The number of parametrs of pruned model is:62231\n",
      "Energy Consumption : 307.392\n",
      "total memory usage of pruned model':7720 \n",
      "cpu usage of pruned model':2.700 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Eval Epoch: 0 | Loss: 0.003 | Acc: 0.570\n",
      "saving....\n",
      "saving....\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.63%\n",
      "pruned_finetune model has size=252475.00 \n",
      "The time inference of pruned_finetune model is =21.950470799999493\n",
      "The number of parametrs of pruned_finetune model is:62231\n",
      "Energy Consumption of pruned_finetune model: 797.900\n",
      "total memory usage of pruned_finetune model':5352 \n",
      "cpu usage of pruned_finetune model':25.000 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_iterations):\n",
    "    \n",
    "        print('_________________________________________')\n",
    "        print(f' This is iteration:{i+1}')\n",
    "        print(f'Training and evaluation before pruning ')\n",
    "        print(\"Starting training...\")\n",
    "        Eva=dict()# It is a dictionary to arrange output of this iteration\n",
    "        best_acc=0  \n",
    "        early_stopping=100\n",
    "        is_pruned=False\n",
    "        save_epoch=10\n",
    "\n",
    "        #Training base model\n",
    "        print(f'Training and evaluation before pruning ')\n",
    "        \n",
    "        model = Net().to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-3)\n",
    "\n",
    "        for epoch in range(num_epochs):  \n",
    "            acc=[]\n",
    "            loss_list = []\n",
    "            train(callbacks=None)        \n",
    "\n",
    "            eval_state =  evaluate_Spline(model,data('val_mask'))\n",
    "            acc_eval=eval_state['acc']\n",
    "            # report train msg\n",
    "            #if epoch % 20 == 0:   \n",
    "            print(f\"Eval Epoch: {epoch} | Loss: {eval_state['loss']:.3f} | Acc: {eval_state['acc']:.3f}\")\n",
    "\n",
    "\n",
    "            # only save the best model\n",
    "            if eval_state['acc'] > best_acc:\n",
    "                early_stop_count = 0\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "\n",
    "            if early_stop_count > early_stopping:\n",
    "                break\n",
    "            is_best = (eval_state['acc'] > best_acc)\n",
    "            if is_best:\n",
    "                best_acc = eval_state['acc']\n",
    "                early_stop_count = 0\n",
    "\n",
    "            if is_best or epoch % save_epoch == 0:\n",
    "                 save_best(ckpt_dir, epoch, model.state_dict(), args.model_name, eval_state['acc'], is_best, is_pruned)   \n",
    "\n",
    "        #### load the best model\n",
    "        base_model_path = os.path.join(ckpt_dir, f'{args.model_name}_best.pth') \n",
    "        checkpoint = torch.load(base_model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        best_checkpoint = dict()\n",
    "        best_checkpoint['state_dict'] = copy.deepcopy(model.state_dict())\n",
    "        model.load_state_dict(best_checkpoint['state_dict'])\n",
    "        recover_model = lambda: model.load_state_dict(best_checkpoint['state_dict'])\n",
    "\n",
    "        # Start monitoring CPU and memory usage, model size, number of parametes, time inference and  power consumption\n",
    "        gc.collect()\n",
    "        time.sleep(5)  # Add a 5-second delay to stabilize the initial state\n",
    "        tracemalloc.start()  # Start tracking memory allocations\n",
    "        snapshot_before = tracemalloc.take_snapshot()#take a snapshot of the current memory state before starting the measurement.\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        initial_cpu_usage = get_cpu_usage()\n",
    "        power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "\n",
    "        train_acc, base_model_accuracy=test(model)\n",
    "    \n",
    "\n",
    "        base_cpu_usage = get_cpu_usage()\n",
    "        t1 = time.perf_counter()\n",
    "        t_base_model=t1-t0\n",
    "\n",
    "        snapshot_after = tracemalloc.take_snapshot()\n",
    "        tracemalloc.stop()\n",
    "        top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "        base_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "        base_energy_consumption = power_usage * t_base_model\n",
    "        base_model_size = os.path.getsize(base_model_path)\n",
    "        num_parm_base_model=get_num_parameters(model, count_nonzero_only=True)\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5) \n",
    "        \n",
    "        print(f'*****Results of base model*********')\n",
    "\n",
    "        print(f\"base model has accuracy on test set={base_model_accuracy:.2f}%\")\n",
    "        print(f\"base model has size={base_model_size:.2f} bit\")\n",
    "        print(f\"The time inference of base model is ={t_base_model}\") \n",
    "        print(f\"The number of parametrs of base model is:{num_parm_base_model}\") \n",
    "\n",
    "        print(f\"Energy Consumption : {base_energy_consumption:.3f}\")\n",
    "        print(f\"total memory usage of base model':{base_total_memory_diff} \")\n",
    "        print(f\"cpu usage of base model':{base_cpu_usage:.3f} %\")\n",
    "\n",
    "\n",
    "        #Update Eva dictionary\n",
    "        Eva.update({'base model accuracy': base_model_accuracy,\n",
    "                    'time inference of base model': t_base_model,\n",
    "                    'number parmameters of base model': num_parm_base_model,\n",
    "                    'size of base model': base_model_size, \n",
    "                    'energy consumption of base model':base_energy_consumption,\n",
    "                    'total memory usage of base model':base_total_memory_diff,\n",
    "                    'cpu usage of base model':base_cpu_usage\n",
    "                   })\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5)  \n",
    "\n",
    "        print('_________******************************_____________')\n",
    "        print(f'Pruning the Model')\n",
    "\n",
    "        # The path of pruned model\n",
    "        pth_name = f\"pruned_model.pth\"  \n",
    "        ckpt_pruned_path = os.path.join(ckpt_dir, pth_name) \n",
    "        pruner = FineGrainedPruner(model,sparsity)\n",
    "        pruned_model=pruner.apply(model,ckpt_pruned_path)\n",
    " \n",
    "        print('****************Result of pruning ******************')\n",
    "       \n",
    "        gc.collect()\n",
    "        time.sleep(5)  \n",
    "        tracemalloc.start()  \n",
    "        snapshot_before = tracemalloc.take_snapshot()\n",
    "        \n",
    "        t0 = time.perf_counter()\n",
    "        initial_cpu_usage = get_cpu_usage()\n",
    "        power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "        train_acc, pruned_model_accuracy=test(pruned_model)\n",
    "        pruned_cpu_usage = get_cpu_usage()\n",
    "        t1 = time.perf_counter()\n",
    "        t_pruned_model=t1-t0\n",
    "\n",
    "        snapshot_after = tracemalloc.take_snapshot()\n",
    "        tracemalloc.stop()\n",
    "        top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "        pruned_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "        pruned_energy_consumption = power_usage * t_pruned_model\n",
    "        pruned_model_size = os.path.getsize(ckpt_pruned_path)\n",
    "        num_parm_pruned_model=get_num_parameters(pruned_model, count_nonzero_only=True)\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5)  \n",
    "        \n",
    "        ###### Report of pruning \n",
    "        print(f\"pruned model has accuracy on test set={pruned_model_accuracy:.2f}%\")\n",
    "        print(f\"pruned model has size={pruned_model_size:.2f}\")\n",
    "        print(f\"The time inference of pruned model is ={t_pruned_model}\") \n",
    "        print(f\"The number of parametrs of pruned model is:{num_parm_pruned_model}\") \n",
    "\n",
    "        print(f\"Energy Consumption : {pruned_energy_consumption:.3f}\")\n",
    "        print(f\"total memory usage of pruned model':{pruned_total_memory_diff} \")\n",
    "        print(f\"cpu usage of pruned model':{pruned_cpu_usage:.3f} %\")\n",
    "\n",
    "\n",
    "        #Update Eva dictionary\n",
    "        Eva.update({'pruned model accuracy': pruned_model_accuracy,\n",
    "                    'time inference of pruned model': t_pruned_model,\n",
    "                    'number parmameters of pruned model': num_parm_pruned_model,\n",
    "                    'size of pruned model': pruned_model_size, \n",
    "                    'energy consumption of pruned model':pruned_energy_consumption,\n",
    "                    'total memory usage of pruned model':pruned_total_memory_diff,\n",
    "                    'cpu usage of pruned model':pruned_cpu_usage\n",
    "                   })\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5)   \n",
    "\n",
    "        print('________*******************************_____________')\n",
    "        print(f'Finetuning Pruned Sparse Model')\n",
    "\n",
    "        best_sparse_checkpoint = dict()\n",
    "        best_sparse_acc = 0\n",
    "        is_pruned=True\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # At the end of each train iteration, we have to apply the pruning mask\n",
    "            #    to keep the model sparse during the training\n",
    "            acc=[]\n",
    "            loss_list = []\n",
    "\n",
    "            acc=train(callbacks=[lambda: pruner.apply(model,ckpt_pruned_path )])        \n",
    "\n",
    "            eval_state =  evaluate_Spline(model,data('val_mask'))\n",
    "            acc_eval=eval_state['acc']\n",
    "\n",
    "            # report train msg\n",
    "            if epoch % 20 == 0:   \n",
    "                print(f\"Eval Epoch: {epoch} | Loss: {eval_state['loss']:.3f} | Acc: {eval_state['acc']:.3f}\")\n",
    "\n",
    "             \n",
    "            \n",
    "            # only save the best model\n",
    "            if eval_state['acc'] > best_sparse_acc :\n",
    "                early_stop_count = 0\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "\n",
    "            if early_stop_count > early_stopping:\n",
    "                break\n",
    "            is_best = (eval_state['acc'] > best_sparse_acc )\n",
    "           \n",
    "            if is_best:\n",
    "                best_acc = eval_state['acc']\n",
    "                early_stop_count = 0\n",
    "               \n",
    "            #Remove Zeroed Weights \n",
    "            pruned_fine_tune_model= SparseModel(model) \n",
    "            sparse_zero_state=state_sparse_model(model, eval_state['acc'])\n",
    "\n",
    "                           \n",
    "            if is_best or epoch % save_epoch == 0:\n",
    "                \n",
    "                save_best(ckpt_dir, epoch,sparse_zero_state, args.model_name, eval_state['acc'], is_best, is_pruned)\n",
    "\n",
    "        #### load the best fine-tune model\n",
    "        fine_tuned_pth_name=f'{args.model_name}_fine_tuned_best.pth'\n",
    "        fine_tuned_model_path = os.path.join(ckpt_dir, fine_tuned_pth_name)\n",
    "\n",
    "        # Assuming `original_model` is the model you used to create the sparse version\n",
    "        original_model =model # Define your original model here\n",
    "\n",
    "        # Load your sparse model\n",
    "        sparse_model = load_sparse_model(fine_tuned_model_path, original_model)\n",
    "\n",
    "        # Now you can use the sparse model for evaluation or further training\n",
    "        print('****************Result of fine-tuning of pruned model ******************')\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5)  \n",
    "        tracemalloc.start() \n",
    "        snapshot_before = tracemalloc.take_snapshot()\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        initial_cpu_usage = get_cpu_usage()\n",
    "        power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "        train_acc,  pruned_finetune_model_accuracy=test( sparse_model)\n",
    "\n",
    "\n",
    "        pruned_finetune_cpu_usage = get_cpu_usage()\n",
    "        t1 = time.perf_counter()\n",
    "        t_pruned_finetune_model=t1-t0\n",
    "\n",
    "        snapshot_after = tracemalloc.take_snapshot()\n",
    "        tracemalloc.stop()\n",
    "        top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "        pruned_finetune_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "        pruned_finetune_energy_consumption = power_usage * t_pruned_finetune_model\n",
    "        pruned_finetune_model_size = os.path.getsize( fine_tuned_model_path)\n",
    "        num_parm_pruned_finetune_model=get_num_parameters(sparse_model, count_nonzero_only=True)\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5)  # Add a 5-second delay to stabilize the initial state    \n",
    "\n",
    "        ###### Report  \n",
    "\n",
    "        print(f\"pruned_finetune model has accuracy on test set={pruned_finetune_model_accuracy:.2f}%\")\n",
    "        print(f\"pruned_finetune model has size={pruned_finetune_model_size:.2f} \")\n",
    "        print(f\"The time inference of pruned_finetune model is ={t_pruned_finetune_model}\") \n",
    "        print(f\"The number of parametrs of pruned_finetune model is:{num_parm_pruned_finetune_model}\") \n",
    "\n",
    "        print(f\"Energy Consumption of pruned_finetune model: {pruned_finetune_energy_consumption:.3f}\")\n",
    "        print(f\"total memory usage of pruned_finetune model':{pruned_finetune_total_memory_diff} \")\n",
    "        print(f\"cpu usage of pruned_finetune model':{pruned_finetune_cpu_usage:.3f} %\")\n",
    "\n",
    "\n",
    "\n",
    "        #Update my Eva dictionary\n",
    "        Eva.update({'pruned and finetune model accuracy': pruned_finetune_model_accuracy,\n",
    "                    'time inference of pruned and finetune model': t_pruned_finetune_model,\n",
    "                    'number parmameters of pruned and finetune model': num_parm_pruned_finetune_model,\n",
    "                    'size of pruned and finetune model': pruned_finetune_model_size, \n",
    "                    'energy consumption of pruned and finetune model':pruned_finetune_energy_consumption,\n",
    "                    'total memory usage of pruned and finetune model':pruned_finetune_total_memory_diff,\n",
    "                    'cpu usage of pruned and finetune model':pruned_finetune_cpu_usage\n",
    "                   })\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5) \n",
    "\n",
    "\n",
    "        Base_model_accuracy.append(Eva['base model accuracy'])\n",
    "        T_base_model.append(Eva['time inference of base model'])\n",
    "        Num_parm_base_model.append(int(Eva['number parmameters of base model']))\n",
    "        Base_model_size.append(int(Eva['size of base model']))\n",
    "        Base_Energy_Consumption.append(Eva['energy consumption of base model'])\n",
    "        Base_Cpu_Usage.append(Eva['cpu usage of base model'])\n",
    "        Base_Memory_Usage.append(Eva['total memory usage of base model'])\n",
    "\n",
    "        Pruned_model_accuracy.append(Eva['pruned model accuracy'])\n",
    "        T_pruned_model.append(Eva['time inference of pruned model'])\n",
    "        Num_parm_pruned_model.append(int(Eva['number parmameters of pruned model']))\n",
    "        Pruned_model_size.append(int(Eva['size of pruned model']))\n",
    "        Pruned_Energy_Consumption.append(Eva['energy consumption of pruned model'])\n",
    "        Pruned_Cpu_Usage.append(Eva['cpu usage of pruned model'])\n",
    "        Pruned_Memory_Usage.append(Eva['total memory usage of pruned model'])\n",
    "\n",
    "\n",
    "        Pruned_finetune_model_accuracy.append(Eva['pruned and finetune model accuracy'])\n",
    "        T_pruned_finetune_model.append(Eva['time inference of pruned and finetune model'])\n",
    "        Num_parm_pruned_finetune_model.append(int(Eva['number parmameters of pruned and finetune model']))\n",
    "        Pruned_finetune_model_size.append(int(Eva['size of pruned and finetune model']))\n",
    "        Pruned_finetune_Energy_Consumption.append(Eva['energy consumption of pruned and finetune model'])\n",
    "        Pruned_finetune_Cpu_Usage.append(Eva['cpu usage of pruned and finetune model'])\n",
    "        Pruned_finetune_Memory_Usage.append(Eva['total memory usage of pruned and finetune model'])\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4446e440",
   "metadata": {},
   "source": [
    "### Computing Mean and Std of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf3cc405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All measurement about pruning process of sparsity:90.0% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Ave of base model accuracy': 0.868,\n",
       " 'Std of base model accuracy': 0.003,\n",
       " 'Ave of time inference of base model': 24.322,\n",
       " 'Std of time inference of base model': 3.169,\n",
       " 'Ave of number parmameters of base model': 69143,\n",
       " 'Std of number parmameters of base model': 0.0,\n",
       " 'Ave of base model size': 279991,\n",
       " 'Std of base model size': 0.0,\n",
       " 'Ave of energy consumption of base model': 753.1753656629735,\n",
       " 'Std of energy consumption of base model': 307.8873957658013,\n",
       " 'Ave of cpu usage of base model': 54.55,\n",
       " 'Std of cpu usage of base model': 38.961583643378766,\n",
       " 'Ave of memory usage of base model': 9356,\n",
       " 'Std of memory usage of base model': 5481.491767758116,\n",
       " 'Ave of pruned model accuracy': 0.761,\n",
       " 'Std of pruned model accuracy': 0.021,\n",
       " 'Ave of time inference of pruned model': 26.022,\n",
       " 'Std of time inference of pruned model': 2.076,\n",
       " 'Ave of number parmameters of pruned model': 6935,\n",
       " 'Std of number parmameters of pruned model': 0.0,\n",
       " 'Ave of pruned model size': 179403,\n",
       " 'Std of pruned_model_size': 0.0,\n",
       " 'Ave of energy consumption of pruned model': 1073.1948908919235,\n",
       " 'Std of energy consumption of pruned model': 698.24162368219,\n",
       " 'Ave of cpu usage of pruned model': 43.3,\n",
       " 'Std of cpu usage of pruned model': 20.36467529817257,\n",
       " 'Ave of memory usage of pruned model': 7792,\n",
       " 'Std of memory usage of pruned model': 79.19595949289332,\n",
       " 'Ave of pruned finetune model accuracy': 0.849,\n",
       " 'Std of pruned finetune model accuracy': 0.004,\n",
       " 'Ave of time inference of pruned finetune model': 25.478,\n",
       " 'Std of time inference of pruned finetune model': 2.831,\n",
       " 'Ave of number parmameters of pruned finetune model': 6935,\n",
       " 'Std of number parmameters of pruned finetune model': 0.0,\n",
       " 'Ave of pruned finetune model size': 179627,\n",
       " 'Std of pruned finetune model size': 0.0,\n",
       " 'Ave of energy consumption of pruned_finetune model': 831.5284162987168,\n",
       " 'Std of energy consumption of pruned_finetune model': 326.05593298515146,\n",
       " 'Ave of cpu usage of pruned_finetune model': 44.05,\n",
       " 'Std of cpu usage of pruned_finetune model': 23.26381310103741,\n",
       " 'Ave of memory usage of pruned_finetune model': 7780,\n",
       " 'Std of memory usage of pruned_finetune model': 1566.9486271093892}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eva_final=dict()\n",
    "base_model_accuracy_mean = stat.mean(Base_model_accuracy)\n",
    "base_model_accuracy_std =  stat.stdev(Base_model_accuracy)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(base_model_accuracy_mean,base_model_accuracy_std)\n",
    "\n",
    "\n",
    "Eva_final.update({'Ave of base model accuracy':float(format(base_model_accuracy_mean, '.3f'))})\n",
    "Eva_final.update({'Std of base model accuracy':float(format(base_model_accuracy_std, '.3f'))})\n",
    "                 \n",
    "t_base_model_mean =stat.mean(T_base_model)\n",
    "t_base_model_std =stat.stdev(T_base_model)  \n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of time inference of base model':float(format(t_base_model_mean, '.3f'))})\n",
    "Eva_final.update({'Std of time inference of base model':float(format(t_base_model_std, '.3f'))})\n",
    "\n",
    "\n",
    "num_parm_base_model_mean = stat.mean(Num_parm_base_model)\n",
    "num_parm_base_model_std = stat.stdev(Num_parm_base_model)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of number parmameters of base model':num_parm_base_model_mean})\n",
    "Eva_final.update({'Std of number parmameters of base model':num_parm_base_model_std})\n",
    "\n",
    "base_model_size_mean = stat.mean(Base_model_size)\n",
    "base_model_size_std = stat.stdev(Base_model_size)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of base model size':base_model_size_mean})\n",
    "Eva_final.update({'Std of base model size':base_model_size_std})\n",
    "\n",
    "\n",
    "base_energy_consumption_mean = stat.mean(Base_Energy_Consumption)\n",
    "base_energy_consumption_std = stat.stdev(Base_Energy_Consumption)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of energy consumption of base model':base_energy_consumption_mean })\n",
    "Eva_final.update({'Std of energy consumption of base model':base_energy_consumption_std})\n",
    "\n",
    "\n",
    "base_cpu_usage_mean = stat.mean(Base_Cpu_Usage)\n",
    "base_cpu_usage_std = stat.stdev(Base_Cpu_Usage)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of cpu usage of base model':base_cpu_usage_mean})\n",
    "Eva_final.update({'Std of cpu usage of base model':base_cpu_usage_std})\n",
    "\n",
    "base_memory_usage_mean = stat.mean(Base_Memory_Usage)\n",
    "base_memory_usage_std = stat.stdev(Base_Memory_Usage)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of memory usage of base model':base_memory_usage_mean})\n",
    "Eva_final.update({'Std of memory usage of base model':base_memory_usage_std})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################\n",
    "\n",
    "pruned_model_accuracy_mean =stat.mean(Pruned_model_accuracy)\n",
    "pruned_model_accuracy_std = stat.stdev(Pruned_model_accuracy)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of pruned model accuracy':float(format(pruned_model_accuracy_mean, '.3f'))})\n",
    "Eva_final.update({'Std of pruned model accuracy':float(format(pruned_model_accuracy_std, '.3f'))})\n",
    "                 \n",
    "\n",
    "t_pruned_model_mean = stat.mean(T_pruned_model)\n",
    "t_pruned_model_std =stat.stdev(T_pruned_model)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of time inference of pruned model':float(format(t_pruned_model_mean, '.3f'))})\n",
    "Eva_final.update({'Std of time inference of pruned model':float(format(t_pruned_model_std, '.3f'))})\n",
    "\n",
    "num_parm_pruned_model_mean = stat.mean(Num_parm_pruned_model)\n",
    "num_parm_pruned_model_std = stat.stdev(Num_parm_pruned_model)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of number parmameters of pruned model':num_parm_pruned_model_mean})\n",
    "Eva_final.update({'Std of number parmameters of pruned model':num_parm_pruned_model_std})\n",
    "\n",
    "pruned_model_size_mean =stat.mean( Pruned_model_size)\n",
    "pruned_model_size_std = stat.stdev(Pruned_model_size)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of pruned model size':pruned_model_size_mean})\n",
    "Eva_final.update({'Std of pruned_model_size':pruned_model_size_std })\n",
    "\n",
    "pruned_energy_consumption_mean = stat.mean(Pruned_Energy_Consumption)\n",
    "pruned_energy_consumption_std = stat.stdev(Pruned_Energy_Consumption)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of energy consumption of pruned model':pruned_energy_consumption_mean })\n",
    "Eva_final.update({'Std of energy consumption of pruned model':pruned_energy_consumption_std})\n",
    "\n",
    "\n",
    "pruned_cpu_usage_mean = stat.mean(Pruned_Cpu_Usage)\n",
    "pruned_cpu_usage_std = stat.stdev(Pruned_Cpu_Usage)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of cpu usage of pruned model':pruned_cpu_usage_mean})\n",
    "Eva_final.update({'Std of cpu usage of pruned model':pruned_cpu_usage_std})\n",
    "\n",
    "pruned_memory_usage_mean = stat.mean(Pruned_Memory_Usage)\n",
    "pruned_memory_usage_std = stat.stdev(Pruned_Memory_Usage)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of memory usage of pruned model':pruned_memory_usage_mean})\n",
    "Eva_final.update({'Std of memory usage of pruned model':pruned_memory_usage_std})\n",
    "\n",
    "\n",
    "#################################\n",
    "pruned_finetune_model_accuracy_mean =stat.mean(Pruned_finetune_model_accuracy)\n",
    "pruned_finetune_model_accuracy_std = stat.stdev(Pruned_finetune_model_accuracy)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of pruned finetune model accuracy':float(format(pruned_finetune_model_accuracy_mean, '.3f'))})\n",
    "Eva_final.update({'Std of pruned finetune model accuracy':float(format(pruned_finetune_model_accuracy_std, '.3f'))})                 \n",
    "\n",
    "t_pruned_finetune_model_mean =stat.mean(T_pruned_finetune_model)\n",
    "t_pruned_finetune_model_std =stat.stdev(T_pruned_finetune_model)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of time inference of pruned finetune model':float(format(t_pruned_finetune_model_mean,'.3f'))})\n",
    "Eva_final.update({'Std of time inference of pruned finetune model':float(format(t_pruned_finetune_model_std,'.3f'))})\n",
    "\n",
    "num_parm_pruned_finetune_model_mean =stat.mean(Num_parm_pruned_finetune_model)\n",
    "num_parm_pruned_finetune_model_std = stat.stdev(Num_parm_pruned_finetune_model)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of number parmameters of pruned finetune model':num_parm_pruned_finetune_model_mean})\n",
    "Eva_final.update({'Std of number parmameters of pruned finetune model':num_parm_pruned_finetune_model_std })\n",
    "\n",
    "pruned_finetune_model_size_mean = stat.mean(Pruned_finetune_model_size)\n",
    "pruned_finetune_model_size_std = stat.stdev(Pruned_finetune_model_size)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of pruned finetune model size':pruned_finetune_model_size_mean})\n",
    "Eva_final.update({'Std of pruned finetune model size':pruned_finetune_model_size_std})\n",
    "\n",
    "\n",
    "pruned_finetune_energy_consumption_mean = stat.mean(Pruned_finetune_Energy_Consumption)\n",
    "pruned_finetune_energy_consumption_std = stat.stdev(Pruned_finetune_Energy_Consumption)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of energy consumption of pruned_finetune model':pruned_finetune_energy_consumption_mean })\n",
    "Eva_final.update({'Std of energy consumption of pruned_finetune model':pruned_finetune_energy_consumption_std})\n",
    "\n",
    "\n",
    "pruned_finetune_cpu_usage_mean = stat.mean(Pruned_finetune_Cpu_Usage)\n",
    "pruned_finetune_cpu_usage_std = stat.stdev(Pruned_finetune_Cpu_Usage)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of cpu usage of pruned_finetune model':pruned_finetune_cpu_usage_mean})\n",
    "Eva_final.update({'Std of cpu usage of pruned_finetune model':pruned_finetune_cpu_usage_std})\n",
    "\n",
    "pruned_finetune_memory_usage_mean = stat.mean(Pruned_finetune_Memory_Usage)\n",
    "pruned_finetune_memory_usage_std = stat.stdev(Pruned_finetune_Memory_Usage)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of memory usage of pruned_finetune model':pruned_finetune_memory_usage_mean})\n",
    "Eva_final.update({'Std of memory usage of pruned_finetune model':pruned_finetune_memory_usage_std})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################\n",
    "\n",
    "\n",
    "print(f\"All measurement about pruning process of sparsity:{sparsity*100}% \")   \n",
    "Eva_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a0b33e",
   "metadata": {},
   "source": [
    "### Recording results on txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "577bf51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Cora'\n",
    "Pruning_Method='Grain_Pruning'\n",
    "max_epoch = 100\n",
    "resume = True\n",
    "result_folder ='pathresult/'\n",
    "if not os.path.exists(result_folder):\n",
    "    os.makedirs(result_folder)\n",
    "\n",
    "\n",
    "\n",
    "file_name = result_folder+Pruning_Method+'_'+'with sparsity of'+'_'+str(sparsity)+'_on_'+dataset_name+'_'+'Node classification task'+'_'+str(max_epoch)+'.txt'\n",
    "if not os.path.exists(file_name):\n",
    "        with open(file_name, 'w') as f:\n",
    "                f.write('%s:%s\\n'%('dataset_name', 'Cora'))\n",
    "                f.write('%s:%s\\n'%('Task', 'Node Classification'))\n",
    "                f.write('%s:%s\\n'%('max_epoch', max_epoch))\n",
    "                f.write('%s:%s\\n'%('sparsity', sparsity))\n",
    "                for key, value in Eva_final.items():\n",
    "                    f.write('%s:%s\\n'%(key, value))\n",
    "                    \n",
    "                for key, value in Eva_measure.items():\n",
    "                    f.write('%s:%s\\n' % (key, ','.join(map(str, value)))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00167381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ab4fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f9f7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a3d9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2176b444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a5e0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4858b529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d388a76d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc0883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
