{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e2ef12d",
   "metadata": {},
   "source": [
    "Grain Pruning Method on Link Prediction  Task of Cora Dataset\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affc6a1c",
   "metadata": {},
   "source": [
    "###  A library for data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abf2d48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x210064ec7c0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import os.path as osp\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import numpy as np\n",
    "import psutil\n",
    "import itertools\n",
    "import tracemalloc\n",
    "import gc\n",
    "import statistics as stat\n",
    "import datatable as dt\n",
    "import dill\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.seed import seed_everything as th_seed\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import Planetoid, WebKB, WikipediaNetwork\n",
    "from torch_geometric.nn import GAE, VGAE, GCNConv\n",
    "from torch_geometric.seed import seed_everything as th_seed\n",
    "\n",
    "from typing import Any, Dict, List, Tuple, Union, Optional\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset,IterableDataset\n",
    "from utils.utils import set_dirs, update_config_with_model_dims\n",
    "from utils.loss_functions import  JointLoss\n",
    "from torch.nn.utils.prune import global_unstructured, L1Unstructured\n",
    "from utils.loss_functions import JointLoss\n",
    "from utils.model_plot import save_auc_plot, save_loss_plot\n",
    "from utils.model_utils import GAEWrapper\n",
    "from utils.utils import set_dirs, set_seed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd44081d",
   "metadata": {},
   "source": [
    "### Sparsity\n",
    "- The sparsity is the parameter that is determines the rate of pruning across the layer. It is a value in range(0,0.1,1). This parameter is fixed for this notebook and change for remaining experiment. We determine it before training process. Here is all values of sparsities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1b13571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1, 0.2, 0.3, 0.4,  0.5, 0.6 , 0.7,  0.8, 0.9\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d854daaf",
   "metadata": {},
   "source": [
    "### Functions for Measuring criterias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b84c45ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_num_parameters(model: nn.Module, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the total number of parameters of model\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    num_counted_elements = 0\n",
    "    for name, param in model.autoencoder.gae.encoder.linear1.named_parameters():\n",
    "     \n",
    "    \n",
    "\n",
    "        if count_nonzero_only:\n",
    "            num_counted_elements += param.count_nonzero()\n",
    "        else:\n",
    "            num_counted_elements += param.numel()\n",
    "    return num_counted_elements\n",
    "\n",
    "\n",
    "# Function to get CPU usage\n",
    "def get_cpu_usage():\n",
    "    return psutil.cpu_percent(interval=1)\n",
    "\n",
    "\n",
    "\n",
    "# Function to approximate power consumption (Assume some average power usage per CPU percentage point)\n",
    "def estimate_power_usage(cpu_usage):\n",
    "    base_power_usage = 10  # Assumed base power usage in watts\n",
    "    power_per_percent = 0.5  # Assumed additional watts per CPU usage percent\n",
    "    return base_power_usage + (power_per_percent * cpu_usage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a8110b",
   "metadata": {},
   "source": [
    "### Functions for pruning and loading pruned model for reevaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a54dd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## New\n",
    "def fine_grained_prune(tensor: torch.Tensor, sparsity : float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    magnitude-based pruning for single tensor\n",
    "    :param tensor: torch.(cuda.)Tensor, weight of conv/fc layer\n",
    "    :param sparsity: float, pruning sparsity\n",
    "        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n",
    "    :return:\n",
    "        torch.(cuda.)Tensor, mask for zeros\n",
    "    \"\"\"\n",
    "    sparsity = min(max(0.0, sparsity), 1.0)\n",
    "    if sparsity == 1.0:\n",
    "        tensor.zero_()\n",
    "        return torch.zeros_like(tensor)\n",
    "    elif sparsity == 0.0:\n",
    "        return torch.ones_like(tensor)\n",
    "\n",
    "    num_elements = tensor.numel()\n",
    "\n",
    "    num_zeros = round(num_elements * sparsity)\n",
    "    importance = tensor.abs()\n",
    "    threshold = importance.view(-1).kthvalue(num_zeros).values\n",
    "    mask = torch.gt(importance, threshold)\n",
    "    tensor.mul_(mask)\n",
    "\n",
    "    return mask\n",
    "\n",
    "class FineGrainedPruner:\n",
    "    def __init__(self, model, sparsity_dict):\n",
    "        self.masks = FineGrainedPruner.prune(model, sparsity_dict)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def apply(self, model):\n",
    "        for name, param in model.autoencoder.gae.encoder.linear1.named_parameters():\n",
    "            if name in self.masks:\n",
    "                param *= self.masks[name]\n",
    "\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def prune(model, sparsity_dict):\n",
    "        masks = dict()\n",
    "        for name, param in model.autoencoder.gae.encoder.linear1.named_parameters():\n",
    "            if param.dim() > 1: # we only prune conv and fc weights\n",
    "                if isinstance(sparsity_dict, dict):\n",
    "                    masks[name] = fine_grained_prune(param, sparsity_dict[name])\n",
    "                else:\n",
    "                    assert(sparsity_dict < 1 and sparsity_dict >= 0)\n",
    "                    if sparsity_dict > 0:\n",
    "                        masks[name] = fine_grained_prune(param, sparsity_dict)\n",
    "        return masks\n",
    " \n",
    "\n",
    "def state_sparse_model(model, eval_acc=None, epoch=None):\n",
    "    model.autoencoder.state_dict()\n",
    "    #state_dict = model.state_dict()\n",
    "    state_dict = model.autoencoder.state_dict()\n",
    "    compressed_state = {}\n",
    "    \n",
    "    for k, v in state_dict.items():\n",
    "        if torch.is_tensor(v):\n",
    "            mask = v != 0\n",
    "            if mask.any():  # Only compress if there are non-zeros\n",
    "                compressed_state[k] = {\n",
    "                    'shape': v.shape,\n",
    "                    'values': v[mask]  # Store only non-zero values\n",
    "                }\n",
    "            else:\n",
    "                compressed_state[k] = v  # Keep original if all zeros\n",
    "        else:\n",
    "            compressed_state[k] = v\n",
    "    \n",
    "    return {'net': compressed_state, 'epoch': epoch, 'acc': eval_acc}\n",
    "\n",
    "def load_sparse_model(state_path, original_model):\n",
    "    \"\"\"\n",
    "    Loads a model saved in the custom compressed format (non-zero values only).\n",
    "    Reconstructs dense tensors before loading into the model.\n",
    "    \"\"\"\n",
    "    # Load the compressed state_dict\n",
    "    compressed_state = torch.load(state_path)\n",
    "    compressed_weights = compressed_state['net']\n",
    "    \n",
    "    # Initialize a new state_dict for the original model\n",
    "    #new_state_dict = original_model.state_dict()\n",
    "    new_state_dict=original_model.autoencoder.state_dict()\n",
    "    \n",
    "    for k, v in compressed_weights.items():\n",
    "        if isinstance(v, dict) and 'shape' in v and 'values' in v:\n",
    "            # Reconstruct dense tensor from compressed format\n",
    "            dense_tensor = torch.zeros(v['shape'], dtype=v['values'].dtype)\n",
    "            mask = (dense_tensor != 0)  # All False initially\n",
    "            # We need to know the positions of non-zero values (if available)\n",
    "            # If indices were saved, use them; otherwise, assume sequential filling (simpler but may not match original positions)\n",
    "            if 'indices' in v:\n",
    "                # If you saved indices (advanced version)\n",
    "                dense_tensor[v['indices']] = v['values']\n",
    "            else:\n",
    "                # If only values were saved (simpler version)\n",
    "                # Flatten and fill non-zeros sequentially (may not match original positions)\n",
    "                flat_tensor = dense_tensor.view(-1)\n",
    "                flat_tensor[:len(v['values'])] = v['values']\n",
    "                dense_tensor = flat_tensor.reshape(v['shape'])\n",
    "            \n",
    "            new_state_dict[k] = dense_tensor\n",
    "        else:\n",
    "            # If it's a normal tensor (e.g., biases, batch norm stats)\n",
    "            new_state_dict[k] = v\n",
    "\n",
    "    \n",
    "    # Load the reconstructed state_dict\n",
    "    original_model.autoencoder.load_state_dict(new_state_dict, strict=False)\n",
    "    # original_model.load_state_dict(new_state_dict, strict=False)\n",
    "    return original_model\n",
    "\n",
    "def load_and_evaluate_pruned_model(config, model_path):\n",
    "        \n",
    "        # Instantiate the model\n",
    "        model = NESS(config)\n",
    "        # Load the pruned model\n",
    "        sparse_model = load_sparse_model(model_path, model)\n",
    "        print(\"Pruned model loaded.\")\n",
    "        return  sparse_model \n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861aea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aabed170",
   "metadata": {},
   "source": [
    "### Start loading data\n",
    "\n",
    "#### functions for data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39bb42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this cell there some class and function for loading dataset and preprocessing\n",
    "\n",
    "class GraphLoader:\n",
    "    \"\"\"\n",
    "    Data loader class for graph data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict[str, Any], dataset_name: str, kwargs: Dict[str, Any] = {}) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the GraphLoader.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config : Dict[str, Any]\n",
    "            Dictionary containing options and arguments.\n",
    "        dataset_name : str\n",
    "            Name of the dataset to load.\n",
    "        kwargs : Dict[str, Any], optional\n",
    "            Dictionary for additional parameters if needed, by default {}.\n",
    "        \"\"\"\n",
    "        # Get config\n",
    "        self.config = config\n",
    "        # Set the seed\n",
    "        th_seed(config[\"seed\"])\n",
    "        # Set the paths\n",
    "        paths = config[\"paths\"]\n",
    "        # data > dataset_name\n",
    "        file_path = os.path.join(paths[\"data\"], dataset_name)\n",
    "        # Get the datasets\n",
    "        self.train_data, self.validation_data, self.test_data = self.get_dataset(dataset_name, file_path)        \n",
    "        \n",
    "\n",
    "    def get_dataset(self, dataset_name: str, file_path: str) -> Tuple[Data, Data, Data]:\n",
    "        \"\"\"\n",
    "        Returns the training, validation, and test datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset_name : str\n",
    "            Name of the dataset to load.\n",
    "        file_path : str\n",
    "            Path to the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Data, Data, Data]\n",
    "            Training, validation, and test datasets.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize Graph dataset class\n",
    "        graph_dataset = GraphDataset(self.config, datadir=file_path, dataset_name=dataset_name)\n",
    "        \n",
    "        # Load Training, Validation, Test datasets\n",
    "        train_data, val_data, test_data = graph_dataset._load_data()\n",
    "        \n",
    "        # Generate static subgraphs from training set\n",
    "        train_data = self.generate_subgraphs(train_data)\n",
    "  \n",
    "        # Return\n",
    "        return train_data, val_data, test_data\n",
    "    \n",
    "    \n",
    "    def generate_subgraphs(self, train_data: Data) -> List[Data]:\n",
    "        \"\"\"\n",
    "        Generates subgraphs from the training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_data : Data\n",
    "            Training data containing the graph.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[Data]\n",
    "            List of subgraphs generated from the training data.\n",
    "        \"\"\"\n",
    "        # Initialize list to hold subgraphs\n",
    "        subgraphs = [train_data]\n",
    "\n",
    "        # Check if we are generating subgraphs from the graph. If False, we are in standard GAE mode\n",
    "        if self.config[\"n_subgraphs\"] > 1:\n",
    "                \n",
    "            # Generate subgraphs\n",
    "            for i in range(self.config[\"n_subgraphs\"]):\n",
    "                \n",
    "                # Change random seed\n",
    "                th_seed(i)\n",
    "                \n",
    "                partition = 1.0/(self.config[\"n_subgraphs\"]-i)\n",
    "                \n",
    "                # For the last subgraph, get 95% of the remaining graph. if num_val=1.0, RandomLinkSplit will raise error\n",
    "                if partition == 1.0:\n",
    "                    partition = 0.95\n",
    "                    \n",
    "                random_link_split = T.RandomLinkSplit(num_val=partition, \n",
    "                                                      num_test=0, \n",
    "                                                      is_undirected=True, \n",
    "                                                      split_labels=True, \n",
    "                                                      add_negative_train_samples=False)\n",
    "\n",
    "                # get a subgraph from training data\n",
    "                train_data, train_subgraph, _ = random_link_split(train_data)\n",
    "                \n",
    "                # Make sure that we are using only the nodes within the subgraph by overwriting the edge index \n",
    "                # with positive edge index + positive edge index reversed in direction (to make it undirected)\n",
    "                pos_swapped = train_subgraph.pos_edge_label_index[[1,0],:] \n",
    "                train_subgraph.edge_index = torch.cat((train_subgraph.pos_edge_label_index, pos_swapped), dim=1)\n",
    "                \n",
    "                # Remove negative edge attributes. We want to sample negative samples during training\n",
    "                # Masks are also not needed\n",
    "                if hasattr(train_subgraph, \"neg_edge_label_index\"):\n",
    "                    delattr(train_subgraph, \"neg_edge_label_index\")\n",
    "                    delattr(train_subgraph, \"neg_edge_label\")\n",
    "                    \n",
    "                if hasattr(train_subgraph, \"train_mask\"):\n",
    "                    delattr(train_subgraph, \"train_mask\")\n",
    "                    delattr(train_subgraph, \"val_mask\")\n",
    "                    delattr(train_subgraph, \"test_mask\")\n",
    "\n",
    "\n",
    "                # store the sampled subgraph\n",
    "                subgraphs = [train_subgraph] + subgraphs\n",
    "                       \n",
    "        # Change random seed back to original\n",
    "        th_seed(self.config[\"seed\"])\n",
    "        \n",
    "        # Return all subgraphs and original larger graph\n",
    "        return subgraphs\n",
    "\n",
    "    \n",
    "def get_transform(options):\n",
    "    \"\"\"Splits data to train, validation and test, and moves them to the device\"\"\"\n",
    "    transform = T.Compose([\n",
    "        T.NormalizeFeatures(),\n",
    "        T.ToDevice(options[\"device\"]),\n",
    "        T.RandomLinkSplit(num_val=0.05, \n",
    "                          num_test=0.15, \n",
    "                          is_undirected=True,\n",
    "                          split_labels=True, \n",
    "                          add_negative_train_samples=False),\n",
    "        ])\n",
    "        \n",
    "    return transform\n",
    "\n",
    "\n",
    "class GraphDataset:\n",
    "    \"\"\"\n",
    "    Dataset class for graph data format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict[str, Any], datadir: str, dataset_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the GraphDataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config : Dict[str, Any]\n",
    "            Dictionary containing options and arguments.\n",
    "        datadir : str\n",
    "            The path to the data directory.\n",
    "        dataset_name : str\n",
    "            Name of the dataset to load.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.paths = config[\"paths\"]\n",
    "        self.dataset_name = dataset_name\n",
    "        self.data_path = os.path.join(self.paths[\"data\"], 'Planetoid')\n",
    "        self.transform = get_transform(config)\n",
    "\n",
    "        \n",
    "    def _load_data(self) -> Tuple[Data, Data, Data]:\n",
    "        \"\"\"\n",
    "        Loads one of many available datasets and returns features and labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Data, Data, Data]\n",
    "            Training, validation, and test datasets.\n",
    "        \"\"\"\n",
    "        if self.dataset_name.lower() in ['cora', 'citeseer', 'pubmed']:\n",
    "            # Get the dataset\n",
    "            dataset = Planetoid(self.data_path, self.dataset_name, split=\"random\", transform = self.transform)\n",
    "        elif  self.dataset_name.lower() in ['chameleon']:\n",
    "            # Get the dataset\n",
    "            dataset = WikipediaNetwork(root=self.data_path, name=self.dataset_name, transform = self.transform)\n",
    "        elif  self.dataset_name.lower() in [\"cornell\", \"texas\", \"wisconsin\"]:\n",
    "            # Get the dataset\n",
    "            dataset = WebKB(root=self.data_path, name=self.dataset_name, transform = self.transform) \n",
    "        else:\n",
    "            print(f\"Given dataset name is not found. Check for typos, or missing condition \")\n",
    "            exit()\n",
    "            \n",
    "        # Data splits\n",
    "        train_data, val_data, test_data = dataset[0]\n",
    "        \n",
    "        # Return\n",
    "        return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248fab14",
   "metadata": {},
   "source": [
    "### Loads arguments and configuration for GNN-based encoder used in NESS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59dc9404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "from argparse import ArgumentParser\n",
    "from os.path import abspath, dirname\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils.utils import get_runtime_and_model_config, print_config\n",
    "\n",
    "\n",
    "def get_arguments():\n",
    "    # Initialize parser\n",
    "    parser = ArgumentParser()\n",
    "    # Dataset can be provided via command line\n",
    "    parser.add_argument(\"-d\", \"--dataset\", type=str, default=\"cora\")\n",
    "    # Encoder type\n",
    "    parser.add_argument(\"-gnn\", \"--gnn\", type=str, default=\"GNAE\")\n",
    "    # Random seed\n",
    "    parser.add_argument(\"-seed\", \"--seed\", type=int, default=57)\n",
    "    # Whether to use contrastive loss\n",
    "    parser.add_argument(\"-cl\", \"--cl\", type=bool, default=False)\n",
    "    # Whether to add noise to input\n",
    "    parser.add_argument(\"-an\", \"--an\", type=bool, default=True)\n",
    "    # Whether to use GPU.\n",
    "    parser.add_argument(\"-g\", \"--gpu\", dest='gpu', action='store_true', \n",
    "                        help='Used to assign GPU as the device, assuming that GPU is available')\n",
    "    \n",
    "    parser.add_argument(\"-ng\", \"--no_gpu\", dest='gpu', action='store_false', \n",
    "                        help='Used to assign CPU as the device')\n",
    "    parser.set_defaults(gpu=True)\n",
    "    \n",
    "    # GPU device number as in \"cuda:0\". Defaul is 0.\n",
    "    parser.add_argument(\"-dn\", \"--device_number\", type=str, default='0', \n",
    "                        help='Defines which GPU to use. It is 0 by default')\n",
    "    \n",
    "    ### Set parameter for L2-Regularization ##****************\n",
    "    parser.add_argument(\"-reg\", \"--l2_reg\", type=float, default='0', \n",
    "                        help='Defines which GPU to use. It is 0 by default')\n",
    "    parser.add_argument(\"-is_pruned\", \"--is_pruned\", type=bool, default=False)\n",
    "    \n",
    "    \n",
    "      ### Set callback for pruning ##****************\n",
    "    parser.add_argument(\"-callbacks\", \"--callbacks\", type=float, default=None, \n",
    "                        help='Defines which GPU to use. It is None by default')\n",
    "    # Experiment number\n",
    "    parser.add_argument(\"-ex\", \"--experiment\", type=int, default=1)\n",
    "    # Load model saved at specific epoch\n",
    "    parser.add_argument(\"-m\", \"--model_at_epoch\", type=int, default=None)\n",
    "    \n",
    "    # Return parser arguments along with the unknown ones\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_config(args):\n",
    "    # Load runtime config from config folder: ./config/\n",
    "    config = get_runtime_and_model_config(args)\n",
    "    # Define which device to use: GPU or CPU\n",
    "    config[\"device\"] = torch.device('cuda:'+args.device_number if torch.cuda.is_available() and args.gpu else 'cpu')\n",
    "    # Model at specific epoch\n",
    "    config[\"model_at_epoch\"] = args.model_at_epoch\n",
    "    # Indicate which device is being used\n",
    "    config[\"l2_reg\"]=args.l2_reg\n",
    "    config[\"is_pruned\"]=args.is_pruned\n",
    "    config[\"callbacks\"]=args.callbacks\n",
    "    print(f\"Device being used is {config['device']}\")\n",
    "    # Return\n",
    "    return config\n",
    "\n",
    "def print_config_summary(config, args=None):\n",
    "    \"\"\"Prints out summary of options and arguments used\"\"\"\n",
    "    # Summarize config on the screen as a sanity check\n",
    "    print(100 * \"=\")\n",
    "    print(f\"Here is the configuration being used:\\n\")\n",
    "    print_config(config)\n",
    "    print(100 * \"=\")\n",
    "    if args is not None:\n",
    "        print(f\"Arguments being used:\\n\")\n",
    "        print_config(args)\n",
    "        print(100 * \"=\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc8f0e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used is cpu\n"
     ]
    }
   ],
   "source": [
    "# Get parser / command line arguments\n",
    "args = get_arguments()\n",
    "# Get configuration file\n",
    "config = get_config(args)\n",
    "\n",
    "\n",
    "# By default, we are using the name of the dataset. This can be customized.\n",
    "config[\"experiment\"] = config[\"dataset\"]\n",
    "\n",
    "# File name to use when saving results as csv. This can be customized\n",
    "config[\"file_name\"] = config[\"experiment\"] + \"_sub\" + str(config[\"n_subgraphs\"]) + '_seed' + str(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b334b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Here is the configuration being used:\n",
      "\n",
      "+-------------------+----------------------------------------------+\n",
      "|     Parameter     |                    Value                     |\n",
      "+===================+==============================================+\n",
      "| Add noise         | True                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Aggregation       | mean                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Batch size        | 128                                          |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Callbacks         | None                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Contrastive loss  | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Cosine similarity | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Dataset           | cora                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Device            | cpu                                          |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Dropout rate      | 0.500                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Encoder type      | GNAE                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Epochs            | 500                                          |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Experiment        | cora                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| File name         | cora_sub4_seed57                             |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Full graph        | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Isbatchnorm       | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Isdropout         | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Is pruned         | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| L2 reg            | 0                                            |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Learning rate     | 0.010                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Model at epoch    | None                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| N subgraphs       | 4                                            |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Normalize         | True                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Nth epoch         | 10                                           |\n",
      "+-------------------+----------------------------------------------+\n",
      "| P noise           | 0.200                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| P norm            | 2                                            |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Paths             | {'data': './data/', 'results': './results/'} |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Patience          | 3                                            |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Scheduler         | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Seed              | 57                                           |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Tau               | 0.100                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Validate          | True                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Z dim             | 32                                           |\n",
      "+-------------------+----------------------------------------------+\n",
      "====================================================================================================\n",
      "Arguments being used:\n",
      "\n",
      "+----------------+-------+\n",
      "|   Parameter    | Value |\n",
      "+================+=======+\n",
      "| An             | True  |\n",
      "+----------------+-------+\n",
      "| Callbacks      | None  |\n",
      "+----------------+-------+\n",
      "| Cl             | False |\n",
      "+----------------+-------+\n",
      "| Dataset        | cora  |\n",
      "+----------------+-------+\n",
      "| Device number  | 0     |\n",
      "+----------------+-------+\n",
      "| Experiment     | 1     |\n",
      "+----------------+-------+\n",
      "| Gnn            | GNAE  |\n",
      "+----------------+-------+\n",
      "| Gpu            | True  |\n",
      "+----------------+-------+\n",
      "| Is pruned      | False |\n",
      "+----------------+-------+\n",
      "| L2 reg         | 0     |\n",
      "+----------------+-------+\n",
      "| Model at epoch | None  |\n",
      "+----------------+-------+\n",
      "| Seed           | 57    |\n",
      "+----------------+-------+\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Summarize config and arguments on the screen as a sanity check\n",
    "print_config_summary(config, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc5a52d",
   "metadata": {},
   "source": [
    "### NESS Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dde381a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NESS:\n",
    "    \"\"\"\n",
    "    Model: Trains a Graph Autoencoder with a Projection network, using NESS framework.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict):\n",
    "        \"\"\"Initializes the NESS class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config : dict\n",
    "            Configuration dictionary with parameters for training a Graph Autoencoder \n",
    "            using NESS framework.\n",
    "        \"\"\"\n",
    "        # Get config\n",
    "        self.config = config\n",
    "        # Set L2-Regularization Parameters******************\n",
    "        self.l2_reg=config[\"l2_reg\"]\n",
    "        # Set callback for pruning\n",
    "        self.callbacks=config[\"callbacks\"]\n",
    "        \n",
    "        # Define which device to use: GPU, or CPU\n",
    "        self.device = config[\"device\"]\n",
    "        # Create empty lists and dictionary\n",
    "        self.model_dict, self.summary = {}, {}\n",
    "        # Set random seed\n",
    "        set_seed(self.config)\n",
    "        # Set paths for results and initialize some arrays to collect data during training\n",
    "        self._set_paths()\n",
    "        # Set directories i.e. create ones that are missing.\n",
    "        set_dirs(self.config)\n",
    "        # ------Network---------\n",
    "        # Instantiate networks\n",
    "        print(\"Building the models for training and evaluation in NESS framework...\")\n",
    "        # Set Autoencoders i.e. setting loss, optimizer, and device assignment (GPU, or CPU)\n",
    "        self.set_autoencoder()\n",
    "        # Set scheduler (its use is optional)\n",
    "        self._set_scheduler()\n",
    "        # Print out model architecture\n",
    "        self.print_model_summary()\n",
    "        \n",
    "\n",
    "        \n",
    "    def set_autoencoder(self) -> None:\n",
    "        \"\"\"Sets up the autoencoder model, optimizer, and loss.\n",
    "        \n",
    "        This function is responsible for initializing the Graph Autoencoder, setting up \n",
    "        the optimizer, and defining the joint loss.\n",
    "        \"\"\"   \n",
    "        # Instantiate the model for the text Autoencoder\n",
    "        self.autoencoder = GAEWrapper(self.config)\n",
    "        # Add the model and its name to a list to save, and load in the future\n",
    "        self.model_dict.update({\"autoencoder\": self.autoencoder})\n",
    "        # Assign autoencoder to a device\n",
    "        for _, model in self.model_dict.items(): model.to(self.device)\n",
    "        \n",
    "        # Get model parameters\n",
    "        parameters = [model.parameters() for _, model in self.model_dict.items()]\n",
    "        \n",
    "        # Joint loss including contrastive, reconstruction and distance losses\n",
    "        self.joint_loss = None if self.config[\"dataset\"][:4] == \"ogbl\" else JointLoss(self.config)\n",
    "        \n",
    "        # Set optimizer for autoencoder\n",
    "        self.optimizer_ae = self._adam(parameters, lr=self.config[\"learning_rate\"])\n",
    "        \n",
    "        # Add items to summary to be used for reporting later\n",
    "        self.summary.update({\"recon_loss\": []})\n",
    "        \n",
    "\n",
    "    def set_parallelism(self, model) -> None:\n",
    "        \"\"\"Sets up parallelism in training if multiple GPUs are available.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : torch.nn.Module\n",
    "            The model for which the parallelism is to be set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model : torch.nn.Module\n",
    "            The input model wrapped in DataParallel if multiple GPUs are available.\n",
    "        \"\"\"\n",
    "        # If we are using GPU, and if there are multiple GPUs, parallelize training\n",
    "        if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "            print(torch.cuda.device_count(), \" GPUs will be used!\")\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        return model\n",
    "\n",
    "    def fit(self, is_pruned:False, data_loader: DataLoader, callbacks=None) -> None:\n",
    "        \"\"\"Fits model to the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_loader : DataLoader\n",
    "            The DataLoader object that provides the data.\n",
    "        \"\"\"\n",
    "        print(f'is_pruned:{is_pruned}')\n",
    "        # Get data loaders\n",
    "        train_data = data_loader.train_data\n",
    "        validation_data = data_loader.validation_data\n",
    "        test_data = data_loader.test_data\n",
    "\n",
    "        # Placeholders to record losses per batch\n",
    "        self.metrics = {\"tloss_e\": [], \"vloss_e\": [], \"rloss_e\": [], \"zloss_e\": [], \"val_auc\": [], \"tr_auc\": []}\n",
    "        self.val_auc = \"NA\"\n",
    "        self.tr_auc = \"NA\"\n",
    "\n",
    "        # Turn on training mode for the model.\n",
    "        self.set_mode(mode=\"training\")\n",
    "\n",
    "        # Reset best test auc\n",
    "        self.best_val_auc = 0\n",
    "        self.best_epoch = 0\n",
    "        self.patient = 0\n",
    "        self.is_pruned=is_pruned\n",
    "\n",
    "        \n",
    "        # Start joint training of Autoencoder with Projection network\n",
    "        for epoch in range(self.config[\"epochs\"]):\n",
    "            \n",
    "            # Keep a record of epoch\n",
    "            self.epoch = epoch\n",
    "            \n",
    "            # 0 - Update Autoencoder\n",
    "            self.update_autoencoder(train_data, callbacks)\n",
    "            if callbacks is not None:\n",
    "                   for callback in callbacks:\n",
    "                          callback()\n",
    "            \n",
    "            # 1 - Update log message using epoch and batch numbers\n",
    "           \n",
    "            if epoch % 40 == 0:\n",
    "                   self.update_log(epoch)\n",
    "            \n",
    "            # 2 - Clean-up for efficient memory usage.\n",
    "            gc.collect()                \n",
    "                \n",
    "            # 3 - Run Validation\n",
    "            self.run_validation(is_pruned, train_data, validation_data)\n",
    "\n",
    "            # 4 - Change learning rate if scheduler==True\n",
    "            _ = self.scheduler.step() if self.config[\"scheduler\"] else None\n",
    "            \n",
    "            # 5 - Stop training if we run out of patience\n",
    "            if self.patient == self.config[\"patience\"]:\n",
    "                break\n",
    "            \n",
    "        # Get the test performance and computing inference time \n",
    "       \n",
    "        start = time.time()\n",
    "        self.test_auc, self.test_ap = self.autoencoder.single_test(train_data, test_data)\n",
    "        end = time.time()\n",
    "        self.t_inference= end-start\n",
    "\n",
    "        # Save plots of training loss and validation auc\n",
    "        save_loss_plot(self.metrics, self._plots_path)\n",
    "        save_auc_plot(self.metrics, self._plots_path)\n",
    "        \n",
    "        # Convert loss dictionary to a dataframe\n",
    "        loss_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in self.metrics.items()]))\n",
    "        \n",
    "        # Save loss dataframe as csv file for later use\n",
    "        loss_df.to_csv(self._loss_path + \"/losses.csv\")\n",
    "        \n",
    "            \n",
    "    def run_validation(self, is_pruned, train_data: Union[List, torch.Tensor], validation_data: Union[List, torch.Tensor]) -> None:\n",
    "        \"\"\"Runs validation on the trained model and save weights if validation AUC improves.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_data : List or torch.Tensor\n",
    "            The training dataset.\n",
    "\n",
    "        validation_data : List or torch.Tensor\n",
    "            The validation dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set the evaluation mode\n",
    "        self.set_mode(mode=\"evaluation\")\n",
    "        \n",
    "        # Validate every nth epoch. n=1 by default, but it can be changed in the config file\n",
    "        if self.config[\"validate\"]:\n",
    "\n",
    "            # Compute validation AUCs\n",
    "            self.val_auc, _ = self.autoencoder.single_test(train_data, validation_data)\n",
    "                \n",
    "            # Append auc's to the list to use for plots\n",
    "            self.metrics[\"val_auc\"].append(self.val_auc)\n",
    "                \n",
    "        # Save intermediate model on regular intervals\n",
    "        if self.epoch >=self.config[\"nth_epoch\"] and self.epoch % self.config[\"nth_epoch\"] == 0:\n",
    "            \n",
    "            # Check the test auc at this epoch\n",
    "            self.config[\"model_at_epoch\"] = self.epoch\n",
    "            val_auc, _ = self.autoencoder.single_test(train_data, validation_data)\n",
    "\n",
    "            # Update the metrics.\n",
    "            if val_auc > self.best_val_auc:\n",
    "                self.best_val_auc = val_auc\n",
    "                self.best_epoch = self.epoch \n",
    "                self.save_weights(is_pruned)\n",
    "                self.patient = 0\n",
    "            else:\n",
    "                self.patient += 1\n",
    "                    \n",
    "        # Set training mode back\n",
    "        self.set_mode(mode=\"training\")\n",
    "      \n",
    "    \n",
    "\n",
    "    def update_autoencoder(self, subgraphs: List[Data], callbacks) -> None:\n",
    "        \"\"\"Updates autoencoder model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        subgraphs : list of Data\n",
    "            A list that contains subgraphs + original training graph.\n",
    "        \"\"\"\n",
    "        total_loss, contrastive_loss, recon_loss, zrecon_loss = [], [], [], []\n",
    "        \n",
    "        # Last element of the list is the original whole training graph\n",
    "        graph = subgraphs[-1]\n",
    "        \n",
    "        # If len(subgraphs) > 1, it means that we sampled subgraphs from the graph. Else, we have a standard GAE\n",
    "        subgraphs = subgraphs[:-1] if len(subgraphs) > 1 else subgraphs\n",
    "        \n",
    "        # A list to hold list of latents --- will be used to compute contrastive loss\n",
    "        z_list = []\n",
    "        \n",
    "        # Initialize total loss\n",
    "        tloss = None\n",
    "        \n",
    "        # pass subgraphs through model to reconstruct the original graph from subgraphs\n",
    "        for sg in subgraphs:\n",
    "            \n",
    "            # Reference graph\n",
    "            ref_graph = graph if self.config[\"full_graph\"] else sg\n",
    "                        \n",
    "            # Drop edges if True\n",
    "            if self.config[\"add_noise\"]:\n",
    "                sg.edge_index, sg.edge_attr = dropout_adj(sg.edge_index, edge_attr= sg.edge_attr, p=self.config[\"p_noise\"])\n",
    "            \n",
    "            # Forwards pass\n",
    "            z, latent = self.autoencoder(sg.x, sg.edge_index)           \n",
    "\n",
    "            # Reconstruction loss by using GAE's native function\n",
    "            rloss = self.autoencoder.gae.recon_loss(latent, ref_graph.pos_edge_label_index)\n",
    "            \n",
    "            # If the model is a variational model\n",
    "            if self.autoencoder.variational:\n",
    "                rloss = rloss + (1 / ref_graph.num_nodes) * self.autoencoder.gae.kl_loss()\n",
    "            \n",
    "            # Store z to the list\n",
    "            z_list.append(z)\n",
    "            \n",
    "            # total loss\n",
    "            tloss = tloss + rloss if tloss  is not None else rloss\n",
    "            \n",
    "            # Accumulate losses\n",
    "            total_loss.append(tloss)\n",
    "            recon_loss.append(rloss)\n",
    "            \n",
    "        # Clean up\n",
    "        del rloss, tloss\n",
    "        gc.collect()\n",
    "\n",
    "        # Compute the losses\n",
    "        n = len(total_loss)\n",
    "        total_loss = sum(total_loss) / n\n",
    "        recon_loss = sum(recon_loss) / n\n",
    "        \n",
    "        # If the graph is large such as pubmed, push the losses to cpu.\n",
    "        if self.config[\"dataset\"] == \"pubmed\":\n",
    "            total_loss = total_loss.cpu()\n",
    "            recon_loss = recon_loss.cpu()\n",
    "            \n",
    "        # Initiliaze contrastive loss\n",
    "        closs = None\n",
    "        zloss = None\n",
    "        \n",
    "        if self.config[\"contrastive_loss\"] and len(subgraphs)>1:\n",
    "                    \n",
    "            # Generate combinations of z's to compute contrastive loss\n",
    "            z_combinations = self.get_combinations_of_subgraphs(z_list)\n",
    "            \n",
    "            # Compute the contrastive loss for each pair of latent vectors\n",
    "            for z in z_combinations:\n",
    "                # Contrastive loss\n",
    "                zloss = self.joint_loss(z)\n",
    "                \n",
    "                # Total contrastive loss\n",
    "                closs = closs + zloss if closs is not None else zloss\n",
    "\n",
    "            # Mean constrative loss\n",
    "            closs = closs/len(z_combinations)\n",
    "        \n",
    "        # Update total loss\n",
    "        total_loss = total_loss + closs if closs is not None else total_loss\n",
    "        \n",
    "        # Record losses\n",
    "        self.metrics[\"tloss_e\"].append(total_loss.item())\n",
    "        self.metrics[\"rloss_e\"].append(recon_loss.item())\n",
    "        self.metrics[\"zloss_e\"].append(closs.item() if closs is not None else 0)\n",
    "        \n",
    "        # Update Autoencoder params\n",
    "        self._update_model(total_loss, self.optimizer_ae, retain_graph=True,  callbacks=None)\n",
    "        \n",
    "        # Delete loss and associated graph for efficient memory usage\n",
    "        del total_loss, recon_loss, closs, zloss\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    def get_combinations_of_subgraphs(self, z_list: List[Data]) -> List[Tuple[Data, Data]]:\n",
    "        \"\"\"Generates a list of combinations of subgraphs from the list of subgraphs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z_list : list of Data\n",
    "            List of subgraphs e.g. [z1, z2, z3, ...]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of tuple\n",
    "            A list of combinations of subgraphs e.g. [(z1, z2), (z1, z3), ...]\n",
    "        \"\"\"                            \n",
    "        # Compute combinations of subgraphs [(z1, z2), (z1, z3)...]\n",
    "        subgraph_combinations = list(itertools.combinations(z_list, 2))\n",
    "        # List to store the concatenated subgraphs\n",
    "        concatenated_subgraphs_list = []\n",
    "        \n",
    "        # Go through combinations\n",
    "        for (zi, zj) in subgraph_combinations:\n",
    "            # Concatenate xi, and xj, and turn it into a tensor\n",
    "            z = torch.cat((zi, zj), dim=0)\n",
    "            \n",
    "            # Add it to the list\n",
    "            concatenated_subgraphs_list.append(z)\n",
    "        \n",
    "        # Return the list of combination of subgraphs\n",
    "        return concatenated_subgraphs_list\n",
    "    \n",
    "    def clean_up_memory(self, losses: List) -> None:\n",
    "        \"\"\"Deletes losses with attached graph, and cleans up memory.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        losses : list\n",
    "            List of loss values to be deleted.\n",
    "        \"\"\"\n",
    "        for loss in losses: del loss\n",
    "        gc.collect()\n",
    "\n",
    "    def update_log(self, epoch: int) -> None:\n",
    "        \"\"\"Updates the messages displayed during training and evaluation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch : int\n",
    "            The current epoch number.\n",
    "        \"\"\"\n",
    "        # For the first epoch, add losses for batches since we still don't have loss for the epoch\n",
    "        if epoch < 1:\n",
    "            description = f\"Epoch:[{epoch - 1}], Total loss:{self.metrics['tloss_e'][-1]:.4f}\"\n",
    "            description += f\", X recon loss:{self.metrics['rloss_e'][-1]:.4f}\"\n",
    "            if self.config[\"contrastive_loss\"]:\n",
    "                description += f\", contrastive loss:{self.metrics['zloss_e'][-1]:.6f}\"\n",
    "            description += f\", val auc:{self.val_auc}\"\n",
    "\n",
    "        # For sub-sequent epochs, display only epoch losses.\n",
    "        else:\n",
    "            description = f\"Epoch:[{epoch - 1}] training loss:{self.metrics['tloss_e'][-1]:.4f}\"\n",
    "            description += f\", X recon loss:{self.metrics['rloss_e'][-1]:.4f}\"\n",
    "            if self.config[\"contrastive_loss\"]:\n",
    "                description += f\", contrastive loss:{self.metrics['zloss_e'][-1]:.6f}\"\n",
    "            # Add validation auc\n",
    "            description += f\", val auc:{self.val_auc}\"\n",
    "\n",
    "        # Update the displayed message\n",
    "        print(description)\n",
    "\n",
    "    def set_mode(self, mode: str = \"training\") -> None:\n",
    "        \"\"\"Sets the mode of the models, either as .train(), or .eval().\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mode : str, optional\n",
    "            Mode in which to set the models, by default \"training\".\n",
    "        \"\"\"\n",
    "        for _, model in self.model_dict.items():\n",
    "            model.train() if mode == \"training\" else model.eval()\n",
    "        \n",
    "            \n",
    "    def save_weights(self,is_pruned:bool=False, with_epoch: bool = False) -> None:\n",
    "        \"\"\"Saves weights of the models.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        with_epoch : bool, optional\n",
    "            If True, includes the epoch number in the filename, by default False.\n",
    "        \"\"\"\n",
    "        for model_name in self.model_dict:\n",
    "            \n",
    "            # Check if we want to save the model at a specific epoch\n",
    "            file_name = model_name + \"_\" + str(self.epoch) if with_epoch else model_name\n",
    "            \n",
    "            # Save the model\n",
    "            if is_pruned==False:\n",
    "            \n",
    "                dill.dump(self.model_dict[model_name], open(self._model_path + \"/\" + file_name + \".pt\", 'wb'))\n",
    "            if is_pruned==True:  \n",
    "                dill.dump(self.model_dict[model_name], open(self._model_path + \"/\" + file_name + \"pruned_fine_tuned.pt\", 'wb'))\n",
    "        \n",
    "        print(\"Done with saving models.\")\n",
    "\n",
    "   \n",
    "\n",
    "    def load_models(self, epoch: Optional[int] = None) -> None:\n",
    "        \"\"\"Loads weights saved at the end of the training.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch : int, optional\n",
    "            If provided, loads the weights saved at the specified epoch, by default None.\n",
    "        \"\"\"\n",
    "        for model_name in self.model_dict:\n",
    "            print(f\"is_pruned:{is_pruned}\" )\n",
    "            \n",
    "            # Check if we want to load the model saved at a specific epoch\n",
    "            file_name = model_name + \"_\" + str(epoch) if epoch is not None else model_name\n",
    "\n",
    "            # Load the model\n",
    "            if is_pruned==False:\n",
    "                model = dill.load(open(self._model_path + \"/\" + file_name + \".pt\", 'rb'))\n",
    "            if is_pruned==True:\n",
    "                \n",
    "                model = dill.load(open(self._model_path + \"/\" + file_name + \"pruned_fine_tuned.pt\", 'rb'))\n",
    "            # Register model to the class\n",
    "            setattr(self, model_name, model.eval())\n",
    "            print(f\"--{model_name} is loaded\")\n",
    "        \n",
    "        print(\"Done with loading models.\")\n",
    "\n",
    "    def print_model_summary(self) -> None:\n",
    "        \"\"\"Displays model architectures as a sanity check to see if the models are constructed correctly.\"\"\"\n",
    "        # Summary of the model\n",
    "        description = f\"{40 * '-'}Summary of the models:{40 * '-'}\\n\"\n",
    "        description += f\"{34 * '='} NESS Architecture {34 * '='}\\n\"\n",
    "        description += f\"{self.autoencoder}\\n\"\n",
    "        # Print model architecture\n",
    "        print(description)\n",
    "\n",
    "    def _update_model(self, loss: torch.Tensor, optimizer: torch.optim.Optimizer, retain_graph: bool = True, callbacks=None) -> None:\n",
    "        \"\"\"Does backpropagation and updates the model parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        loss : torch.Tensor\n",
    "            Loss containing computational graph.\n",
    "\n",
    "        optimizer : torch.optim.Optimizer\n",
    "            Optimizer used during training.\n",
    "\n",
    "        retain_graph : bool, optional\n",
    "            If True, retains the computational graph after backpropagation, by default True.\n",
    "        \"\"\"\n",
    "        # Reset optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # Backward propagation to compute gradients\n",
    "        loss.backward(retain_graph=retain_graph)\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        if self.callbacks is not None:\n",
    "            for callback in self.callbacks:\n",
    "                    callback()\n",
    "\n",
    "    def _set_scheduler(self) -> None:\n",
    "        \"\"\"Sets a scheduler for the learning rate of the autoencoder.\"\"\"\n",
    "        # Set scheduler (Its use will be optional)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer_ae, step_size=1, gamma=0.99)\n",
    "\n",
    "    def _set_paths(self) -> None:\n",
    "        \"\"\"Sets paths to be used for saving results at the end of the training.\"\"\"\n",
    "        # Top results directory\n",
    "        self._results_path = os.path.join(self.config[\"paths\"][\"results\"], self.config[\"experiment\"])\n",
    "        # Directory to save model\n",
    "        self._model_path = os.path.join(self._results_path, \"training\", \"model\")\n",
    "        # Directory to save plots as png files\n",
    "        self._plots_path = os.path.join(self._results_path, \"training\", \"plots\")\n",
    "        # Directory to save losses as csv file\n",
    "        self._loss_path = os.path.join(self._results_path, \"training\", \"loss\")\n",
    "\n",
    "    def _adam(self, params: Union[List, Tuple], lr: float = 1e-4,weight_decay: float = 0) -> torch.optim.AdamW:\n",
    "        \"\"\"Sets up AdamW optimizer using model parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : list or tuple\n",
    "            Parameters of the models to optimize.\n",
    "\n",
    "        lr : float, optional\n",
    "            Learning rate, by default 1e-4.\n",
    "\n",
    "        weight_decay: float, optional\n",
    "            L2 regularization coefficient, by default 1e-5.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.optim.AdamW\n",
    "            AdamW optimizer.\n",
    "         \"\"\"\n",
    "        return torch.optim.AdamW(itertools.chain(*params), lr=lr, betas=(0.9, 0.999), eps=1e-07, weight_decay= self.l2_reg)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f57d45a",
   "metadata": {},
   "source": [
    "###  Functions for training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f86388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train( config: Dict, data_loader: IterableDataset,is_pruned:bool=False, save_weights: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Trains the model using provided configuration and data loader.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict\n",
    "        Dictionary containing options.\n",
    "\n",
    "    data_loader : IterableDataset\n",
    "        Pytorch data loader used for training the model.\n",
    "\n",
    "    save_weights : bool, optional\n",
    "        If True, the trained model is saved. By default, it's True.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate model\n",
    "    model = NESS(config)\n",
    "    # Start the clock to measure the training time\n",
    "    start = time.process_time()\n",
    "    # Fit the model to the data\n",
    "    model.fit(is_pruned, data_loader)\n",
    "    # Total time spent on training\n",
    "    training_time = time.process_time() - start\n",
    "    # Report the training time\n",
    "    print(\"Done with training...\")\n",
    "    print(f\"Training time:  {training_time//60} minutes, {training_time%60} seconds\")\n",
    "    \n",
    "\n",
    "    \n",
    "    # Return the best Test set AUC\n",
    "    return model.test_auc, model.test_ap, model.t_inference, model.val_auc, model\n",
    "\n",
    "\n",
    "def main(config: Dict,is_pruned:bool=False ) -> None:\n",
    "    \"\"\"\n",
    "    The main function that starts the execution of the program. Takes the \n",
    "    configuration dictionary as input.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict\n",
    "        Dictionary containing options.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Ser directories (or create if they don't exist)\n",
    "    set_dirs(config)\n",
    "    # Get data loader for first dataset.\n",
    "    ds_loader = GraphLoader(config, dataset_name=config[\"dataset\"])\n",
    "    # Add the number of features in a dataset as the first dimension of the model\n",
    "    config = update_config_with_model_dims(ds_loader, config)\n",
    "    # Start training and save model weights at the end\n",
    "    test_auc, test_ap,t_inference,val_auc, model= train( config, ds_loader, is_pruned=is_pruned,save_weights=True)\n",
    "    # Return best test auc\n",
    "    return test_auc, test_ap, t_inference, val_auc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b3ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db4b5d57",
   "metadata": {},
   "source": [
    "###  Pruning the Model and Re-Evaluate the Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a927fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Settig Sparsity\n",
    "sparsity=0.5\n",
    "\n",
    "# The number of iterations\n",
    "num_iterations= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "802de2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as stat\n",
    "\n",
    "\n",
    "# This is a dictionary to save all measurements. Aftre measuring, we can compute mean and std of each item.\n",
    "Eva_final=dict()\n",
    "\n",
    "# The following are all list of criteria for measurements. \n",
    "# We collect all desired datas of each list across iterations. \n",
    "# Then, we compute average and std of each list.\n",
    "\n",
    "\n",
    "\n",
    "#Base model\n",
    "Base_model_accuracy=[]\n",
    "T_base_model=[]\n",
    "Num_parm_base_model=[]\n",
    "Base_model_size=[]\n",
    "Base_Energy_Consumption=[]\n",
    "Base_Cpu_Usage=[]\n",
    "Base_Memory_Usage=[]\n",
    "\n",
    "#Pruned model\n",
    "Pruned_model_accuracy=[]\n",
    "T_pruned_model=[]\n",
    "Num_parm_pruned_model=[]\n",
    "Pruned_model_size=[]\n",
    "Pruned_Energy_Consumption=[]\n",
    "Pruned_Cpu_Usage=[]\n",
    "Pruned_Memory_Usage=[]\n",
    "\n",
    "#Pruned and finetune model\n",
    "Pruned_finetune_model_accuracy=[]\n",
    "T_pruned_finetune_model=[]\n",
    "Num_parm_pruned_finetune_model=[]\n",
    "Pruned_finetune_model_size=[]\n",
    "Pruned_finetune_Energy_Consumption=[]\n",
    "Pruned_finetune_Cpu_Usage=[]\n",
    "Pruned_finetune_Memory_Usage=[]\n",
    "\n",
    "# Here is the dictionary to record the list of all measurements\n",
    "Eva_measure={'base model accuracy':Base_model_accuracy,\n",
    "            'time inference of base model':T_base_model,\n",
    "            'number parmameters of base model':Num_parm_base_model,\n",
    "            'base model size':Base_model_size,\n",
    "            'energy consumption of base model':Base_Energy_Consumption,\n",
    "            'cpu usage of base model':Base_Cpu_Usage,\n",
    "            'memory usage of base model':Base_Memory_Usage,\n",
    "            'pruned model accuracy': Pruned_model_accuracy,\n",
    "            'time inference of pruned model':T_pruned_model,\n",
    "            'number parmameters of pruned model':Num_parm_pruned_model,\n",
    "            'pruned model size':Pruned_model_size,\n",
    "            'energy consumption of pruned model':Pruned_Energy_Consumption,\n",
    "            'cpu usage of pruned model':Pruned_Cpu_Usage,\n",
    "            'memory usage of pruned model':Pruned_Memory_Usage,\n",
    "            'pruned finetune model accuracy':Pruned_finetune_model_accuracy,\n",
    "            'time inference of pruned finetune model':T_pruned_finetune_model,\n",
    "            'number parmameters of pruned finetune model':Num_parm_pruned_finetune_model,\n",
    "            'pruned finetune model size':Pruned_finetune_model_size,\n",
    "            'energy consumption of pruned_finetune model':Pruned_finetune_Energy_Consumption,\n",
    "            'cpu usage of pruned_finetune model':Pruned_finetune_Cpu_Usage,\n",
    "            'memory usage of pruned_finetune model':Pruned_finetune_Memory_Usage}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99fdc88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Path os saved model\n",
    "results_path = os.path.join(config[\"paths\"][\"results\"], config[\"experiment\"])\n",
    "\n",
    "# Dividing dataset \n",
    "ds_loader = GraphLoader(config, dataset_name=config[\"dataset\"])\n",
    "train_data = ds_loader.train_data\n",
    "validation_data = ds_loader.validation_data\n",
    "test_data = ds_loader.test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b53e42",
   "metadata": {},
   "source": [
    "### Training, Pruning, Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44fdc221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________\n",
      "This is iteration 0\n",
      "Training and evaluation before pruning \n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "is_pruned:False\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3546, X recon loss:0.9534, val auc:0.8978877820989171\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2055, X recon loss:0.8897, val auc:0.9173184519076465\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1614, X recon loss:0.8776, val auc:0.9201520912547531\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1232, X recon loss:0.8566, val auc:0.9204846101577296\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  3.0 minutes, 24.109375 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=188003.00 bit\n",
      "The time inference of base model is =2.0990955000161193\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 20.991\n",
      "total memory usage of base model':16959 \n",
      "cpu usage of base model':0.000 %\n",
      "_______________________________________________________\n",
      "Prune the Model and Re-Evaluate the Accuracy\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.70%\n",
      "pruned model has size=166415.00\n",
      "The time inference of pruned model is =2.278005000029225\n",
      "The number of parametrs of pruned model is:41302\n",
      "Energy Consumption : 24.602\n",
      "total memory usage of pruned model':16024 \n",
      "cpu usage of pruned model':0.000 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "is_pruned:True\n",
      "Epoch:[-1], Total loss:1.9657, X recon loss:0.8002, val auc:NA\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.0818, X recon loss:0.8439, val auc:0.9189665890789225\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.70%\n",
      "pruned_finetune model has size=166523.00 \n",
      "The time inference of pruned_finetune model is =2.274788599985186\n",
      "The number of parametrs of pruned_finetune model is:41302\n",
      "Energy Consumption of pruned_finetune model: 23.203\n",
      "total memory usage of pruned_finetune model':16232 \n",
      "cpu usage of pruned_finetune model':0.000 %\n",
      "________________________________________\n",
      "This is iteration 1\n",
      "Training and evaluation before pruning \n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "is_pruned:False\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3546, X recon loss:0.9534, val auc:0.8978877820989171\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2055, X recon loss:0.8897, val auc:0.9173184519076465\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1614, X recon loss:0.8776, val auc:0.9201520912547531\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1232, X recon loss:0.8566, val auc:0.9204846101577296\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  3.0 minutes, 30.3125 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=188003.00 bit\n",
      "The time inference of base model is =2.0945386000094004\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 22.621\n",
      "total memory usage of base model':15965 \n",
      "cpu usage of base model':0.000 %\n",
      "_______________________________________________________\n",
      "Prune the Model and Re-Evaluate the Accuracy\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.70%\n",
      "pruned model has size=166415.00\n",
      "The time inference of pruned model is =2.2457392999785952\n",
      "The number of parametrs of pruned model is:41302\n",
      "Energy Consumption : 23.356\n",
      "total memory usage of pruned model':15972 \n",
      "cpu usage of pruned model':0.000 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "is_pruned:True\n",
      "Epoch:[-1], Total loss:2.0445, X recon loss:0.8362, val auc:NA\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.0818, X recon loss:0.8440, val auc:0.9189665890789225\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.70%\n",
      "pruned_finetune model has size=166523.00 \n",
      "The time inference of pruned_finetune model is =2.0747236000024714\n",
      "The number of parametrs of pruned_finetune model is:41302\n",
      "Energy Consumption of pruned_finetune model: 21.162\n",
      "total memory usage of pruned_finetune model':16076 \n",
      "cpu usage of pruned_finetune model':0.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________\n",
      "This is iteration 0\n",
      "Training and evaluation before pruning \n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "is_pruned:False\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3546, X recon loss:0.9534, val auc:0.8978877820989171\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2055, X recon loss:0.8897, val auc:0.9173184519076465\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1614, X recon loss:0.8776, val auc:0.9201520912547531\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1232, X recon loss:0.8566, val auc:0.9204846101577296\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  3.0 minutes, 23.15625 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=188003.00 bit\n",
      "The time inference of base model is =2.085691099986434\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 40.150\n",
      "total memory usage of base model':16135 \n",
      "cpu usage of base model':39.100 %\n",
      "_______________________________________________________\n",
      "Prune the Model and Re-Evaluate the Accuracy\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.69%\n",
      "pruned model has size=148047.00\n",
      "The time inference of pruned model is =2.288287199975457\n",
      "The number of parametrs of pruned model is:36717\n",
      "Energy Consumption : 31.807\n",
      "total memory usage of pruned model':16135 \n",
      "cpu usage of pruned model':0.800 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "is_pruned:True\n",
      "Epoch:[-1], Total loss:2.0446, X recon loss:0.8362, val auc:NA\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.0850, X recon loss:0.8454, val auc:0.9184605820526536\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.69%\n",
      "pruned_finetune model has size=148219.00 \n",
      "The time inference of pruned_finetune model is =2.2560684999916703\n",
      "The number of parametrs of pruned_finetune model is:36717\n",
      "Energy Consumption of pruned_finetune model: 23.463\n",
      "total memory usage of pruned_finetune model':16343 \n",
      "cpu usage of pruned_finetune model':0.400 %\n",
      "________________________________________\n",
      "This is iteration 1\n",
      "Training and evaluation before pruning \n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "is_pruned:False\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3546, X recon loss:0.9534, val auc:0.8978877820989171\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2055, X recon loss:0.8897, val auc:0.9173184519076465\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1614, X recon loss:0.8776, val auc:0.9201520912547531\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1232, X recon loss:0.8566, val auc:0.9204846101577296\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  3.0 minutes, 22.546875 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=188003.00 bit\n",
      "The time inference of base model is =2.0897665999946184\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 20.898\n",
      "total memory usage of base model':16083 \n",
      "cpu usage of base model':0.000 %\n",
      "_______________________________________________________\n",
      "Prune the Model and Re-Evaluate the Accuracy\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.69%\n",
      "pruned model has size=148047.00\n",
      "The time inference of pruned model is =2.266035999986343\n",
      "The number of parametrs of pruned model is:36717\n",
      "Energy Consumption : 33.991\n",
      "total memory usage of pruned model':16083 \n",
      "cpu usage of pruned model':12.300 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "is_pruned:True\n",
      "Epoch:[-1], Total loss:2.0446, X recon loss:0.8362, val auc:NA\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.0850, X recon loss:0.8454, val auc:0.9184605820526536\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.69%\n",
      "pruned_finetune model has size=148219.00 \n",
      "The time inference of pruned_finetune model is =2.248616500000935\n",
      "The number of parametrs of pruned_finetune model is:36717\n",
      "Energy Consumption of pruned_finetune model: 23.386\n",
      "total memory usage of pruned_finetune model':16343 \n",
      "cpu usage of pruned_finetune model':0.400 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________\n",
      "This is iteration 0\n",
      "Training and evaluation before pruning \n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "is_pruned:False\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3546, X recon loss:0.9534, val auc:0.8978877820989171\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2055, X recon loss:0.8897, val auc:0.9173184519076465\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1614, X recon loss:0.8776, val auc:0.9201520912547531\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1232, X recon loss:0.8566, val auc:0.9204846101577296\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  3.0 minutes, 27.078125 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=188003.00 bit\n",
      "The time inference of base model is =2.0902169999899343\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 22.574\n",
      "total memory usage of base model':16135 \n",
      "cpu usage of base model':0.800 %\n",
      "_______________________________________________________\n",
      "Prune the Model and Re-Evaluate the Accuracy\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.68%\n",
      "pruned model has size=129743.00\n",
      "The time inference of pruned model is =2.187288500019349\n",
      "The number of parametrs of pruned model is:32131\n",
      "Energy Consumption : 27.013\n",
      "total memory usage of pruned model':16135 \n",
      "cpu usage of pruned model':0.800 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "is_pruned:True\n",
      "Epoch:[-1], Total loss:2.0446, X recon loss:0.8362, val auc:NA\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.1010, X recon loss:0.8516, val auc:0.9166100420708699\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.68%\n",
      "pruned_finetune model has size=129851.00 \n",
      "The time inference of pruned_finetune model is =2.2384730000048876\n",
      "The number of parametrs of pruned_finetune model is:32131\n",
      "Energy Consumption of pruned_finetune model: 22.832\n",
      "total memory usage of pruned_finetune model':16291 \n",
      "cpu usage of pruned_finetune model':0.000 %\n",
      "________________________________________\n",
      "This is iteration 1\n",
      "Training and evaluation before pruning \n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "is_pruned:False\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3546, X recon loss:0.9534, val auc:0.8978877820989171\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2055, X recon loss:0.8897, val auc:0.9173184519076465\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1614, X recon loss:0.8776, val auc:0.9201520912547531\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1232, X recon loss:0.8566, val auc:0.9204846101577296\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  3.0 minutes, 22.890625 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=188003.00 bit\n",
      "The time inference of base model is =2.0838150000199676\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 21.672\n",
      "total memory usage of base model':16076 \n",
      "cpu usage of base model':0.400 %\n",
      "_______________________________________________________\n",
      "Prune the Model and Re-Evaluate the Accuracy\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.68%\n",
      "pruned model has size=129743.00\n",
      "The time inference of pruned model is =2.083481199981179\n",
      "The number of parametrs of pruned model is:32131\n",
      "Energy Consumption : 20.835\n",
      "total memory usage of pruned model':16135 \n",
      "cpu usage of pruned model':1.200 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "is_pruned:True\n",
      "Epoch:[-1], Total loss:2.0446, X recon loss:0.8362, val auc:NA\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.1010, X recon loss:0.8516, val auc:0.9166100420708699\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.68%\n",
      "pruned_finetune model has size=129851.00 \n",
      "The time inference of pruned_finetune model is =2.080245500023011\n",
      "The number of parametrs of pruned_finetune model is:32131\n",
      "Energy Consumption of pruned_finetune model: 35.052\n",
      "total memory usage of pruned_finetune model':16291 \n",
      "cpu usage of pruned_finetune model':1.600 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________\n",
      "This is iteration 0\n",
      "Training and evaluation before pruning \n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "is_pruned:False\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3546, X recon loss:0.9534, val auc:0.8978877820989171\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2055, X recon loss:0.8897, val auc:0.9173184519076465\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1614, X recon loss:0.8776, val auc:0.9201520912547531\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1232, X recon loss:0.8566, val auc:0.9204846101577296\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  3.0 minutes, 21.15625 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=188003.00 bit\n",
      "The time inference of base model is =2.0800860000308603\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 21.633\n",
      "total memory usage of base model':16083 \n",
      "cpu usage of base model':0.800 %\n",
      "_______________________________________________________\n",
      "Prune the Model and Re-Evaluate the Accuracy\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.67%\n",
      "pruned model has size=111375.00\n",
      "The time inference of pruned model is =2.0766425000038\n",
      "The number of parametrs of pruned model is:27546\n",
      "Energy Consumption : 20.766\n",
      "total memory usage of pruned model':16135 \n",
      "cpu usage of pruned model':0.800 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "is_pruned:True\n",
      "Epoch:[-1], Total loss:2.0446, X recon loss:0.8362, val auc:NA\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.1396, X recon loss:0.8664, val auc:0.9156703147363704\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.67%\n",
      "pruned_finetune model has size=111483.00 \n",
      "The time inference of pruned_finetune model is =2.0795932000037283\n",
      "The number of parametrs of pruned_finetune model is:27546\n",
      "Energy Consumption of pruned_finetune model: 22.460\n",
      "total memory usage of pruned_finetune model':16083 \n",
      "cpu usage of pruned_finetune model':0.800 %\n",
      "________________________________________\n",
      "This is iteration 1\n",
      "Training and evaluation before pruning \n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "is_pruned:False\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3546, X recon loss:0.9534, val auc:0.8978877820989171\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2055, X recon loss:0.8897, val auc:0.9173184519076465\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1614, X recon loss:0.8776, val auc:0.9201520912547531\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1232, X recon loss:0.8566, val auc:0.9204846101577296\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  3.0 minutes, 23.453125 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=188003.00 bit\n",
      "The time inference of base model is =2.0832840999937616\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 21.666\n",
      "total memory usage of base model':16135 \n",
      "cpu usage of base model':2.300 %\n",
      "_______________________________________________________\n",
      "Prune the Model and Re-Evaluate the Accuracy\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.67%\n",
      "pruned model has size=111375.00\n",
      "The time inference of pruned model is =2.078000699984841\n",
      "The number of parametrs of pruned model is:27546\n",
      "Energy Consumption : 32.001\n",
      "total memory usage of pruned model':16017 \n",
      "cpu usage of pruned model':0.000 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "is_pruned:True\n",
      "Epoch:[-1], Total loss:2.0446, X recon loss:0.8362, val auc:NA\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.1396, X recon loss:0.8664, val auc:0.9156703147363704\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.67%\n",
      "pruned_finetune model has size=111483.00 \n",
      "The time inference of pruned_finetune model is =2.0958179000299424\n",
      "The number of parametrs of pruned_finetune model is:27546\n",
      "Energy Consumption of pruned_finetune model: 22.216\n",
      "total memory usage of pruned_finetune model':16135 \n",
      "cpu usage of pruned_finetune model':1.200 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________\n",
      "This is iteration 0\n",
      "Training and evaluation before pruning \n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "is_pruned:False\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3546, X recon loss:0.9534, val auc:0.8978877820989171\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2055, X recon loss:0.8897, val auc:0.9173184519076465\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1614, X recon loss:0.8776, val auc:0.9201520912547531\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1232, X recon loss:0.8566, val auc:0.9204846101577296\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  3.0 minutes, 23.140625 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=188003.00 bit\n",
      "The time inference of base model is =2.0789222000166774\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 26.402\n",
      "total memory usage of base model':16135 \n",
      "cpu usage of base model':0.800 %\n",
      "_______________________________________________________\n",
      "Prune the Model and Re-Evaluate the Accuracy\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.68%\n",
      "pruned model has size=93071.00\n",
      "The time inference of pruned model is =2.0847025000257418\n",
      "The number of parametrs of pruned model is:22960\n",
      "Energy Consumption : 21.681\n",
      "total memory usage of pruned model':16135 \n",
      "cpu usage of pruned model':0.400 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "is_pruned:True\n",
      "Epoch:[-1], Total loss:2.0446, X recon loss:0.8362, val auc:NA\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.2069, X recon loss:0.8924, val auc:0.9098295479188654\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.68%\n",
      "pruned_finetune model has size=93179.00 \n",
      "The time inference of pruned_finetune model is =2.0844547000015154\n",
      "The number of parametrs of pruned_finetune model is:22960\n",
      "Energy Consumption of pruned_finetune model: 21.261\n",
      "total memory usage of pruned_finetune model':16225 \n",
      "cpu usage of pruned_finetune model':0.400 %\n",
      "________________________________________\n",
      "This is iteration 1\n",
      "Training and evaluation before pruning \n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "is_pruned:False\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3546, X recon loss:0.9534, val auc:0.8978877820989171\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2055, X recon loss:0.8897, val auc:0.9173184519076465\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1614, X recon loss:0.8776, val auc:0.9201520912547531\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1232, X recon loss:0.8566, val auc:0.9204846101577296\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  3.0 minutes, 20.703125 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=188003.00 bit\n",
      "The time inference of base model is =2.0815597000182606\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 21.648\n",
      "total memory usage of base model':16076 \n",
      "cpu usage of base model':0.000 %\n",
      "_______________________________________________________\n",
      "Prune the Model and Re-Evaluate the Accuracy\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.68%\n",
      "pruned model has size=93071.00\n",
      "The time inference of pruned model is =2.081218599982094\n",
      "The number of parametrs of pruned model is:22960\n",
      "Energy Consumption : 21.228\n",
      "total memory usage of pruned model':16024 \n",
      "cpu usage of pruned model':0.800 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "is_pruned:True\n",
      "Epoch:[-1], Total loss:2.0446, X recon loss:0.8362, val auc:NA\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.2069, X recon loss:0.8924, val auc:0.9098295479188654\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.68%\n",
      "pruned_finetune model has size=93179.00 \n",
      "The time inference of pruned_finetune model is =2.1068395999609493\n",
      "The number of parametrs of pruned_finetune model is:22960\n",
      "Energy Consumption of pruned_finetune model: 23.491\n",
      "total memory usage of pruned_finetune model':16135 \n",
      "cpu usage of pruned_finetune model':2.700 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________\n",
      "This is iteration 0\n",
      "Training and evaluation before pruning \n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "is_pruned:False\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3546, X recon loss:0.9534, val auc:0.8978877820989171\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2055, X recon loss:0.8897, val auc:0.9173184519076465\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1614, X recon loss:0.8776, val auc:0.9201520912547531\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1232, X recon loss:0.8566, val auc:0.9204846101577296\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  3.0 minutes, 22.171875 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=188003.00 bit\n",
      "The time inference of base model is =2.084936600003857\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 21.266\n",
      "total memory usage of base model':16076 \n",
      "cpu usage of base model':0.800 %\n",
      "_______________________________________________________\n",
      "Prune the Model and Re-Evaluate the Accuracy\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.68%\n",
      "pruned model has size=74703.00\n",
      "The time inference of pruned model is =2.0889460999751464\n",
      "The number of parametrs of pruned model is:18374\n",
      "Energy Consumption : 21.307\n",
      "total memory usage of pruned model':16135 \n",
      "cpu usage of pruned model':1.200 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "is_pruned:True\n",
      "Epoch:[-1], Total loss:2.0446, X recon loss:0.8362, val auc:NA\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3282, X recon loss:0.9392, val auc:0.9082392401220201\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.68%\n",
      "pruned_finetune model has size=74811.00 \n",
      "The time inference of pruned_finetune model is =2.083263500011526\n",
      "The number of parametrs of pruned_finetune model is:18374\n",
      "Energy Consumption of pruned_finetune model: 20.833\n",
      "total memory usage of pruned_finetune model':16135 \n",
      "cpu usage of pruned_finetune model':1.600 %\n",
      "________________________________________\n",
      "This is iteration 1\n",
      "Training and evaluation before pruning \n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "is_pruned:False\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3546, X recon loss:0.9534, val auc:0.8978877820989171\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2055, X recon loss:0.8897, val auc:0.9173184519076465\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1614, X recon loss:0.8776, val auc:0.9201520912547531\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1232, X recon loss:0.8566, val auc:0.9204846101577296\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  3.0 minutes, 23.09375 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=188003.00 bit\n",
      "The time inference of base model is =2.081667699967511\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 20.817\n",
      "total memory usage of base model':16083 \n",
      "cpu usage of base model':0.800 %\n",
      "_______________________________________________________\n",
      "Prune the Model and Re-Evaluate the Accuracy\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.68%\n",
      "pruned model has size=74703.00\n",
      "The time inference of pruned model is =2.0772187000256963\n",
      "The number of parametrs of pruned model is:18374\n",
      "Energy Consumption : 21.188\n",
      "total memory usage of pruned model':16076 \n",
      "cpu usage of pruned model':0.400 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "is_pruned:True\n",
      "Epoch:[-1], Total loss:2.0446, X recon loss:0.8362, val auc:NA\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3282, X recon loss:0.9392, val auc:0.9082392401220201\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.68%\n",
      "pruned_finetune model has size=74811.00 \n",
      "The time inference of pruned_finetune model is =2.0843653000192717\n",
      "The number of parametrs of pruned_finetune model is:18374\n",
      "Energy Consumption of pruned_finetune model: 22.511\n",
      "total memory usage of pruned_finetune model':16076 \n",
      "cpu usage of pruned_finetune model':0.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________\n",
      "This is iteration 0\n",
      "Training and evaluation before pruning \n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "is_pruned:False\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3546, X recon loss:0.9534, val auc:0.8978877820989171\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2055, X recon loss:0.8897, val auc:0.9173184519076465\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1614, X recon loss:0.8776, val auc:0.9201520912547531\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1232, X recon loss:0.8566, val auc:0.9204846101577296\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  3.0 minutes, 23.203125 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=188003.00 bit\n",
      "The time inference of base model is =2.0826173999812454\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 22.388\n",
      "total memory usage of base model':16135 \n",
      "cpu usage of base model':4.200 %\n",
      "_______________________________________________________\n",
      "Prune the Model and Re-Evaluate the Accuracy\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.65%\n",
      "pruned model has size=56335.00\n",
      "The time inference of pruned model is =2.0914566999999806\n",
      "The number of parametrs of pruned model is:13789\n",
      "Energy Consumption : 21.333\n",
      "total memory usage of pruned model':16135 \n",
      "cpu usage of pruned model':2.700 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "is_pruned:True\n",
      "Epoch:[-1], Total loss:2.0446, X recon loss:0.8362, val auc:NA\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.4851, X recon loss:1.0017, val auc:0.8940999580737036\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.65%\n",
      "pruned_finetune model has size=56507.00 \n",
      "The time inference of pruned_finetune model is =2.0777415999909863\n",
      "The number of parametrs of pruned_finetune model is:13789\n",
      "Energy Consumption of pruned_finetune model: 43.529\n",
      "total memory usage of pruned_finetune model':16083 \n",
      "cpu usage of pruned_finetune model':0.800 %\n",
      "________________________________________\n",
      "This is iteration 1\n",
      "Training and evaluation before pruning \n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "is_pruned:False\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3546, X recon loss:0.9534, val auc:0.8978877820989171\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2055, X recon loss:0.8897, val auc:0.9173184519076465\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1614, X recon loss:0.8776, val auc:0.9201520912547531\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1232, X recon loss:0.8566, val auc:0.9204846101577296\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  3.0 minutes, 23.578125 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=188003.00 bit\n",
      "The time inference of base model is =2.0856260000145994\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 21.691\n",
      "total memory usage of base model':16135 \n",
      "cpu usage of base model':0.800 %\n",
      "_______________________________________________________\n",
      "Prune the Model and Re-Evaluate the Accuracy\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.65%\n",
      "pruned model has size=56335.00\n",
      "The time inference of pruned model is =2.079061400028877\n",
      "The number of parametrs of pruned model is:13789\n",
      "Energy Consumption : 37.007\n",
      "total memory usage of pruned model':16017 \n",
      "cpu usage of pruned model':0.400 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "is_pruned:True\n",
      "Epoch:[-1], Total loss:2.0446, X recon loss:0.8362, val auc:NA\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.4851, X recon loss:1.0017, val auc:0.8940999580737036\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.65%\n",
      "pruned_finetune model has size=56507.00 \n",
      "The time inference of pruned_finetune model is =2.0836164000211284\n",
      "The number of parametrs of pruned_finetune model is:13789\n",
      "Energy Consumption of pruned_finetune model: 38.026\n",
      "total memory usage of pruned_finetune model':16076 \n",
      "cpu usage of pruned_finetune model':1.900 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________\n",
      "This is iteration 0\n",
      "Training and evaluation before pruning \n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "is_pruned:False\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3546, X recon loss:0.9534, val auc:0.8978877820989171\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2055, X recon loss:0.8897, val auc:0.9173184519076465\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1614, X recon loss:0.8776, val auc:0.9201520912547531\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1232, X recon loss:0.8566, val auc:0.9204846101577296\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  3.0 minutes, 27.359375 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=188003.00 bit\n",
      "The time inference of base model is =2.0804585000150837\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 21.637\n",
      "total memory usage of base model':16135 \n",
      "cpu usage of base model':0.800 %\n",
      "_______________________________________________________\n",
      "Prune the Model and Re-Evaluate the Accuracy\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.64%\n",
      "pruned model has size=38031.00\n",
      "The time inference of pruned model is =2.098602000041865\n",
      "The number of parametrs of pruned model is:9203\n",
      "Energy Consumption : 20.986\n",
      "total memory usage of pruned model':15972 \n",
      "cpu usage of pruned model':0.800 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "is_pruned:True\n",
      "Epoch:[-1], Total loss:2.0446, X recon loss:0.8362, val auc:NA\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.7567, X recon loss:1.1081, val auc:0.8716043314201449\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.64%\n",
      "pruned_finetune model has size=38139.00 \n",
      "The time inference of pruned_finetune model is =2.0856049999711104\n",
      "The number of parametrs of pruned_finetune model is:9203\n",
      "Energy Consumption of pruned_finetune model: 20.856\n",
      "total memory usage of pruned_finetune model':16076 \n",
      "cpu usage of pruned_finetune model':0.400 %\n",
      "________________________________________\n",
      "This is iteration 1\n",
      "Training and evaluation before pruning \n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "is_pruned:False\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3546, X recon loss:0.9534, val auc:0.8978877820989171\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2055, X recon loss:0.8897, val auc:0.9173184519076465\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1614, X recon loss:0.8776, val auc:0.9201520912547531\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1232, X recon loss:0.8566, val auc:0.9204846101577296\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  3.0 minutes, 22.859375 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=188003.00 bit\n",
      "The time inference of base model is =2.0805270000128075\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 22.054\n",
      "total memory usage of base model':15972 \n",
      "cpu usage of base model':1.200 %\n",
      "_______________________________________________________\n",
      "Prune the Model and Re-Evaluate the Accuracy\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.64%\n",
      "pruned model has size=38031.00\n",
      "The time inference of pruned model is =2.0824777000234462\n",
      "The number of parametrs of pruned model is:9203\n",
      "Energy Consumption : 21.658\n",
      "total memory usage of pruned model':16135 \n",
      "cpu usage of pruned model':0.800 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "is_pruned:True\n",
      "Epoch:[-1], Total loss:2.0446, X recon loss:0.8362, val auc:NA\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.7567, X recon loss:1.1081, val auc:0.8716043314201449\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.64%\n",
      "pruned_finetune model has size=38139.00 \n",
      "The time inference of pruned_finetune model is =2.082952100026887\n",
      "The number of parametrs of pruned_finetune model is:9203\n",
      "Energy Consumption of pruned_finetune model: 21.663\n",
      "total memory usage of pruned_finetune model':16076 \n",
      "cpu usage of pruned_finetune model':1.200 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________\n",
      "This is iteration 0\n",
      "Training and evaluation before pruning \n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "is_pruned:False\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3546, X recon loss:0.9534, val auc:0.8978877820989171\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2055, X recon loss:0.8897, val auc:0.9173184519076465\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1614, X recon loss:0.8776, val auc:0.9201520912547531\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1232, X recon loss:0.8566, val auc:0.9204846101577296\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  3.0 minutes, 21.0625 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=188003.00 bit\n",
      "The time inference of base model is =2.081215899961535\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 21.228\n",
      "total memory usage of base model':16024 \n",
      "cpu usage of base model':0.800 %\n",
      "_______________________________________________________\n",
      "Prune the Model and Re-Evaluate the Accuracy\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.61%\n",
      "pruned model has size=19663.00\n",
      "The time inference of pruned model is =2.0839067999622785\n",
      "The number of parametrs of pruned model is:4618\n",
      "Energy Consumption : 21.673\n",
      "total memory usage of pruned model':16135 \n",
      "cpu usage of pruned model':0.400 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "is_pruned:True\n",
      "Epoch:[-1], Total loss:2.0446, X recon loss:0.8362, val auc:NA\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:3.3453, X recon loss:1.3409, val auc:0.8351139961543468\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.61%\n",
      "pruned_finetune model has size=19771.00 \n",
      "The time inference of pruned_finetune model is =2.082064200018067\n",
      "The number of parametrs of pruned_finetune model is:4618\n",
      "Energy Consumption of pruned_finetune model: 21.653\n",
      "total memory usage of pruned_finetune model':16083 \n",
      "cpu usage of pruned_finetune model':0.400 %\n",
      "________________________________________\n",
      "This is iteration 1\n",
      "Training and evaluation before pruning \n",
      "Directories are set.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "is_pruned:False\n",
      "Epoch:[-1], Total loss:7.8579, X recon loss:3.1438, val auc:NA\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3546, X recon loss:0.9534, val auc:0.8978877820989171\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2055, X recon loss:0.8897, val auc:0.9173184519076465\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[119] training loss:2.1614, X recon loss:0.8776, val auc:0.9201520912547531\n",
      "Done with saving models.\n",
      "Epoch:[159] training loss:2.1232, X recon loss:0.8566, val auc:0.9204846101577296\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  3.0 minutes, 21.203125 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=188003.00 bit\n",
      "The time inference of base model is =2.0785069999983534\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 22.032\n",
      "total memory usage of base model':16135 \n",
      "cpu usage of base model':1.200 %\n",
      "_______________________________________________________\n",
      "Prune the Model and Re-Evaluate the Accuracy\n",
      "_________******************************_____________\n",
      "Pruning the Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of pruning ******************\n",
      "pruned model has accuracy on test set=0.61%\n",
      "pruned model has size=19663.00\n",
      "The time inference of pruned model is =2.0918467000010423\n",
      "The number of parametrs of pruned model is:4618\n",
      "Energy Consumption : 21.755\n",
      "total memory usage of pruned model':16017 \n",
      "cpu usage of pruned model':11.200 %\n",
      "________*******************************_____________\n",
      "Finetuning Pruned Sparse Model\n",
      "Done with saving models.\n",
      "is_pruned:False\n",
      "--autoencoder is loaded\n",
      "Done with loading models.\n",
      "is_pruned:True\n",
      "Epoch:[-1], Total loss:2.0446, X recon loss:0.8362, val auc:NA\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:3.3453, X recon loss:1.3409, val auc:0.8351139961543468\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Pruned model loaded.\n",
      "****************Result of fine-tuning of pruned model ******************\n",
      "pruned_finetune model has accuracy on test set=0.61%\n",
      "pruned_finetune model has size=19771.00 \n",
      "The time inference of pruned_finetune model is =2.08326359995408\n",
      "The number of parametrs of pruned_finetune model is:4618\n",
      "Energy Consumption of pruned_finetune model: 21.249\n",
      "total memory usage of pruned_finetune model':16135 \n",
      "cpu usage of pruned_finetune model':2.000 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import statistics as stat\n",
    "rate_spar=[0.1, 0.2, 0.3, 0.4,  0.5, 0.6 , 0.7,  0.8, 0.9]\n",
    "for sparsity in rate_spar:\n",
    "\n",
    "    # This is a dictionary to save all measurements. Aftre measuring, we can compute mean and std of each item.\n",
    "    Eva_final=dict()\n",
    "\n",
    "    # The following are all list of criteria for measurements. \n",
    "    # We collect all desired datas of each list across iterations. \n",
    "    # Then, we compute average and std of each list.\n",
    "\n",
    "\n",
    "\n",
    "    #Base model\n",
    "    Base_model_accuracy=[]\n",
    "    T_base_model=[]\n",
    "    Num_parm_base_model=[]\n",
    "    Base_model_size=[]\n",
    "    Base_Energy_Consumption=[]\n",
    "    Base_Cpu_Usage=[]\n",
    "    Base_Memory_Usage=[]\n",
    "\n",
    "    #Pruned model\n",
    "    Pruned_model_accuracy=[]\n",
    "    T_pruned_model=[]\n",
    "    Num_parm_pruned_model=[]\n",
    "    Pruned_model_size=[]\n",
    "    Pruned_Energy_Consumption=[]\n",
    "    Pruned_Cpu_Usage=[]\n",
    "    Pruned_Memory_Usage=[]\n",
    "\n",
    "    #Pruned and finetune model\n",
    "    Pruned_finetune_model_accuracy=[]\n",
    "    T_pruned_finetune_model=[]\n",
    "    Num_parm_pruned_finetune_model=[]\n",
    "    Pruned_finetune_model_size=[]\n",
    "    Pruned_finetune_Energy_Consumption=[]\n",
    "    Pruned_finetune_Cpu_Usage=[]\n",
    "    Pruned_finetune_Memory_Usage=[]\n",
    "\n",
    "    # Here is the dictionary to record the list of all measurements\n",
    "    Eva_measure={'base model accuracy':Base_model_accuracy,\n",
    "                'time inference of base model':T_base_model,\n",
    "                'number parmameters of base model':Num_parm_base_model,\n",
    "                'base model size':Base_model_size,\n",
    "                'energy consumption of base model':Base_Energy_Consumption,\n",
    "                'cpu usage of base model':Base_Cpu_Usage,\n",
    "                'memory usage of base model':Base_Memory_Usage,\n",
    "                'pruned model accuracy': Pruned_model_accuracy,\n",
    "                'time inference of pruned model':T_pruned_model,\n",
    "                'number parmameters of pruned model':Num_parm_pruned_model,\n",
    "                'pruned model size':Pruned_model_size,\n",
    "                'energy consumption of pruned model':Pruned_Energy_Consumption,\n",
    "                'cpu usage of pruned model':Pruned_Cpu_Usage,\n",
    "                'memory usage of pruned model':Pruned_Memory_Usage,\n",
    "                'pruned finetune model accuracy':Pruned_finetune_model_accuracy,\n",
    "                'time inference of pruned finetune model':T_pruned_finetune_model,\n",
    "                'number parmameters of pruned finetune model':Num_parm_pruned_finetune_model,\n",
    "                'pruned finetune model size':Pruned_finetune_model_size,\n",
    "                'energy consumption of pruned_finetune model':Pruned_finetune_Energy_Consumption,\n",
    "                'cpu usage of pruned_finetune model':Pruned_finetune_Cpu_Usage,\n",
    "                'memory usage of pruned_finetune model':Pruned_finetune_Memory_Usage}\n",
    "\n",
    "    \n",
    "    for i in range(num_iterations)  :\n",
    "            \n",
    "            print('________________________________________')\n",
    "            print(f'This is iteration {i}')   \n",
    "\n",
    "            # It is a dictionary to arrange output of this iteration\n",
    "            Eva=dict() \n",
    "\n",
    "            print(f'Training and evaluation before pruning ')\n",
    "            is_pruned=False\n",
    "            base_model_accuracy, test_ap, t_base_model,val_auc, model = main(config,is_pruned=False)\n",
    "\n",
    "            best_checkpoint = dict()\n",
    "            best_checkpoint['state_dict'] = copy.deepcopy(model.autoencoder.state_dict())\n",
    "            model.autoencoder.load_state_dict(best_checkpoint['state_dict'])\n",
    "            recover_model = lambda: model.autoencoder.load_state_dict(best_checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "            # Start monitoring CPU and memory usage, model size, number of parametes, time inference and  power consumption\n",
    "            gc.collect()\n",
    "            time.sleep(5)  # Add a 5-second delay to stabilize the initial state\n",
    "            tracemalloc.start()  # Start tracking memory allocations\n",
    "            snapshot_before = tracemalloc.take_snapshot()#take a snapshot of the current memory state before starting the measurement.\n",
    "\n",
    "            t0 = time.perf_counter()\n",
    "            initial_cpu_usage = get_cpu_usage()\n",
    "            power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "\n",
    "            test_auc, test_ap = model.autoencoder.single_test(train_data, test_data)\n",
    "            base_model_accuracy =test_auc\n",
    "\n",
    "\n",
    "            base_cpu_usage = get_cpu_usage()\n",
    "            t1 = time.perf_counter()\n",
    "            t_base_model=t1-t0\n",
    "\n",
    "            snapshot_after = tracemalloc.take_snapshot()\n",
    "            tracemalloc.stop()\n",
    "            top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "            base_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "            base_energy_consumption = power_usage * t_base_model\n",
    "            num_parm_base_model=get_num_parameters(model, count_nonzero_only=True)\n",
    "\n",
    "            # Base Model Size\n",
    "            model_path = os.path.join(results_path, \"training\", \"model\", \"autoencoder.pt\")\n",
    "            base_model_size = os.path.getsize(model_path)\n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5) \n",
    "\n",
    "            print(f'*****Results of base model*********')\n",
    "\n",
    "            print(f\"base model has accuracy on test set={base_model_accuracy:.2f}%\")\n",
    "            print(f\"base model has size={base_model_size:.2f} bit\")\n",
    "            print(f\"The time inference of base model is ={t_base_model}\") \n",
    "            print(f\"The number of parametrs of base model is:{num_parm_base_model}\") \n",
    "\n",
    "            print(f\"Energy Consumption : {base_energy_consumption:.3f}\")\n",
    "            print(f\"total memory usage of base model':{base_total_memory_diff} \")\n",
    "            print(f\"cpu usage of base model':{base_cpu_usage:.3f} %\")\n",
    "\n",
    "\n",
    "            #Update Eva dictionary\n",
    "            Eva.update({'base model accuracy': base_model_accuracy,\n",
    "                        'time inference of base model': t_base_model,\n",
    "                        'number parmameters of base model': num_parm_base_model,\n",
    "                        'size of base model': base_model_size, \n",
    "                        'energy consumption of base model':base_energy_consumption,\n",
    "                        'total memory usage of base model':base_total_memory_diff,\n",
    "                        'cpu usage of base model':base_cpu_usage\n",
    "                       })\n",
    "\n",
    "\n",
    "\n",
    "            print('_______________________________________________________')\n",
    "            print(f'Prune the Model and Re-Evaluate the Accuracy')\n",
    "\n",
    "            print('_________******************************_____________')\n",
    "            print(f'Pruning the Model')\n",
    "\n",
    "            recover_model()\n",
    "            model.save_weights()\n",
    "            model.load_models()\n",
    "\n",
    "            # The path of pruned model\n",
    "            pruned_model_path = os.path.join(results_path, \"training\", \"model\", \"autoencoder\" + \"pruned.pt\")\n",
    "            # Applying Pruning method\n",
    "            pruner = FineGrainedPruner(model, sparsity)\n",
    "            # Remove zero weights \n",
    "            state = state_sparse_model(model)\n",
    "            # Saving pruned model on disk\n",
    "            torch.save(state, pruned_model_path)\n",
    "\n",
    "            # Reload Pruned model from disk\n",
    "            pruned_model=load_and_evaluate_pruned_model(config,pruned_model_path)\n",
    "\n",
    "\n",
    "            print('****************Result of pruning ******************')\n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5)  \n",
    "            tracemalloc.start()  \n",
    "            snapshot_before = tracemalloc.take_snapshot()\n",
    "\n",
    "            t0 = time.perf_counter()\n",
    "            initial_cpu_usage = get_cpu_usage()\n",
    "            power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "            # accuracy\n",
    "            pruned_model_accuracy, test_ap=pruned_model.autoencoder.single_test(train_data, test_data)\n",
    "\n",
    "            pruned_cpu_usage = get_cpu_usage()\n",
    "            t1 = time.perf_counter()\n",
    "            t_pruned_model=t1-t0\n",
    "\n",
    "            snapshot_after = tracemalloc.take_snapshot()\n",
    "            tracemalloc.stop()\n",
    "            top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "            pruned_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "            pruned_energy_consumption = power_usage * t_pruned_model\n",
    "\n",
    "            pruned_model_size = os.path.getsize(pruned_model_path )\n",
    "            num_parm_pruned_model=get_num_parameters(pruned_model, count_nonzero_only=True)\n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5)  \n",
    "\n",
    "            ###### Report of pruning \n",
    "            print(f\"pruned model has accuracy on test set={pruned_model_accuracy:.2f}%\")\n",
    "            print(f\"pruned model has size={pruned_model_size:.2f}\")\n",
    "            print(f\"The time inference of pruned model is ={t_pruned_model}\") \n",
    "            print(f\"The number of parametrs of pruned model is:{num_parm_pruned_model}\") \n",
    "\n",
    "            print(f\"Energy Consumption : {pruned_energy_consumption:.3f}\")\n",
    "            print(f\"total memory usage of pruned model':{pruned_total_memory_diff} \")\n",
    "            print(f\"cpu usage of pruned model':{pruned_cpu_usage:.3f} %\")\n",
    "\n",
    "\n",
    "            #Update Eva dictionary\n",
    "            Eva.update({'pruned model accuracy': pruned_model_accuracy,\n",
    "                        'time inference of pruned model': t_pruned_model,\n",
    "                        'number parmameters of pruned model': num_parm_pruned_model,\n",
    "                        'size of pruned model': pruned_model_size, \n",
    "                        'energy consumption of pruned model':pruned_energy_consumption,\n",
    "                        'total memory usage of pruned model':pruned_total_memory_diff,\n",
    "                        'cpu usage of pruned model':pruned_cpu_usage\n",
    "                       })\n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5) \n",
    "\n",
    "            print('________*******************************_____________')\n",
    "            print(f'Finetuning Pruned Sparse Model')\n",
    "\n",
    "            is_prund=True\n",
    "            model.save_weights()\n",
    "            model.load_models()\n",
    "             # Callbacks: It prunes model after trainig in each epoch\n",
    "            callbacks=[lambda:FineGrainedPruner(model, sparsity)]\n",
    "             # Finetuning\n",
    "            model.fit(is_prund,ds_loader,callbacks)\n",
    "            fine_tuned_state=state_sparse_model(model)\n",
    "            # Fine-tuned model path\n",
    "            pruned_finetune_model_path = os.path.join(results_path, \"training\", \"model\", \"autoencoder\" + \"pruned_fine_tuned.pt\")\n",
    "            torch.save(fine_tuned_state, pruned_finetune_model_path)\n",
    "            #load_sparse_model(pruned_fine_tuned_model_path, model)\n",
    "            pruned_fine_tuned_model=load_and_evaluate_pruned_model(config, pruned_finetune_model_path )\n",
    "\n",
    "            print('****************Result of fine-tuning of pruned model ******************')\n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5)  \n",
    "            tracemalloc.start() \n",
    "            snapshot_before = tracemalloc.take_snapshot()\n",
    "\n",
    "            t0 = time.perf_counter()\n",
    "            initial_cpu_usage = get_cpu_usage()\n",
    "            power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "\n",
    "            pruned_finetune_model_accuracy, test_ap=pruned_fine_tuned_model.autoencoder.single_test(train_data, test_data)\n",
    "\n",
    "            pruned_finetune_cpu_usage = get_cpu_usage()\n",
    "            t1 = time.perf_counter()\n",
    "            t_pruned_finetune_model=t1-t0\n",
    "\n",
    "            snapshot_after = tracemalloc.take_snapshot()\n",
    "            tracemalloc.stop()\n",
    "            top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "            pruned_finetune_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "            pruned_finetune_energy_consumption = power_usage * t_pruned_finetune_model\n",
    "            pruned_finetune_model_size = os.path.getsize( pruned_finetune_model_path)\n",
    "            num_parm_pruned_finetune_model=get_num_parameters(pruned_fine_tuned_model, count_nonzero_only=True)\n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5)  # Add a 5-second delay to stabilize the initial state    \n",
    "\n",
    "            ###### Report  \n",
    "\n",
    "            print(f\"pruned_finetune model has accuracy on test set={pruned_finetune_model_accuracy:.2f}%\")\n",
    "            print(f\"pruned_finetune model has size={pruned_finetune_model_size:.2f} \")\n",
    "            print(f\"The time inference of pruned_finetune model is ={t_pruned_finetune_model}\") \n",
    "            print(f\"The number of parametrs of pruned_finetune model is:{num_parm_pruned_finetune_model}\") \n",
    "\n",
    "            print(f\"Energy Consumption of pruned_finetune model: {pruned_finetune_energy_consumption:.3f}\")\n",
    "            print(f\"total memory usage of pruned_finetune model':{pruned_finetune_total_memory_diff} \")\n",
    "            print(f\"cpu usage of pruned_finetune model':{pruned_finetune_cpu_usage:.3f} %\")\n",
    "\n",
    "\n",
    "            #Update my Eva dictionary\n",
    "            Eva.update({'pruned and finetune model accuracy': pruned_finetune_model_accuracy,\n",
    "                        'time inference of pruned and finetune model': t_pruned_finetune_model,\n",
    "                        'number parmameters of pruned and finetune model': num_parm_pruned_finetune_model,\n",
    "                        'size of pruned and finetune model': pruned_finetune_model_size, \n",
    "                        'energy consumption of pruned and finetune model':pruned_finetune_energy_consumption,\n",
    "                        'total memory usage of pruned and finetune model':pruned_finetune_total_memory_diff,\n",
    "                        'cpu usage of pruned and finetune model':pruned_finetune_cpu_usage\n",
    "                       })\n",
    "\n",
    "            gc.collect()\n",
    "            time.sleep(5) \n",
    "\n",
    "            Base_model_accuracy.append(Eva['base model accuracy'])\n",
    "            T_base_model.append(Eva['time inference of base model'])\n",
    "            Num_parm_base_model.append(int(Eva['number parmameters of base model']))\n",
    "            Base_model_size.append(int(Eva['size of base model']))\n",
    "            Base_Energy_Consumption.append(Eva['energy consumption of base model'])\n",
    "            Base_Cpu_Usage.append(Eva['cpu usage of base model'])\n",
    "            Base_Memory_Usage.append(Eva['total memory usage of base model'])\n",
    "\n",
    "            Pruned_model_accuracy.append(Eva['pruned model accuracy'])\n",
    "            T_pruned_model.append(Eva['time inference of pruned model'])\n",
    "            Num_parm_pruned_model.append(int(Eva['number parmameters of pruned model']))\n",
    "            Pruned_model_size.append(int(Eva['size of pruned model']))\n",
    "            Pruned_Energy_Consumption.append(Eva['energy consumption of pruned model'])\n",
    "            Pruned_Cpu_Usage.append(Eva['cpu usage of pruned model'])\n",
    "            Pruned_Memory_Usage.append(Eva['total memory usage of pruned model'])\n",
    "\n",
    "\n",
    "            Pruned_finetune_model_accuracy.append(Eva['pruned and finetune model accuracy'])\n",
    "            T_pruned_finetune_model.append(Eva['time inference of pruned and finetune model'])\n",
    "            Num_parm_pruned_finetune_model.append(int(Eva['number parmameters of pruned and finetune model']))\n",
    "            Pruned_finetune_model_size.append(int(Eva['size of pruned and finetune model']))\n",
    "            Pruned_finetune_Energy_Consumption.append(Eva['energy consumption of pruned and finetune model'])\n",
    "            Pruned_finetune_Cpu_Usage.append(Eva['cpu usage of pruned and finetune model'])\n",
    "            Pruned_finetune_Memory_Usage.append(Eva['total memory usage of pruned and finetune model'])\n",
    "\n",
    "\n",
    "    Eva_final=dict()\n",
    "    base_model_accuracy_mean = stat.mean(Base_model_accuracy)\n",
    "    base_model_accuracy_std =  stat.stdev(Base_model_accuracy)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(base_model_accuracy_mean,base_model_accuracy_std)\n",
    "\n",
    "\n",
    "    Eva_final.update({'Ave of base model accuracy':float(format(base_model_accuracy_mean, '.3f'))})\n",
    "    Eva_final.update({'Std of base model accuracy':float(format(base_model_accuracy_std, '.3f'))})\n",
    "\n",
    "    t_base_model_mean =stat.mean(T_base_model)\n",
    "    t_base_model_std =stat.stdev(T_base_model)  \n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of time inference of base model':float(format(t_base_model_mean, '.3f'))})\n",
    "    Eva_final.update({'Std of time inference of base model':float(format(t_base_model_std, '.3f'))})\n",
    "\n",
    "\n",
    "    num_parm_base_model_mean = stat.mean(Num_parm_base_model)\n",
    "    num_parm_base_model_std = stat.stdev(Num_parm_base_model)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of number parmameters of base model':num_parm_base_model_mean})\n",
    "    Eva_final.update({'Std of number parmameters of base model':num_parm_base_model_std})\n",
    "\n",
    "    base_model_size_mean = stat.mean(Base_model_size)\n",
    "    base_model_size_std = stat.stdev(Base_model_size)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of base model size':base_model_size_mean})\n",
    "    Eva_final.update({'Std of base model size':base_model_size_std})\n",
    "\n",
    "\n",
    "    base_energy_consumption_mean = stat.mean(Base_Energy_Consumption)\n",
    "    base_energy_consumption_std = stat.stdev(Base_Energy_Consumption)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of energy consumption of base model':base_energy_consumption_mean })\n",
    "    Eva_final.update({'Std of energy consumption of base model':base_energy_consumption_std})\n",
    "\n",
    "\n",
    "    base_cpu_usage_mean = stat.mean(Base_Cpu_Usage)\n",
    "    base_cpu_usage_std = stat.stdev(Base_Cpu_Usage)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of cpu usage of base model':base_cpu_usage_mean})\n",
    "    Eva_final.update({'Std of cpu usage of base model':base_cpu_usage_std})\n",
    "\n",
    "    base_memory_usage_mean = stat.mean(Base_Memory_Usage)\n",
    "    base_memory_usage_std = stat.stdev(Base_Memory_Usage)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of memory usage of base model':base_memory_usage_mean})\n",
    "    Eva_final.update({'Std of memory usage of base model':base_memory_usage_std})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #################################\n",
    "\n",
    "    pruned_model_accuracy_mean =stat.mean(Pruned_model_accuracy)\n",
    "    pruned_model_accuracy_std = stat.stdev(Pruned_model_accuracy)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of pruned model accuracy':float(format(pruned_model_accuracy_mean, '.3f'))})\n",
    "    Eva_final.update({'Std of pruned model accuracy':float(format(pruned_model_accuracy_std, '.3f'))})\n",
    "\n",
    "\n",
    "    t_pruned_model_mean = stat.mean(T_pruned_model)\n",
    "    t_pruned_model_std =stat.stdev(T_pruned_model)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of time inference of pruned model':float(format(t_pruned_model_mean, '.3f'))})\n",
    "    Eva_final.update({'Std of time inference of pruned model':float(format(t_pruned_model_std, '.3f'))})\n",
    "\n",
    "    num_parm_pruned_model_mean = stat.mean(Num_parm_pruned_model)\n",
    "    num_parm_pruned_model_std = stat.stdev(Num_parm_pruned_model)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of number parmameters of pruned model':num_parm_pruned_model_mean})\n",
    "    Eva_final.update({'Std of number parmameters of pruned model':num_parm_pruned_model_std})\n",
    "\n",
    "    pruned_model_size_mean =stat.mean( Pruned_model_size)\n",
    "    pruned_model_size_std = stat.stdev(Pruned_model_size)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of pruned model size':pruned_model_size_mean})\n",
    "    Eva_final.update({'Std of pruned_model_size':pruned_model_size_std })\n",
    "\n",
    "    pruned_energy_consumption_mean = stat.mean(Pruned_Energy_Consumption)\n",
    "    pruned_energy_consumption_std = stat.stdev(Pruned_Energy_Consumption)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of energy consumption of pruned model':pruned_energy_consumption_mean })\n",
    "    Eva_final.update({'Std of energy consumption of pruned model':pruned_energy_consumption_std})\n",
    "\n",
    "\n",
    "    pruned_cpu_usage_mean = stat.mean(Pruned_Cpu_Usage)\n",
    "    pruned_cpu_usage_std = stat.stdev(Pruned_Cpu_Usage)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of cpu usage of pruned model':pruned_cpu_usage_mean})\n",
    "    Eva_final.update({'Std of cpu usage of pruned model':pruned_cpu_usage_std})\n",
    "\n",
    "    pruned_memory_usage_mean = stat.mean(Pruned_Memory_Usage)\n",
    "    pruned_memory_usage_std = stat.stdev(Pruned_Memory_Usage)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of memory usage of pruned model':pruned_memory_usage_mean})\n",
    "    Eva_final.update({'Std of memory usage of pruned model':pruned_memory_usage_std})\n",
    "\n",
    "\n",
    "    #################################\n",
    "    pruned_finetune_model_accuracy_mean =stat.mean(Pruned_finetune_model_accuracy)\n",
    "    pruned_finetune_model_accuracy_std = stat.stdev(Pruned_finetune_model_accuracy)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of pruned finetune model accuracy':float(format(pruned_finetune_model_accuracy_mean, '.3f'))})\n",
    "    Eva_final.update({'Std of pruned finetune model accuracy':float(format(pruned_finetune_model_accuracy_std, '.3f'))})                 \n",
    "\n",
    "    t_pruned_finetune_model_mean =stat.mean(T_pruned_finetune_model)\n",
    "    t_pruned_finetune_model_std =stat.stdev(T_pruned_finetune_model)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of time inference of pruned finetune model':float(format(t_pruned_finetune_model_mean,'.3f'))})\n",
    "    Eva_final.update({'Std of time inference of pruned finetune model':float(format(t_pruned_finetune_model_std,'.3f'))})\n",
    "\n",
    "    num_parm_pruned_finetune_model_mean =stat.mean(Num_parm_pruned_finetune_model)\n",
    "    num_parm_pruned_finetune_model_std = stat.stdev(Num_parm_pruned_finetune_model)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of number parmameters of pruned finetune model':num_parm_pruned_finetune_model_mean})\n",
    "    Eva_final.update({'Std of number parmameters of pruned finetune model':num_parm_pruned_finetune_model_std })\n",
    "\n",
    "    pruned_finetune_model_size_mean = stat.mean(Pruned_finetune_model_size)\n",
    "    pruned_finetune_model_size_std = stat.stdev(Pruned_finetune_model_size)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of pruned finetune model size':pruned_finetune_model_size_mean})\n",
    "    Eva_final.update({'Std of pruned finetune model size':pruned_finetune_model_size_std})\n",
    "\n",
    "\n",
    "    pruned_finetune_energy_consumption_mean = stat.mean(Pruned_finetune_Energy_Consumption)\n",
    "    pruned_finetune_energy_consumption_std = stat.stdev(Pruned_finetune_Energy_Consumption)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of energy consumption of pruned_finetune model':pruned_finetune_energy_consumption_mean })\n",
    "    Eva_final.update({'Std of energy consumption of pruned_finetune model':pruned_finetune_energy_consumption_std})\n",
    "\n",
    "\n",
    "    pruned_finetune_cpu_usage_mean = stat.mean(Pruned_finetune_Cpu_Usage)\n",
    "    pruned_finetune_cpu_usage_std = stat.stdev(Pruned_finetune_Cpu_Usage)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of cpu usage of pruned_finetune model':pruned_finetune_cpu_usage_mean})\n",
    "    Eva_final.update({'Std of cpu usage of pruned_finetune model':pruned_finetune_cpu_usage_std})\n",
    "\n",
    "    pruned_finetune_memory_usage_mean = stat.mean(Pruned_finetune_Memory_Usage)\n",
    "    pruned_finetune_memory_usage_std = stat.stdev(Pruned_finetune_Memory_Usage)\n",
    "    #desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "    Eva_final.update({'Ave of memory usage of pruned_finetune model':pruned_finetune_memory_usage_mean})\n",
    "    Eva_final.update({'Std of memory usage of pruned_finetune model':pruned_finetune_memory_usage_std})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### The sparsity changes across range(0, 1, .01)\n",
    "    dataset_name = 'Cora_Link'\n",
    "    Pruning_Method='Grain_Pruning'\n",
    "    max_epoch = 100\n",
    "    resume = True\n",
    "    result_folder ='pathresult/'\n",
    "    if not os.path.exists(result_folder):\n",
    "        os.makedirs(result_folder)\n",
    "\n",
    "\n",
    "\n",
    "    file_name = result_folder+Pruning_Method+'_'+'with sparsity of'+'_'+str(sparsity)+'_on_'+dataset_name+'_'+str(max_epoch)+'.txt'\n",
    "\n",
    "    with open(file_name, 'w') as f:\n",
    "            f.write('%s:%s\\n'%('dataset_name', 'Cora_Link'))\n",
    "            f.write('%s:%s\\n'%('max_epoch', max_epoch))\n",
    "            f.write('%s:%s\\n'%('sparsity', sparsity))\n",
    "            for key, value in Eva_final.items():\n",
    "                f.write('%s:%s\\n'%(key, value))\n",
    "\n",
    "            for key, value in Eva_measure.items():\n",
    "                f.write('%s:%s\\n' % (key, ','.join(map(str, value))))                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5637ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c87a6698",
   "metadata": {},
   "source": [
    "### Computing the mean and std "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b977607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eva_final=dict()\n",
    "base_model_accuracy_mean = stat.mean(Base_model_accuracy)\n",
    "base_model_accuracy_std =  stat.stdev(Base_model_accuracy)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(base_model_accuracy_mean,base_model_accuracy_std)\n",
    "\n",
    "\n",
    "Eva_final.update({'Ave of base model accuracy':float(format(base_model_accuracy_mean, '.3f'))})\n",
    "Eva_final.update({'Std of base model accuracy':float(format(base_model_accuracy_std, '.3f'))})\n",
    "                 \n",
    "t_base_model_mean =stat.mean(T_base_model)\n",
    "t_base_model_std =stat.stdev(T_base_model)  \n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of time inference of base model':float(format(t_base_model_mean, '.3f'))})\n",
    "Eva_final.update({'Std of time inference of base model':float(format(t_base_model_std, '.3f'))})\n",
    "\n",
    "\n",
    "num_parm_base_model_mean = stat.mean(Num_parm_base_model)\n",
    "num_parm_base_model_std = stat.stdev(Num_parm_base_model)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of number parmameters of base model':num_parm_base_model_mean})\n",
    "Eva_final.update({'Std of number parmameters of base model':num_parm_base_model_std})\n",
    "\n",
    "base_model_size_mean = stat.mean(Base_model_size)\n",
    "base_model_size_std = stat.stdev(Base_model_size)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of base model size':base_model_size_mean})\n",
    "Eva_final.update({'Std of base model size':base_model_size_std})\n",
    "\n",
    "\n",
    "base_energy_consumption_mean = stat.mean(Base_Energy_Consumption)\n",
    "base_energy_consumption_std = stat.stdev(Base_Energy_Consumption)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of energy consumption of base model':base_energy_consumption_mean })\n",
    "Eva_final.update({'Std of energy consumption of base model':base_energy_consumption_std})\n",
    "\n",
    "\n",
    "base_cpu_usage_mean = stat.mean(Base_Cpu_Usage)\n",
    "base_cpu_usage_std = stat.stdev(Base_Cpu_Usage)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of cpu usage of base model':base_cpu_usage_mean})\n",
    "Eva_final.update({'Std of cpu usage of base model':base_cpu_usage_std})\n",
    "\n",
    "base_memory_usage_mean = stat.mean(Base_Memory_Usage)\n",
    "base_memory_usage_std = stat.stdev(Base_Memory_Usage)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of memory usage of base model':base_memory_usage_mean})\n",
    "Eva_final.update({'Std of memory usage of base model':base_memory_usage_std})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################\n",
    "\n",
    "pruned_model_accuracy_mean =stat.mean(Pruned_model_accuracy)\n",
    "pruned_model_accuracy_std = stat.stdev(Pruned_model_accuracy)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of pruned model accuracy':float(format(pruned_model_accuracy_mean, '.3f'))})\n",
    "Eva_final.update({'Std of pruned model accuracy':float(format(pruned_model_accuracy_std, '.3f'))})\n",
    "                 \n",
    "\n",
    "t_pruned_model_mean = stat.mean(T_pruned_model)\n",
    "t_pruned_model_std =stat.stdev(T_pruned_model)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of time inference of pruned model':float(format(t_pruned_model_mean, '.3f'))})\n",
    "Eva_final.update({'Std of time inference of pruned model':float(format(t_pruned_model_std, '.3f'))})\n",
    "\n",
    "num_parm_pruned_model_mean = stat.mean(Num_parm_pruned_model)\n",
    "num_parm_pruned_model_std = stat.stdev(Num_parm_pruned_model)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of number parmameters of pruned model':num_parm_pruned_model_mean})\n",
    "Eva_final.update({'Std of number parmameters of pruned model':num_parm_pruned_model_std})\n",
    "\n",
    "pruned_model_size_mean =stat.mean( Pruned_model_size)\n",
    "pruned_model_size_std = stat.stdev(Pruned_model_size)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of pruned model size':pruned_model_size_mean})\n",
    "Eva_final.update({'Std of pruned_model_size':pruned_model_size_std })\n",
    "\n",
    "pruned_energy_consumption_mean = stat.mean(Pruned_Energy_Consumption)\n",
    "pruned_energy_consumption_std = stat.stdev(Pruned_Energy_Consumption)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of energy consumption of pruned model':pruned_energy_consumption_mean })\n",
    "Eva_final.update({'Std of energy consumption of pruned model':pruned_energy_consumption_std})\n",
    "\n",
    "\n",
    "pruned_cpu_usage_mean = stat.mean(Pruned_Cpu_Usage)\n",
    "pruned_cpu_usage_std = stat.stdev(Pruned_Cpu_Usage)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of cpu usage of pruned model':pruned_cpu_usage_mean})\n",
    "Eva_final.update({'Std of cpu usage of pruned model':pruned_cpu_usage_std})\n",
    "\n",
    "pruned_memory_usage_mean = stat.mean(Pruned_Memory_Usage)\n",
    "pruned_memory_usage_std = stat.stdev(Pruned_Memory_Usage)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of memory usage of pruned model':pruned_memory_usage_mean})\n",
    "Eva_final.update({'Std of memory usage of pruned model':pruned_memory_usage_std})\n",
    "\n",
    "\n",
    "#################################\n",
    "pruned_finetune_model_accuracy_mean =stat.mean(Pruned_finetune_model_accuracy)\n",
    "pruned_finetune_model_accuracy_std = stat.stdev(Pruned_finetune_model_accuracy)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of pruned finetune model accuracy':float(format(pruned_finetune_model_accuracy_mean, '.3f'))})\n",
    "Eva_final.update({'Std of pruned finetune model accuracy':float(format(pruned_finetune_model_accuracy_std, '.3f'))})                 \n",
    "\n",
    "t_pruned_finetune_model_mean =stat.mean(T_pruned_finetune_model)\n",
    "t_pruned_finetune_model_std =stat.stdev(T_pruned_finetune_model)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of time inference of pruned finetune model':float(format(t_pruned_finetune_model_mean,'.3f'))})\n",
    "Eva_final.update({'Std of time inference of pruned finetune model':float(format(t_pruned_finetune_model_std,'.3f'))})\n",
    "\n",
    "num_parm_pruned_finetune_model_mean =stat.mean(Num_parm_pruned_finetune_model)\n",
    "num_parm_pruned_finetune_model_std = stat.stdev(Num_parm_pruned_finetune_model)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of number parmameters of pruned finetune model':num_parm_pruned_finetune_model_mean})\n",
    "Eva_final.update({'Std of number parmameters of pruned finetune model':num_parm_pruned_finetune_model_std })\n",
    "\n",
    "pruned_finetune_model_size_mean = stat.mean(Pruned_finetune_model_size)\n",
    "pruned_finetune_model_size_std = stat.stdev(Pruned_finetune_model_size)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of pruned finetune model size':pruned_finetune_model_size_mean})\n",
    "Eva_final.update({'Std of pruned finetune model size':pruned_finetune_model_size_std})\n",
    "\n",
    "\n",
    "pruned_finetune_energy_consumption_mean = stat.mean(Pruned_finetune_Energy_Consumption)\n",
    "pruned_finetune_energy_consumption_std = stat.stdev(Pruned_finetune_Energy_Consumption)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of energy consumption of pruned_finetune model':pruned_finetune_energy_consumption_mean })\n",
    "Eva_final.update({'Std of energy consumption of pruned_finetune model':pruned_finetune_energy_consumption_std})\n",
    "\n",
    "\n",
    "pruned_finetune_cpu_usage_mean = stat.mean(Pruned_finetune_Cpu_Usage)\n",
    "pruned_finetune_cpu_usage_std = stat.stdev(Pruned_finetune_Cpu_Usage)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of cpu usage of pruned_finetune model':pruned_finetune_cpu_usage_mean})\n",
    "Eva_final.update({'Std of cpu usage of pruned_finetune model':pruned_finetune_cpu_usage_std})\n",
    "\n",
    "pruned_finetune_memory_usage_mean = stat.mean(Pruned_finetune_Memory_Usage)\n",
    "pruned_finetune_memory_usage_std = stat.stdev(Pruned_finetune_Memory_Usage)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of memory usage of pruned_finetune model':pruned_finetune_memory_usage_mean})\n",
    "Eva_final.update({'Std of memory usage of pruned_finetune model':pruned_finetune_memory_usage_std})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################\n",
    "\n",
    "\n",
    "print(f\"All measurement about pruning process of sparsity:{sparsity*100}% \")   \n",
    "Eva_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e26364c",
   "metadata": {},
   "source": [
    "### Recording results on txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39c97586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### The sparsity changes across range(0, 1, .01)\n",
    "dataset_name = 'Cora_Link'\n",
    "Pruning_Method='Grain_Pruning'\n",
    "max_epoch = 100\n",
    "resume = True\n",
    "result_folder ='pathresult/'\n",
    "if not os.path.exists(result_folder):\n",
    "    os.makedirs(result_folder)\n",
    "\n",
    "\n",
    "\n",
    "file_name = result_folder+Pruning_Method+'_'+'with sparsity of'+'_'+str(sparsity)+'_on_'+dataset_name+'_'+str(max_epoch)+'.txt'\n",
    "\n",
    "with open(file_name, 'w') as f:\n",
    "        f.write('%s:%s\\n'%('dataset_name', 'Cora_Link'))\n",
    "        f.write('%s:%s\\n'%('max_epoch', max_epoch))\n",
    "        f.write('%s:%s\\n'%('sparsity', sparsity))\n",
    "        for key, value in Eva_final.items():\n",
    "            f.write('%s:%s\\n'%(key, value))\n",
    "            \n",
    "        for key, value in Eva_measure.items():\n",
    "            f.write('%s:%s\\n' % (key, ','.join(map(str, value))))                    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a66f063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67d6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5ee590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
