{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "403c3c94",
   "metadata": {},
   "source": [
    "L2-Regularization Method on Graph Classification Task of Proteins Dataset\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec68bf61",
   "metadata": {},
   "source": [
    "### All libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09f0b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "import statistics as stat\n",
    "import psutil\n",
    "import itertools\n",
    "import tracemalloc\n",
    "import gc\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sparse_softmax import Sparsemax\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.pool.topk_pool import topk, filter_adj\n",
    "from torch_geometric.utils import softmax, dense_to_sparse, add_remaining_self_loops\n",
    "from torch_scatter import scatter_add\n",
    "from torch_sparse import spspmm, coalesce\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440ba02b",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "018660ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ebd517",
   "metadata": {},
   "source": [
    "### Regularization Rate\n",
    "#### Regularization rates range from the following numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a55f39aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1e2, 1e3, 1e6\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a8a6f5",
   "metadata": {},
   "source": [
    "### Functions for Mmeasuring criterias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e92d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_parameters(model: nn.Module, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the total number of parameters of model\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    num_counted_elements = 0\n",
    "    for param in model.parameters():\n",
    "        if count_nonzero_only:\n",
    "            num_counted_elements += param.count_nonzero()\n",
    "        else:\n",
    "            num_counted_elements += param.numel()\n",
    "    return num_counted_elements\n",
    "\n",
    "# Function to get CPU usage\n",
    "def get_cpu_usage():\n",
    "    return psutil.cpu_percent(interval=1)\n",
    "\n",
    "\n",
    "\n",
    "# Function to approximate power consumption (Assume some average power usage per CPU percentage point)\n",
    "def estimate_power_usage(cpu_usage):\n",
    "    base_power_usage = 10  # Assumed base power usage in watts\n",
    "    power_per_percent = 0.5  # Assumed additional watts per CPU usage percent\n",
    "    return base_power_usage + (power_per_percent * cpu_usage)\n",
    "\n",
    "# The model size based on the number of parameters\n",
    "def get_model_size_param(model: nn.Module, data_width=32, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the model size in bits\n",
    "    :param data_width: #bits per element\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    return get_num_parameters(model, count_nonzero_only) * data_width\n",
    "\n",
    "Byte = 8\n",
    "KiB = 1024 * Byte\n",
    "MiB = 1024 * KiB\n",
    "GiB = 1024 * MiB\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa81264",
   "metadata": {},
   "source": [
    "### Setting Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a2ea1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2c47495a9d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=777, help='random seed')\n",
    "parser.add_argument('--batch_size', type=int, default=512, help='batch size')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.001, help='weight decay')\n",
    "parser.add_argument('--nhid', type=int, default=128, help='hidden size')\n",
    "parser.add_argument('--sample_neighbor', type=bool, default=True, help='whether sample neighbors')\n",
    "parser.add_argument('--sparse_attention', type=bool, default=True, help='whether use sparse attention')\n",
    "parser.add_argument('--structure_learning', type=bool, default=True, help='whether perform structure learning')\n",
    "parser.add_argument('--pooling_ratio', type=float, default=0.5, help='pooling ratio')\n",
    "parser.add_argument('--dropout_ratio', type=float, default=0.0, help='dropout ratio')\n",
    "parser.add_argument('--lamb', type=float, default=1.0, help='trade-off parameter')\n",
    "parser.add_argument('--dataset', type=str, default='PROTEINS', help='DD/PROTEINS/NCI1/NCI109/Mutagenicity/ENZYMES')\n",
    "parser.add_argument('--device', type=str, default='cpu', help='specify cuda devices')\n",
    "parser.add_argument('--epochs', type=int, default=2, help='maximum number of epochs')\n",
    "parser.add_argument('--patience', type=int, default=100, help='patience for early stopping')\n",
    "parser.add_argument('--model_name', type=str, default='HGPSL', help='-')\n",
    "\n",
    "args = parser.parse_args()\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea7061a",
   "metadata": {},
   "source": [
    "### save path for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c119eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.isdir('checkpoint'):\n",
    "    os.mkdir('checkpoint')\n",
    "if not os.path.isdir(os.path.join('checkpoint', args.dataset)):\n",
    "    os.mkdir(os.path.join('checkpoint', f\"{args.dataset}\"))\n",
    "ckpt_dir = f\"./checkpoint/{args.dataset}/\"\n",
    "\n",
    "\n",
    "\n",
    "def save_best(ckpt_dir, epoch, state, model_name, eval_acc, is_best, is_reg):\n",
    "    print('saving....')\n",
    "    model.to(device)\n",
    "    state_save = {\n",
    "        'net':state,\n",
    "        'epoch':epoch,\n",
    "        'acc': eval_acc \n",
    "        }\n",
    "    best_pth_name = f'{args.model_name}_best.pth'\n",
    "    reg_pth_name = f'{args.model_name}_reg_best.pth'\n",
    "    \n",
    "  \n",
    "    if is_reg & is_best:\n",
    "        ckpt_path = os.path.join(ckpt_dir, reg_pth_name) \n",
    "        torch.save(state_save, ckpt_path)\n",
    "    \n",
    "     \n",
    "    if is_reg == False & is_best:\n",
    "        ckpt_path = os.path.join(ckpt_dir, best_pth_name)  \n",
    "        torch.save(state_save, ckpt_path)\n",
    "           \n",
    "        \n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be3ebaf",
   "metadata": {},
   "source": [
    "### start loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4df6ad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/PROTEINS.zip\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(seed=777, batch_size=512, lr=0.001, weight_decay=0.001, nhid=128, sample_neighbor=True, sparse_attention=True, structure_learning=True, pooling_ratio=0.5, dropout_ratio=0.0, lamb=1.0, dataset='PROTEINS', device='cpu', epochs=2, patience=100, model_name='HGPSL', num_classes=2, num_features=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(os.path.join('data', args.dataset), name=args.dataset, use_node_attr=True)\n",
    "\n",
    "args.num_classes = dataset.num_classes\n",
    "args.num_features = dataset.num_features\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa096cbf",
   "metadata": {},
   "source": [
    "### Preprocessing  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "418e883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training = int(len(dataset) * 0.8)\n",
    "num_val = int(len(dataset) * 0.1)\n",
    "num_test = len(dataset) - (num_training + num_val)\n",
    "training_set, validation_set, test_set = random_split(dataset, [num_training, num_val, num_test])\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=args.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_set, batch_size=args.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c4c315",
   "metadata": {},
   "source": [
    "### Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e979cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TwoHopNeighborhood(object):\n",
    "    def __call__(self, data):\n",
    "        edge_index, edge_attr = data.edge_index, data.edge_attr\n",
    "        n = data.num_nodes\n",
    "\n",
    "        fill = 1e16\n",
    "        value = edge_index.new_full((edge_index.size(1),), fill, dtype=torch.float)\n",
    "\n",
    "        index, value = spspmm(edge_index, value, edge_index, value, n, n, n, True)\n",
    "\n",
    "        edge_index = torch.cat([edge_index, index], dim=1)\n",
    "        if edge_attr is None:\n",
    "            data.edge_index, _ = coalesce(edge_index, None, n, n)\n",
    "        else:\n",
    "            value = value.view(-1, *[1 for _ in range(edge_attr.dim() - 1)])\n",
    "            value = value.expand(-1, *list(edge_attr.size())[1:])\n",
    "            edge_attr = torch.cat([edge_attr, value], dim=0)\n",
    "            #, fill_value=fill\n",
    "            data.edge_index, edge_attr = coalesce(edge_index, edge_attr, n, n, op='min')\n",
    "            edge_attr[edge_attr >= fill] = 0\n",
    "            data.edge_attr = edge_attr\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}()'.format(self.__class__.__name__)\n",
    "\n",
    "\n",
    "class GCN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, cached=False, bias=True, **kwargs):\n",
    "        super(GCN, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "            nn.init.zeros_(self.bias.data)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = torch.matmul(x, self.weight)\n",
    "\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\n",
    "\n",
    "\n",
    "class NodeInformationScore(MessagePassing):\n",
    "    def __init__(self, improved=False, cached=False, **kwargs):\n",
    "        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes)\n",
    "\n",
    "        row, col = edge_index\n",
    "        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n",
    "        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "\n",
    "class HGPSLPool(torch.nn.Module):\n",
    "    def __init__(self, in_channels, ratio=0.8, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2):\n",
    "        super(HGPSLPool, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = ratio\n",
    "        self.sample = sample\n",
    "        self.sparse = sparse\n",
    "        self.sl = sl\n",
    "        self.negative_slop = negative_slop\n",
    "        self.lamb = lamb\n",
    "\n",
    "        self.att = Parameter(torch.Tensor(1, self.in_channels * 2))\n",
    "        nn.init.xavier_uniform_(self.att.data)\n",
    "        self.sparse_attention = Sparsemax()\n",
    "        self.neighbor_augment = TwoHopNeighborhood()\n",
    "        self.calc_information_score = NodeInformationScore()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch=None):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "\n",
    "        x_information_score = self.calc_information_score(x, edge_index, edge_attr)\n",
    "        score = torch.sum(torch.abs(x_information_score), dim=1)\n",
    "\n",
    "        # Graph Pooling\n",
    "        original_x = x\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "        x = x[perm]\n",
    "        batch = batch[perm]\n",
    "        induced_edge_index, induced_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "        # Discard structure learning layer, directly return\n",
    "        if self.sl is False:\n",
    "            return x, induced_edge_index, induced_edge_attr, batch\n",
    "\n",
    "        # Structure Learning\n",
    "        if self.sample:\n",
    "            # A fast mode for large graphs.\n",
    "            # In large graphs, learning the possible edge weights between each pair of nodes is time consuming.\n",
    "            # To accelerate this process, we sample it's K-Hop neighbors for each node and then learn the\n",
    "            # edge weights between them.\n",
    "            k_hop = 3\n",
    "            if edge_attr is None:\n",
    "                edge_attr = torch.ones((edge_index.size(1),), dtype=torch.float, device=edge_index.device)\n",
    "\n",
    "            hop_data = Data(x=original_x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "            for _ in range(k_hop - 1):\n",
    "                hop_data = self.neighbor_augment(hop_data)\n",
    "            hop_edge_index = hop_data.edge_index\n",
    "            hop_edge_attr = hop_data.edge_attr\n",
    "            new_edge_index, new_edge_attr = filter_adj(hop_edge_index, hop_edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "            new_edge_index, new_edge_attr = add_remaining_self_loops(new_edge_index, new_edge_attr, 0, x.size(0))\n",
    "            row, col = new_edge_index\n",
    "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
    "            weights = F.leaky_relu(weights, self.negative_slop) + new_edge_attr * self.lamb\n",
    "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
    "            adj[row, col] = weights\n",
    "            new_edge_index, weights = dense_to_sparse(adj)\n",
    "            row, col = new_edge_index\n",
    "            if self.sparse:\n",
    "                new_edge_attr = self.sparse_attention(weights, row)\n",
    "            else:\n",
    "                new_edge_attr = softmax(weights, row, x.size(0))\n",
    "            # filter out zero weight edges\n",
    "            adj[row, col] = new_edge_attr\n",
    "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
    "            # release gpu memory\n",
    "            del adj\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            # Learning the possible edge weights between each pair of nodes in the pooled subgraph, relative slower.\n",
    "            if edge_attr is None:\n",
    "                induced_edge_attr = torch.ones((induced_edge_index.size(1),), dtype=x.dtype,\n",
    "                                               device=induced_edge_index.device)\n",
    "            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "            shift_cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "            cum_num_nodes = num_nodes.cumsum(dim=0)\n",
    "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
    "            # Construct batch fully connected graph in block diagonal matirx format\n",
    "            for idx_i, idx_j in zip(shift_cum_num_nodes, cum_num_nodes):\n",
    "                adj[idx_i:idx_j, idx_i:idx_j] = 1.0\n",
    "            new_edge_index, _ = dense_to_sparse(adj)\n",
    "            row, col = new_edge_index\n",
    "\n",
    "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
    "            weights = F.leaky_relu(weights, self.negative_slop)\n",
    "            adj[row, col] = weights\n",
    "            induced_row, induced_col = induced_edge_index\n",
    "\n",
    "            adj[induced_row, induced_col] += induced_edge_attr * self.lamb\n",
    "            weights = adj[row, col]\n",
    "            if self.sparse:\n",
    "                new_edge_attr = self.sparse_attention(weights, row)\n",
    "            else:\n",
    "                new_edge_attr = softmax(weights, row, x.size(0))\n",
    "            # filter out zero weight edges\n",
    "            adj[row, col] = new_edge_attr\n",
    "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
    "            # release gpu memory\n",
    "            del adj\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return x, new_edge_index, new_edge_attr, batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4e02a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Model, self).__init__()\n",
    "        self.args = args\n",
    "        self.num_features = args.num_features\n",
    "        self.nhid = args.nhid\n",
    "        self.num_classes = args.num_classes\n",
    "        self.pooling_ratio = args.pooling_ratio\n",
    "        self.dropout_ratio = args.dropout_ratio\n",
    "        self.sample = args.sample_neighbor\n",
    "        self.sparse = args.sparse_attention\n",
    "        self.sl = args.structure_learning\n",
    "        self.lamb = args.lamb\n",
    "\n",
    "        self.conv1 = GCNConv(self.num_features, self.nhid)\n",
    "        self.conv2 = GCN(self.nhid, self.nhid)\n",
    "        self.conv3 = GCN(self.nhid, self.nhid)\n",
    "\n",
    "        self.pool1 = HGPSLPool(self.nhid, self.pooling_ratio, self.sample, self.sparse, self.sl, self.lamb)\n",
    "        self.pool2 = HGPSLPool(self.nhid, self.pooling_ratio, self.sample, self.sparse, self.sl, self.lamb)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(self.nhid * 2, self.nhid)\n",
    "        self.lin2 = torch.nn.Linear(self.nhid, self.nhid // 2)\n",
    "        self.lin3 = torch.nn.Linear(self.nhid // 2, self.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        edge_attr = None\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x, edge_index, edge_attr, batch = self.pool1(x, edge_index, edge_attr, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x, edge_index, edge_attr, batch = self.pool2(x, edge_index, edge_attr, batch)\n",
    "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv3(x, edge_index, edge_attr))\n",
    "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(x1) + F.relu(x2) + F.relu(x3)\n",
    "\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n",
    "        x = F.log_softmax(self.lin3(x), dim=-1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08d0aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(args)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6de11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a6ea248",
   "metadata": {},
   "source": [
    "### Required functions  for training with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21600921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, reg_rate):\n",
    "    loss_train = 0.0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for i, data in enumerate(train_loader):\n",
    "            #data = data.to(args.device)\n",
    "            out = model(data)\n",
    "            loss = F.nll_loss(out, data.y)\n",
    "            l2_reg = torch.tensor(0.)\n",
    "            for param in model.parameters():\n",
    "                l2_reg += torch.norm(param)\n",
    "\n",
    "            # Combine the loss function with L2 regularization\n",
    "            loss += (reg_rate * l2_reg)\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "            pred = out.max(dim=1)[1]\n",
    "            correct += pred.eq(data.y).sum().item()\n",
    "    acc_train = correct / len(train_loader.dataset)\n",
    "    \n",
    "    return   loss_train,acc_train  \n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "def compute_test(loader):\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    loss_test = 0.0\n",
    "    for data in loader:\n",
    "        #data = data.to(args.device)\n",
    "        out = model(data)\n",
    "        pred = out.max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "        loss_test += F.nll_loss(out, data.y).item()\n",
    "    return correct / len(loader.dataset), loss_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3761e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, train_loader, l2_lambda):\n",
    "    min_loss = 1e10\n",
    "    patience_cnt = 0\n",
    "    val_loss_values = []\n",
    "    best_epoch = 0\n",
    "    save_epoch=100\n",
    "    print(f\"the reregularization rate is : {l2_lambda}\")\n",
    "    if l2_lambda==0:\n",
    "        is_reg=False\n",
    "    else:\n",
    "        is_reg=True\n",
    "   \n",
    "    #model.train()\n",
    "    t = time.time()\n",
    "    for epoch in range(args.epochs):\n",
    "        #loss_train = 0.0\n",
    "        #correct = 0\n",
    "        loss_train,acc_train =train(model, train_loader,l2_lambda)\n",
    "        \n",
    "        acc_val, loss_val = compute_test(val_loader)\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print('Epoch: {:04d}'.format(epoch ), 'loss_train: {:.6f}'.format(loss_train),\n",
    "                  'acc_train: {:.6f}'.format(acc_train), 'loss_val: {:.6f}'.format(loss_val),\n",
    "                  'acc_val: {:.6f}'.format(acc_val), 'time: {:.6f}s'.format(time.time() - t))\n",
    "\n",
    "        val_loss_values.append(loss_val)\n",
    "\n",
    "        if val_loss_values[-1] < min_loss:\n",
    "            min_loss = val_loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            patience_cnt = 0\n",
    "            is_best=True\n",
    "            save_best(ckpt_dir, epoch, model.state_dict(), args.model_name, acc_val, is_best, is_reg)\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "\n",
    "        if patience_cnt == args.patience:\n",
    "            break\n",
    "  \n",
    "\n",
    "    print('Optimization Finished! Total time elapsed: {:.6f}'.format(time.time() - t))\n",
    "    \n",
    "  \n",
    "    return best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2448a2e6",
   "metadata": {},
   "source": [
    "## Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e1a8f",
   "metadata": {},
   "source": [
    "#### Setting Regularization rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b678d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Regularization Rate\n",
    "\n",
    "l2_lambda = 0.9\n",
    "# The number of iteration\n",
    "num_iterations=1\n",
    "# The number of epochs\n",
    "args.epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de17578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a dictionary to save all measurements. Aftre measuring, we can compute mean and std of each item.\n",
    "Eva_final=dict()\n",
    "\n",
    "# The following are all list of criteria for measurements. \n",
    "# We collect all desired datas of each list across iterations. \n",
    "# Then, we compute average and std of each list.\n",
    "\n",
    "#Base model\n",
    "Base_model_accuracy=[]\n",
    "T_base_model=[]\n",
    "Num_parm_base_model=[]\n",
    "Base_model_size=[]\n",
    "Base_Energy_Consumption=[]\n",
    "Base_Cpu_Usage=[]\n",
    "Base_Memory_Usage=[]\n",
    "\n",
    "#regularized model\n",
    "Reg_model_accuracy=[]\n",
    "T_Reg_model=[]\n",
    "Num_parm_Reg_model=[]\n",
    "Reg_model_size=[]\n",
    "Reg_Energy_Consumption=[]\n",
    "Reg_Cpu_Usage=[]\n",
    "Reg_Memory_Usage=[]\n",
    "\n",
    "\n",
    "# Here is the dictionary to record the list of all measurements\n",
    "Eva_measure={'base model accuracy':Base_model_accuracy,\n",
    "            'time inference of base model':T_base_model,\n",
    "            'number parmameters of base model':Num_parm_base_model,\n",
    "            'base model size':Base_model_size,\n",
    "            'energy consumption of base model':Base_Energy_Consumption,\n",
    "            'cpu usage of base model':Base_Cpu_Usage,\n",
    "            'memory usage of base model':Base_Memory_Usage,\n",
    "            'regularized model accuracy': Reg_model_accuracy,\n",
    "            'time inference of regularized model':T_Reg_model,\n",
    "            'number parmameters of regularized model':Num_parm_Reg_model,\n",
    "            'regularized model size':Reg_model_size,\n",
    "            'energy consumption of regularized model':Reg_Energy_Consumption,\n",
    "            'cpu usage of regularized model':Reg_Cpu_Usage,\n",
    "            'memory usage of regularized model':Reg_Memory_Usage\n",
    "            }\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74512d4",
   "metadata": {},
   "source": [
    "### Training and Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59b23f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________\n",
      "************************************************\n",
      "This is iteration :1\n",
      "Training and evaluation before regularization \n",
      "Starting training...\n",
      "the reregularization rate is : 0\n",
      "Epoch: 0000 loss_train: 1.338034 acc_train: 0.621348 loss_val: 0.646126 acc_val: 0.621622 time: 9.124206s\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "Epoch: 0020 loss_train: 1.243334 acc_train: 0.666292 loss_val: 0.603657 acc_val: 0.711712 time: 188.603925s\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "Epoch: 0040 loss_train: 1.178836 acc_train: 0.706742 loss_val: 0.589234 acc_val: 0.729730 time: 360.740319s\n",
      "saving....\n",
      "saving....\n",
      "Epoch: 0060 loss_train: 1.174197 acc_train: 0.695506 loss_val: 0.579936 acc_val: 0.720721 time: 532.519994s\n",
      "saving....\n",
      "Epoch: 0080 loss_train: 1.117458 acc_train: 0.733708 loss_val: 0.597407 acc_val: 0.702703 time: 702.769598s\n",
      "Optimization Finished! Total time elapsed: 863.841113\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.84%\n",
      "base model has size=306667.00 bit\n",
      "The time inference of base model is =2.7072160999996413\n",
      "The number of parametrs of base model is:75437\n",
      "Energy Consumption : 39.119\n",
      "total memory usage of base model':135292 \n",
      "cpu usage of base model':2.700 %\n",
      "___________*******************************__________\n",
      "Regularized Model\n",
      "the reregularization rate is : 0.9\n",
      "Epoch: 0000 loss_train: 74.720615 acc_train: 0.485393 loss_val: 0.655182 acc_val: 0.621622 time: 8.548690s\n",
      "saving....\n",
      "saving....\n",
      "Epoch: 0020 loss_train: 42.658421 acc_train: 0.579775 loss_val: 0.673443 acc_val: 0.621622 time: 182.525005s\n",
      "Epoch: 0040 loss_train: 20.644137 acc_train: 0.579775 loss_val: 0.692912 acc_val: 0.621622 time: 363.247303s\n",
      "Epoch: 0060 loss_train: 7.516366 acc_train: 0.579775 loss_val: 0.693157 acc_val: 0.378378 time: 549.119931s\n",
      "Epoch: 0080 loss_train: 2.518693 acc_train: 0.420225 loss_val: 0.693177 acc_val: 0.378378 time: 758.934919s\n",
      "Optimization Finished! Total time elapsed: 929.987462\n",
      "****************Results of regularized model ******************\n",
      "0.9 regularized model has accuracy on test set=0.70%\n",
      "0.9 regularized model has size=306731.00 bit\n",
      "The time inference of 0.9 regularized model is =2.6395898000009765\n",
      "The number of parametrs of 0.9 regularized model is:75406\n",
      "Energy Consumption of 0.9 regularized model: 28.904\n",
      "total memory usage of 0.9 regularized model':38566 \n",
      "cpu usage of 0.9 regularized model':6.500 %\n",
      "________________________________________________\n",
      "************************************************\n",
      "This is iteration :2\n",
      "Training and evaluation before regularization \n",
      "Starting training...\n",
      "the reregularization rate is : 0\n",
      "Epoch: 0000 loss_train: 1.413451 acc_train: 0.457303 loss_val: 0.663303 acc_val: 0.621622 time: 8.577790s\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n",
      "saving....\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(args)\n\u001b[0;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mweight_decay)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreg_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#### load the best model\u001b[39;00m\n\u001b[0;32m     15\u001b[0m base_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ckpt_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_best.pth\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(model, train_loader, l2_lambda)\u001b[0m\n\u001b[0;32m     14\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m#loss_train = 0.0\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m#correct = 0\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     loss_train,acc_train \u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43ml2_lambda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     acc_val, loss_val \u001b[38;5;241m=\u001b[39m compute_test(val_loader)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, reg_rate)\u001b[0m\n\u001b[0;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;66;03m#data = data.to(args.device)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m         loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(out, data\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m     10\u001b[0m         l2_reg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     36\u001b[0m x2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([gmp(x, batch), gap(x, batch)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x, edge_index, edge_attr))\n\u001b[1;32m---> 39\u001b[0m x3 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([gmp(x, batch), \u001b[43mgap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x1) \u001b[38;5;241m+\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x2) \u001b[38;5;241m+\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x3)\n\u001b[0;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin1(x))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\nn\\pool\\glob.py:63\u001b[0m, in \u001b[0;36mglobal_mean_pool\u001b[1;34m(x, batch, size)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39mdim, keepdim\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\utils\\_scatter.py:82\u001b[0m, in \u001b[0;36mscatter\u001b[1;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[0;32m     79\u001b[0m count\u001b[38;5;241m.\u001b[39mscatter_add_(\u001b[38;5;241m0\u001b[39m, index, src\u001b[38;5;241m.\u001b[39mnew_ones(src\u001b[38;5;241m.\u001b[39msize(dim)))\n\u001b[0;32m     80\u001b[0m count \u001b[38;5;241m=\u001b[39m count\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mbroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m out \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(size)\u001b[38;5;241m.\u001b[39mscatter_add_(dim, index, src)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out \u001b[38;5;241m/\u001b[39m broadcast(count, out, dim)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\utils\\_scatter.py:195\u001b[0m, in \u001b[0;36mbroadcast\u001b[1;34m(src, ref, dim)\u001b[0m\n\u001b[0;32m    193\u001b[0m dim \u001b[38;5;241m=\u001b[39m ref\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m+\u001b[39m dim \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m dim\n\u001b[0;32m    194\u001b[0m size \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;241m1\u001b[39m, ) \u001b[38;5;241m*\u001b[39m dim) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ) \u001b[38;5;241m+\u001b[39m ((\u001b[38;5;241m1\u001b[39m, ) \u001b[38;5;241m*\u001b[39m (ref\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m-\u001b[39m dim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexpand_as(ref)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(num_iterations):\n",
    "        print('________________________________________________')\n",
    "        print('************************************************')\n",
    "        print(f\"This is iteration :{i+1}\")\n",
    "\n",
    "        Eva=dict() # It is a dictionary to arrange output of this iteration\n",
    "\n",
    "        print(f'Training and evaluation before regularization ')\n",
    "        print(\"Starting training...\")\n",
    "        reg_rate=0\n",
    "        model = Model(args)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        run(model, train_loader,reg_rate)\n",
    "        #### load the best model\n",
    "        base_model_path = os.path.join(ckpt_dir, f'{args.model_name}_best.pth') \n",
    "        checkpoint = torch.load(base_model_path)\n",
    "        model.load_state_dict(checkpoint['net'])      \n",
    "        recover_model = lambda: model.load_state_dict(checkpoint['net'])\n",
    "\n",
    "        # Start monitoring CPU and memory usage, model size, number of parametes, time inference and  power consumption\n",
    "        gc.collect()\n",
    "        time.sleep(5)  # Add a 5-second delay to stabilize the initial state\n",
    "        tracemalloc.start()  # Start tracking memory allocations\n",
    "        snapshot_before = tracemalloc.take_snapshot()#take a snapshot of the current memory state before starting the measurement.\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        initial_cpu_usage = get_cpu_usage()\n",
    "        power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "\n",
    "        base_model_accuracy, test_loss = compute_test(test_loader)\n",
    "\n",
    "        base_cpu_usage = get_cpu_usage()\n",
    "        t1 = time.perf_counter()\n",
    "        t_base_model=t1-t0\n",
    "\n",
    "        snapshot_after = tracemalloc.take_snapshot()\n",
    "        tracemalloc.stop()\n",
    "        top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "        base_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "        base_energy_consumption = power_usage * t_base_model\n",
    "        base_model_size = os.path.getsize(base_model_path)\n",
    "        num_parm_base_model=get_num_parameters(model, count_nonzero_only=True)\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5) \n",
    "\n",
    "        print(f'*****Results of base model*********')\n",
    "\n",
    "        print(f\"base model has accuracy on test set={base_model_accuracy:.2f}%\")\n",
    "        print(f\"base model has size={base_model_size:.2f} bit\")\n",
    "        print(f\"The time inference of base model is ={t_base_model}\") \n",
    "        print(f\"The number of parametrs of base model is:{num_parm_base_model}\") \n",
    "\n",
    "        print(f\"Energy Consumption : {base_energy_consumption:.3f}\")\n",
    "        print(f\"total memory usage of base model':{base_total_memory_diff} \")\n",
    "        print(f\"cpu usage of base model':{base_cpu_usage:.3f} %\")\n",
    "\n",
    "\n",
    "        #Update Eva dictionary\n",
    "        Eva.update({'base model accuracy': base_model_accuracy,\n",
    "                    'time inference of base model': t_base_model,\n",
    "                    'number parmameters of base model': num_parm_base_model,\n",
    "                    'size of base model': base_model_size, \n",
    "                    'energy consumption of base model':base_energy_consumption,\n",
    "                    'total memory usage of base model':base_total_memory_diff,\n",
    "                    'cpu usage of base model':base_cpu_usage\n",
    "                   })\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5)  \n",
    "\n",
    "        #### Regularization of the Model\n",
    "        gc.collect()\n",
    "        time.sleep(5)   \n",
    "\n",
    "        print('___________*******************************__________')\n",
    "        print(f'Regularized Model')\n",
    "      \n",
    "        reg_rate=l2_lambda\n",
    " \n",
    "        model = Model(args)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        run(model, train_loader,reg_rate)\n",
    "      \n",
    "\n",
    "        reg_pth_name = f'{args.model_name}_reg_best.pth'\n",
    "        reg_model_path = os.path.join(ckpt_dir, reg_pth_name) \n",
    "        checkpoint = torch.load(reg_model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        recover_model = lambda: model.load_state_dict(checkpoint['net'])\n",
    "\n",
    "\n",
    "        # Result of regularization\n",
    "\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5)  \n",
    "        tracemalloc.start() \n",
    "        snapshot_before = tracemalloc.take_snapshot()\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        initial_cpu_usage = get_cpu_usage()\n",
    "        power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "\n",
    "        regularized_model_accuracy, test_loss = compute_test(test_loader)\n",
    "\n",
    "        regularized_cpu_usage = get_cpu_usage()\n",
    "        t1 = time.perf_counter()\n",
    "        t_regularized_model=t1-t0\n",
    "\n",
    "        snapshot_after = tracemalloc.take_snapshot()\n",
    "        tracemalloc.stop()\n",
    "        top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "        regularized_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "        regularized_energy_consumption = power_usage * t_regularized_model\n",
    "        regularized_model_size = os.path.getsize( reg_model_path )\n",
    "        num_parm_regularized_model=get_num_parameters(model, count_nonzero_only=True)\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5)  # Add a 5-second delay to stabilize the initial state    \n",
    "\n",
    "\n",
    "\n",
    "        print('****************Results of regularized model ******************')\n",
    "\n",
    "\n",
    "        print(f\"{l2_lambda} regularized model has accuracy on test set={regularized_model_accuracy:.2f}%\")\n",
    "        print(f\"{l2_lambda} regularized model has size={regularized_model_size:.2f} bit\")\n",
    "        print(f\"The time inference of {l2_lambda} regularized model is ={t_regularized_model}\") \n",
    "        print(f\"The number of parametrs of {l2_lambda} regularized model is:{num_parm_regularized_model}\") \n",
    "\n",
    "        print(f\"Energy Consumption of {l2_lambda} regularized model: {regularized_energy_consumption:.3f}\")\n",
    "        print(f\"total memory usage of {l2_lambda} regularized model':{regularized_total_memory_diff} \")\n",
    "        print(f\"cpu usage of {l2_lambda} regularized model':{regularized_cpu_usage:.3f} %\")\n",
    "\n",
    "\n",
    "        #Update Eva dictionary\n",
    "        Eva.update({'regularized model accuracy': regularized_model_accuracy,\n",
    "                    'time inference of regularized model': t_regularized_model,\n",
    "                    'number parmameters of regularized model': num_parm_regularized_model,\n",
    "                    'size of regularized model': regularized_model_size, \n",
    "                    'energy consumption of regularized model':regularized_energy_consumption,\n",
    "                    'total memory usage of regularized model':regularized_total_memory_diff,\n",
    "                    'cpu usage of regularized model':regularized_cpu_usage\n",
    "                   })\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5)   \n",
    "\n",
    "\n",
    "\n",
    "        Base_model_accuracy.append(Eva['base model accuracy'])\n",
    "        T_base_model.append(Eva['time inference of base model'])\n",
    "        Num_parm_base_model.append(int(Eva['number parmameters of base model']))\n",
    "        Base_model_size.append(int(Eva['size of base model']))\n",
    "        Base_Energy_Consumption.append(Eva['energy consumption of base model'])\n",
    "        Base_Cpu_Usage.append(Eva['cpu usage of base model'])\n",
    "        Base_Memory_Usage.append(Eva['total memory usage of base model'])\n",
    "\n",
    "        Reg_model_accuracy.append(Eva['regularized model accuracy'])\n",
    "        T_Reg_model.append(Eva['time inference of regularized model'])\n",
    "        Num_parm_Reg_model.append(int(Eva['number parmameters of regularized model']))\n",
    "        Reg_model_size.append(int(Eva['size of regularized model']))\n",
    "        Reg_Energy_Consumption.append(Eva['energy consumption of regularized model'])\n",
    "        Reg_Cpu_Usage.append(Eva['cpu usage of regularized model'])\n",
    "        Reg_Memory_Usage.append(Eva['total memory usage of regularized model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e449299c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model accuracy is:0.757 ± 0.026\n",
      "Time inference of Base model :2.631 ± 0.048\n",
      "Time number of parameters of Base model :75420.000 ± 6.595\n",
      "The size of Base model :306667 bytes\n",
      "The energy consumption of Base model :56.884 ± 16.359 \n",
      "The CPU usage of Base model :21.560 ± 10.104 \n",
      "The memory usage of Base model :36201.400 ± 336.838 \n",
      "====================================================================================================\n",
      "Regularized model accuracy is:0.736 ± 0.043\n",
      "Time inference of Regularized model :2.680 ± 0.056\n",
      "Time number of parameters of Regularized model :75423.000 ± 2.236\n",
      "The size of Regularized model :306731.000 ± 0.000 bytes\n",
      "The energy consumption of Regularized model :75.679 ± 39.322 \n",
      "The CPU usage of Regularized model :24.360 ± 6.916 \n",
      "The memory usage of Regularized model :35879.800 ± 117.001 \n",
      "====================================================================================================\n",
      "All measurement about regularization process of rate:0.9 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Ave of base model accuracy': 0.757,\n",
       " 'Std of base model accuracy': 0.026,\n",
       " 'Ave of time inference of base model': 2.631,\n",
       " 'Std of time inference of base model': 0.048,\n",
       " 'Ave of number parmameters of base model': 75420,\n",
       " 'Std of number parmameters of base model': 6.59545297913646,\n",
       " 'Ave of base model size': 306667,\n",
       " 'Std of base model size': 0.0,\n",
       " 'Ave of energy consumption of base model': 56.883636994300176,\n",
       " 'Std of energy consumption of base model': 16.359177016313676,\n",
       " 'Ave of cpu usage of base model': 21.56,\n",
       " 'Std of cpu usage of base model': 10.104108075431498,\n",
       " 'Ave of memory usage of base model': 36201.4,\n",
       " 'Std of memory usage of base model': 336.83794323086585,\n",
       " 'Ave of regularized model accuracy': 0.736,\n",
       " 'Std of regularized model accuracy': 0.043,\n",
       " 'Ave of time inference of regularized model': 2.68,\n",
       " 'Std of time inference of regularized model': 0.056,\n",
       " 'Ave of number parmameters of regularized model': 75423,\n",
       " 'Std of number parmameters of regularized model': 2.23606797749979,\n",
       " 'Ave of regularized model size': 306731,\n",
       " 'Std of regularized model size': 0.0,\n",
       " 'Ave of energy consumption of regularized model': 75.67862416579621,\n",
       " 'Std of energy consumption of regularized model': 39.321624901934406,\n",
       " 'Ave of cpu usage of regularized model': 24.36,\n",
       " 'Std of cpu usage of regularized model': 6.915779059513108,\n",
       " 'Ave of memory usage of regularized model': 35879.8,\n",
       " 'Std of memory usage of regularized model': 117.00085469773288}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eva_final=dict()\n",
    "base_model_accuracy_mean = stat.mean(Base_model_accuracy)\n",
    "base_model_accuracy_std =  stat.stdev(Base_model_accuracy)\n",
    "Eva_final.update({'Ave of base model accuracy':float(format(base_model_accuracy_mean, '.3f'))})\n",
    "Eva_final.update({'Std of base model accuracy':float(format(base_model_accuracy_std, '.3f'))})\n",
    "base_model_accuracy = \"{:.3f} ± {:.3f}\".format(base_model_accuracy_mean ,base_model_accuracy_std)\n",
    "print(f\"Base model accuracy is:{base_model_accuracy}\")\n",
    "\n",
    "                 \n",
    "t_base_model_mean =stat.mean(T_base_model)\n",
    "t_base_model_std =stat.stdev(T_base_model)  \n",
    "Eva_final.update({'Ave of time inference of base model':float(format(t_base_model_mean, '.3f'))})\n",
    "Eva_final.update({'Std of time inference of base model':float(format(t_base_model_std, '.3f'))})\n",
    "t_base_model = \"{:.3f} ± {:.3f}\".format(t_base_model_mean ,t_base_model_std)\n",
    "print(f\"Time inference of Base model :{t_base_model}\")\n",
    "\n",
    "\n",
    "num_parm_base_model_mean = stat.mean(Num_parm_base_model)\n",
    "num_parm_base_model_std = stat.stdev(Num_parm_base_model)\n",
    "Eva_final.update({'Ave of number parmameters of base model':num_parm_base_model_mean})\n",
    "Eva_final.update({'Std of number parmameters of base model':num_parm_base_model_std})\n",
    "num_parm_base_model = \"{:.3f} ± {:.3f}\".format(num_parm_base_model_mean ,num_parm_base_model_std)\n",
    "print(f\"Time number of parameters of Base model :{num_parm_base_model}\")\n",
    "\n",
    "base_model_size_mean = stat.mean(Base_model_size)\n",
    "base_model_size_std = stat.stdev(Base_model_size)\n",
    "Eva_final.update({'Ave of base model size':base_model_size_mean})\n",
    "Eva_final.update({'Std of base model size':base_model_size_std})\n",
    "base_model_size_model = \"{:.3f} ± {:.3f}\".format(base_model_size_mean ,base_model_size_std)\n",
    "print(f\"The size of Base model :{base_model_size} bytes\")\n",
    "\n",
    "\n",
    "base_energy_consumption_mean = stat.mean(Base_Energy_Consumption)\n",
    "base_energy_consumption_std = stat.stdev(Base_Energy_Consumption)\n",
    "Eva_final.update({'Ave of energy consumption of base model':base_energy_consumption_mean })\n",
    "Eva_final.update({'Std of energy consumption of base model':base_energy_consumption_std})\n",
    "base_energy_consumption = \"{:.3f} ± {:.3f}\".format(base_energy_consumption_mean ,base_energy_consumption_std)\n",
    "print(f\"The energy consumption of Base model :{base_energy_consumption} \")\n",
    "\n",
    "\n",
    "base_cpu_usage_mean = stat.mean(Base_Cpu_Usage)\n",
    "base_cpu_usage_std = stat.stdev(Base_Cpu_Usage)\n",
    "Eva_final.update({'Ave of cpu usage of base model':base_cpu_usage_mean})\n",
    "Eva_final.update({'Std of cpu usage of base model':base_cpu_usage_std})\n",
    "base_cpu_usage = \"{:.3f} ± {:.3f}\".format(base_cpu_usage_mean ,base_cpu_usage_std)\n",
    "print(f\"The CPU usage of Base model :{base_cpu_usage} \")\n",
    "\n",
    "\n",
    "base_memory_usage_mean = stat.mean(Base_Memory_Usage)\n",
    "base_memory_usage_std = stat.stdev(Base_Memory_Usage)\n",
    "Eva_final.update({'Ave of memory usage of base model':base_memory_usage_mean})\n",
    "Eva_final.update({'Std of memory usage of base model':base_memory_usage_std})\n",
    "base_memory_usage = \"{:.3f} ± {:.3f}\".format(base_memory_usage_mean ,base_memory_usage_std)\n",
    "print(f\"The memory usage of Base model :{base_memory_usage} \")\n",
    "\n",
    "print(100 * \"=\")\n",
    "####################################################\n",
    "\n",
    "reg_model_accuracy_mean =stat.mean(Reg_model_accuracy)\n",
    "reg_model_accuracy_std = stat.stdev(Reg_model_accuracy)\n",
    "Eva_final.update({'Ave of regularized model accuracy':float(format(reg_model_accuracy_mean, '.3f'))})\n",
    "Eva_final.update({'Std of regularized model accuracy':float(format(reg_model_accuracy_std, '.3f'))})\n",
    "reg_model_accuracy = \"{:.3f} ± {:.3f}\".format(reg_model_accuracy_mean ,reg_model_accuracy_std)\n",
    "print(f\"Regularized model accuracy is:{reg_model_accuracy}\")\n",
    "                 \n",
    "\n",
    "t_reg_model_mean = stat.mean(T_Reg_model)\n",
    "t_reg_model_std =stat.stdev(T_Reg_model)\n",
    "Eva_final.update({'Ave of time inference of regularized model':float(format(t_reg_model_mean, '.3f'))})\n",
    "Eva_final.update({'Std of time inference of regularized model':float(format(t_reg_model_std, '.3f'))})\n",
    "t_reg_model = \"{:.3f} ± {:.3f}\".format(t_reg_model_mean ,t_reg_model_std)\n",
    "print(f\"Time inference of Regularized model :{t_reg_model}\")\n",
    "\n",
    "num_parm_reg_model_mean = stat.mean(Num_parm_Reg_model)\n",
    "num_parm_reg_model_std = stat.stdev(Num_parm_Reg_model)\n",
    "Eva_final.update({'Ave of number parmameters of regularized model':num_parm_reg_model_mean})\n",
    "Eva_final.update({'Std of number parmameters of regularized model':num_parm_reg_model_std})\n",
    "num_parm_reg_model = \"{:.3f} ± {:.3f}\".format(num_parm_reg_model_mean ,num_parm_reg_model_std)\n",
    "print(f\"Time number of parameters of Regularized model :{num_parm_reg_model}\")\n",
    "\n",
    "reg_model_size_mean =stat.mean( Reg_model_size)\n",
    "reg_model_size_std = stat.stdev(Reg_model_size)\n",
    "Eva_final.update({'Ave of regularized model size':reg_model_size_mean})\n",
    "Eva_final.update({'Std of regularized model size':reg_model_size_std })\n",
    "reg_model_size = \"{:.3f} ± {:.3f}\".format(reg_model_size_mean ,reg_model_size_std)\n",
    "print(f\"The size of Regularized model :{reg_model_size} bytes\")\n",
    "\n",
    "reg_energy_consumption_mean = stat.mean(Reg_Energy_Consumption)\n",
    "reg_energy_consumption_std = stat.stdev(Reg_Energy_Consumption)\n",
    "Eva_final.update({'Ave of energy consumption of regularized model':reg_energy_consumption_mean })\n",
    "Eva_final.update({'Std of energy consumption of regularized model':reg_energy_consumption_std})\n",
    "reg_energy_consumption = \"{:.3f} ± {:.3f}\".format(reg_energy_consumption_mean ,reg_energy_consumption_std)\n",
    "print(f\"The energy consumption of Regularized model :{reg_energy_consumption} \")\n",
    "\n",
    "\n",
    "reg_cpu_usage_mean = stat.mean(Reg_Cpu_Usage)\n",
    "reg_cpu_usage_std = stat.stdev(Reg_Cpu_Usage)\n",
    "Eva_final.update({'Ave of cpu usage of regularized model':reg_cpu_usage_mean})\n",
    "Eva_final.update({'Std of cpu usage of regularized model':reg_cpu_usage_std})\n",
    "reg_cpu_usage = \"{:.3f} ± {:.3f}\".format(reg_cpu_usage_mean ,reg_cpu_usage_std)\n",
    "print(f\"The CPU usage of Regularized model :{reg_cpu_usage} \")\n",
    "\n",
    "\n",
    "reg_memory_usage_mean = stat.mean(Reg_Memory_Usage)\n",
    "reg_memory_usage_std = stat.stdev(Reg_Memory_Usage)\n",
    "#desc = \"{:.3f} ± {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of memory usage of regularized model':reg_memory_usage_mean})\n",
    "Eva_final.update({'Std of memory usage of regularized model':reg_memory_usage_std})\n",
    "reg_memory_usage = \"{:.3f} ± {:.3f}\".format(reg_memory_usage_mean ,reg_memory_usage_std)\n",
    "print(f\"The memory usage of Regularized model :{reg_memory_usage} \")\n",
    "\n",
    "\n",
    "\n",
    "#################################\n",
    "\n",
    "print(100 * \"=\")\n",
    "print(f\"All measurement about regularization process of rate:{reg_rate} \")   \n",
    "Eva_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b10afb",
   "metadata": {},
   "source": [
    "### Recording results in txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5bdc1aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_name = 'Proteins'\n",
    "Pruning_Method='Regularization'\n",
    "max_epoch = 100\n",
    "result_folder ='pathresult/'\n",
    "if not os.path.exists(result_folder):\n",
    "    os.makedirs(result_folder)\n",
    "\n",
    "\n",
    "\n",
    "file_name = result_folder+Pruning_Method+'_'+'with rate of regularization of'+'_'+str(reg_rate)+'_on_'+dataset_name+'_'+str(max_epoch)+'.txt'\n",
    "\n",
    "with open(file_name, 'w') as f:\n",
    "        f.write('%s:%s\\n'%('dataset_name', 'Proteins'))\n",
    "        f.write('%s:%s\\n'%('max_epoch', max_epoch))\n",
    "        f.write('%s:%s\\n'%('sparsity', l2_lambda))\n",
    "        for key, value in Eva_final.items():\n",
    "            f.write('%s:%s\\n'%(key, value))\n",
    "        for key, value in Eva_measure.items():\n",
    "            f.write('%s:%s\\n' % (key, ','.join(map(str, value))))             \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78604857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fadb50b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
