{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978e0ff4",
   "metadata": {},
   "source": [
    "L2-Regularization Method on Link Prediction Task of Cora Dataset\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453ca57b",
   "metadata": {},
   "source": [
    "### All libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf2d48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x20094a04a00>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import os.path as osp\n",
    "from os.path import abspath, dirname\n",
    "import shutil\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import statistics as stat\n",
    "import psutil\n",
    "import itertools\n",
    "import tracemalloc\n",
    "import gc\n",
    "import dill\n",
    "import csv\n",
    "import datatable as dt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Union, Tuple, Optional, Any\n",
    "import statistics as stat\n",
    "import pprint\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "\n",
    "\n",
    "from utils.loss_functions import JointLoss\n",
    "from utils.model_plot import save_auc_plot, save_loss_plot\n",
    "from utils.model_utils import GAEWrapper\n",
    "from utils.utils import set_dirs, set_seed\n",
    "from utils.utils import get_runtime_and_model_config, print_config\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader,IterableDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.seed import seed_everything as th_seed\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_geometric.datasets import Planetoid, WebKB, WikipediaNetwork\n",
    "from torch_geometric.nn import GAE, VGAE, GCNConv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from utils.utils import set_dirs, update_config_with_model_dims\n",
    "from utils.loss_functions import  JointLoss\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dfd229",
   "metadata": {},
   "source": [
    "### Regularization Rate\n",
    "#### Regularization rates range from the following numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b84525f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1e2, 1e3, 1e6\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b2b5ef",
   "metadata": {},
   "source": [
    "### Functions for Measuring criterias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8767a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_num_parameters(model: nn.Module, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the total number of parameters of model\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    num_counted_elements = 0\n",
    "    for name, param in model.autoencoder.gae.encoder.linear1.named_parameters():\n",
    "     \n",
    "    \n",
    "\n",
    "        if count_nonzero_only:\n",
    "            num_counted_elements += param.count_nonzero()\n",
    "        else:\n",
    "            num_counted_elements += param.numel()\n",
    "    return num_counted_elements\n",
    "\n",
    "\n",
    "# Function to get CPU usage\n",
    "def get_cpu_usage():\n",
    "    return psutil.cpu_percent(interval=1)\n",
    "\n",
    "\n",
    "\n",
    "# Function to approximate power consumption (Assume some average power usage per CPU percentage point)\n",
    "def estimate_power_usage(cpu_usage):\n",
    "    base_power_usage = 10  # Assumed base power usage in watts\n",
    "    power_per_percent = 0.5  # Assumed additional watts per CPU usage percent\n",
    "    return base_power_usage + (power_per_percent * cpu_usage)\n",
    "\n",
    "# The model size based on the number of parameters\n",
    "def get_model_size_param(model: nn.Module, data_width=32, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the model size in bits\n",
    "    :param data_width: #bits per element\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    return get_num_parameters(model, count_nonzero_only) * data_width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a422392",
   "metadata": {},
   "source": [
    "### Start loading data\n",
    "\n",
    "#### functions for data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39bb42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this cell there some class and function for loading dataset and preprocessing\n",
    "\n",
    "class GraphLoader:\n",
    "    \"\"\"\n",
    "    Data loader class for graph data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict[str, Any], dataset_name: str, kwargs: Dict[str, Any] = {}) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the GraphLoader.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config : Dict[str, Any]\n",
    "            Dictionary containing options and arguments.\n",
    "        dataset_name : str\n",
    "            Name of the dataset to load.\n",
    "        kwargs : Dict[str, Any], optional\n",
    "            Dictionary for additional parameters if needed, by default {}.\n",
    "        \"\"\"\n",
    "        # Get config\n",
    "        self.config = config\n",
    "        # Set the seed\n",
    "        th_seed(config[\"seed\"])\n",
    "        # Set the paths\n",
    "        paths = config[\"paths\"]\n",
    "        # data > dataset_name\n",
    "        file_path = os.path.join(paths[\"data\"], dataset_name)\n",
    "        # Get the datasets\n",
    "        self.train_data, self.validation_data, self.test_data = self.get_dataset(dataset_name, file_path)        \n",
    "        \n",
    "\n",
    "    def get_dataset(self, dataset_name: str, file_path: str) -> Tuple[Data, Data, Data]:\n",
    "        \"\"\"\n",
    "        Returns the training, validation, and test datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset_name : str\n",
    "            Name of the dataset to load.\n",
    "        file_path : str\n",
    "            Path to the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Data, Data, Data]\n",
    "            Training, validation, and test datasets.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize Graph dataset class\n",
    "        graph_dataset = GraphDataset(self.config, datadir=file_path, dataset_name=dataset_name)\n",
    "        \n",
    "        # Load Training, Validation, Test datasets\n",
    "        train_data, val_data, test_data = graph_dataset._load_data()\n",
    "        \n",
    "        # Generate static subgraphs from training set\n",
    "        train_data = self.generate_subgraphs(train_data)\n",
    "  \n",
    "        # Return\n",
    "        return train_data, val_data, test_data\n",
    "    \n",
    "    \n",
    "    def generate_subgraphs(self, train_data: Data) -> List[Data]:\n",
    "        \"\"\"\n",
    "        Generates subgraphs from the training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_data : Data\n",
    "            Training data containing the graph.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[Data]\n",
    "            List of subgraphs generated from the training data.\n",
    "        \"\"\"\n",
    "        # Initialize list to hold subgraphs\n",
    "        subgraphs = [train_data]\n",
    "\n",
    "        # Check if we are generating subgraphs from the graph. If False, we are in standard GAE mode\n",
    "        if self.config[\"n_subgraphs\"] > 1:\n",
    "                \n",
    "            # Generate subgraphs\n",
    "            for i in range(self.config[\"n_subgraphs\"]):\n",
    "                \n",
    "                # Change random seed\n",
    "                th_seed(i)\n",
    "                \n",
    "                partition = 1.0/(self.config[\"n_subgraphs\"]-i)\n",
    "                \n",
    "                # For the last subgraph, get 95% of the remaining graph. if num_val=1.0, RandomLinkSplit will raise error\n",
    "                if partition == 1.0:\n",
    "                    partition = 0.95\n",
    "                    \n",
    "                random_link_split = T.RandomLinkSplit(num_val=partition, \n",
    "                                                      num_test=0, \n",
    "                                                      is_undirected=True, \n",
    "                                                      split_labels=True, \n",
    "                                                      add_negative_train_samples=False)\n",
    "\n",
    "                # get a subgraph from training data\n",
    "                train_data, train_subgraph, _ = random_link_split(train_data)\n",
    "                \n",
    "                # Make sure that we are using only the nodes within the subgraph by overwriting the edge index \n",
    "                # with positive edge index + positive edge index reversed in direction (to make it undirected)\n",
    "                pos_swapped = train_subgraph.pos_edge_label_index[[1,0],:] \n",
    "                train_subgraph.edge_index = torch.cat((train_subgraph.pos_edge_label_index, pos_swapped), dim=1)\n",
    "                \n",
    "                # Remove negative edge attributes. We want to sample negative samples during training\n",
    "                # Masks are also not needed\n",
    "                if hasattr(train_subgraph, \"neg_edge_label_index\"):\n",
    "                    delattr(train_subgraph, \"neg_edge_label_index\")\n",
    "                    delattr(train_subgraph, \"neg_edge_label\")\n",
    "                    \n",
    "                if hasattr(train_subgraph, \"train_mask\"):\n",
    "                    delattr(train_subgraph, \"train_mask\")\n",
    "                    delattr(train_subgraph, \"val_mask\")\n",
    "                    delattr(train_subgraph, \"test_mask\")\n",
    "\n",
    "\n",
    "                # store the sampled subgraph\n",
    "                subgraphs = [train_subgraph] + subgraphs\n",
    "                       \n",
    "        # Change random seed back to original\n",
    "        th_seed(self.config[\"seed\"])\n",
    "        \n",
    "        # Return all subgraphs and original larger graph\n",
    "        return subgraphs\n",
    "\n",
    "    \n",
    "def get_transform(options):\n",
    "    \"\"\"Splits data to train, validation and test, and moves them to the device\"\"\"\n",
    "    transform = T.Compose([\n",
    "        T.NormalizeFeatures(),\n",
    "        T.ToDevice(options[\"device\"]),\n",
    "        T.RandomLinkSplit(num_val=0.05, \n",
    "                          num_test=0.15, \n",
    "                          is_undirected=True,\n",
    "                          split_labels=True, \n",
    "                          add_negative_train_samples=False),\n",
    "        ])\n",
    "        \n",
    "    return transform\n",
    "\n",
    "\n",
    "class GraphDataset:\n",
    "    \"\"\"\n",
    "    Dataset class for graph data format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict[str, Any], datadir: str, dataset_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the GraphDataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config : Dict[str, Any]\n",
    "            Dictionary containing options and arguments.\n",
    "        datadir : str\n",
    "            The path to the data directory.\n",
    "        dataset_name : str\n",
    "            Name of the dataset to load.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.paths = config[\"paths\"]\n",
    "        self.dataset_name = dataset_name\n",
    "        self.data_path = os.path.join(self.paths[\"data\"], 'Planetoid')\n",
    "        self.transform = get_transform(config)\n",
    "\n",
    "        \n",
    "    def _load_data(self) -> Tuple[Data, Data, Data]:\n",
    "        \"\"\"\n",
    "        Loads one of many available datasets and returns features and labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Data, Data, Data]\n",
    "            Training, validation, and test datasets.\n",
    "        \"\"\"\n",
    "        if self.dataset_name.lower() in ['cora', 'citeseer', 'pubmed']:\n",
    "            # Get the dataset\n",
    "            dataset = Planetoid(self.data_path, self.dataset_name, split=\"random\", transform = self.transform)\n",
    "        elif  self.dataset_name.lower() in ['chameleon']:\n",
    "            # Get the dataset\n",
    "            dataset = WikipediaNetwork(root=self.data_path, name=self.dataset_name, transform = self.transform)\n",
    "        elif  self.dataset_name.lower() in [\"cornell\", \"texas\", \"wisconsin\"]:\n",
    "            # Get the dataset\n",
    "            dataset = WebKB(root=self.data_path, name=self.dataset_name, transform = self.transform) \n",
    "        else:\n",
    "            print(f\"Given dataset name is not found. Check for typos, or missing condition \")\n",
    "            exit()\n",
    "            \n",
    "        # Data splits\n",
    "        train_data, val_data, test_data = dataset[0]\n",
    "        \n",
    "        # Return\n",
    "        return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b8dd62",
   "metadata": {},
   "source": [
    "### Loads arguments and configuration for GNN-based encoder used in NESS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "540aa293",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_arguments():\n",
    "    # Initialize parser\n",
    "    parser = ArgumentParser()\n",
    "    # Dataset can be provided via command line\n",
    "    parser.add_argument(\"-d\", \"--dataset\", type=str, default=\"cora\")\n",
    "    # Encoder type\n",
    "    parser.add_argument(\"-gnn\", \"--gnn\", type=str, default=\"GNAE\")\n",
    "    # Random seed\n",
    "    parser.add_argument(\"-seed\", \"--seed\", type=int, default=57)\n",
    "    # Whether to use contrastive loss\n",
    "    parser.add_argument(\"-cl\", \"--cl\", type=bool, default=False)\n",
    "    # Whether to add noise to input\n",
    "    parser.add_argument(\"-an\", \"--an\", type=bool, default=True)\n",
    "    # Whether to use GPU.\n",
    "    parser.add_argument(\"-g\", \"--gpu\", dest='gpu', action='store_true', \n",
    "                        help='Used to assign GPU as the device, assuming that GPU is available')\n",
    "    \n",
    "    parser.add_argument(\"-ng\", \"--no_gpu\", dest='gpu', action='store_false', \n",
    "                        help='Used to assign CPU as the device')\n",
    "    parser.set_defaults(gpu=True)\n",
    "    \n",
    "    # GPU device number as in \"cuda:0\". Defaul is 0.\n",
    "    parser.add_argument(\"-dn\", \"--device_number\", type=str, default='0', \n",
    "                        help='Defines which GPU to use. It is 0 by default')\n",
    "    \n",
    "    ### Set parameter for L2-Regularization and Pruning ##****************\n",
    "    parser.add_argument(\"-is_reg\", \"--is_reg\", type=bool, default=False)\n",
    "    parser.add_argument(\"-reg\", \"--l2_reg\", type=float, default='0', \n",
    "            help='Defines which GPU to use. It is 0 by default')\n",
    "\n",
    "    parser.add_argument(\"-is_pruned\", \"--is_pruned\", type=bool, default=False)\n",
    "    parser.add_argument(\"-sparsity\", \"--sparsity\", type=float, default='0', \n",
    "            help='Defines which GPU to use. It is 0 by default')\n",
    "            \n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "    # Experiment number\n",
    "    parser.add_argument(\"-ex\", \"--experiment\", type=int, default=1)\n",
    "    # Load model saved at specific epoch\n",
    "    parser.add_argument(\"-m\", \"--model_at_epoch\", type=int, default=None)\n",
    "    \n",
    "    # Return parser arguments along with the unknown ones\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_config(args):\n",
    "    # Load runtime config from config folder: ./config/\n",
    "    config = get_runtime_and_model_config(args)\n",
    "    # Define which device to use: GPU or CPU\n",
    "    config[\"device\"] = torch.device('cuda:'+args.device_number if torch.cuda.is_available() and args.gpu else 'cpu')\n",
    "    # Model at specific epoch\n",
    "    config[\"model_at_epoch\"] = args.model_at_epoch\n",
    "    # Indicate which device is being used\n",
    "    config[\"l2_reg\"] = args.l2_reg\n",
    "\n",
    "    config[\"is_reg\"]= args.is_reg\n",
    "    \n",
    "\n",
    "\n",
    "    print(f\"Device being used is {config['device']}\")\n",
    "    # Return\n",
    "    return config\n",
    "\n",
    "def print_config_summary(config, args=None):\n",
    "    \"\"\"Prints out summary of options and arguments used\"\"\"\n",
    "    # Summarize config on the screen as a sanity check\n",
    "    print(100 * \"=\")\n",
    "    print(f\"Here is the configuration being used:\\n\")\n",
    "    print_config(config)\n",
    "    print(100 * \"=\")\n",
    "    if args is not None:\n",
    "        print(f\"Arguments being used:\\n\")\n",
    "        print_config(args)\n",
    "        print(100 * \"=\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc8f0e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used is cpu\n"
     ]
    }
   ],
   "source": [
    "# Get parser / command line arguments\n",
    "args = get_arguments()\n",
    "# Get configuration file\n",
    "config = get_config(args)\n",
    "\n",
    "\n",
    "# By default, we are using the name of the dataset. This can be customized.\n",
    "config[\"experiment\"] = config[\"dataset\"]\n",
    "\n",
    "# File name to use when saving results as csv. This can be customized\n",
    "config[\"file_name\"] = config[\"experiment\"] + \"_sub\" + str(config[\"n_subgraphs\"]) + '_seed' + str(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b334b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Here is the configuration being used:\n",
      "\n",
      "+-------------------+----------------------------------------------+\n",
      "|     Parameter     |                    Value                     |\n",
      "+===================+==============================================+\n",
      "| Add noise         | True                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Aggregation       | mean                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Batch size        | 128                                          |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Contrastive loss  | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Cosine similarity | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Dataset           | cora                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Device            | cpu                                          |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Dropout rate      | 0.500                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Encoder type      | GNAE                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Epochs            | 500                                          |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Experiment        | cora                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| File name         | cora_sub4_seed57                             |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Full graph        | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Isbatchnorm       | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Isdropout         | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Is reg            | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| L2 reg            | 0                                            |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Learning rate     | 0.010                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Model at epoch    | None                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| N subgraphs       | 4                                            |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Normalize         | True                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Nth epoch         | 10                                           |\n",
      "+-------------------+----------------------------------------------+\n",
      "| P noise           | 0.200                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| P norm            | 2                                            |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Paths             | {'data': './data/', 'results': './results/'} |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Patience          | 3                                            |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Scheduler         | False                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Seed              | 57                                           |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Tau               | 0.100                                        |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Validate          | True                                         |\n",
      "+-------------------+----------------------------------------------+\n",
      "| Z dim             | 32                                           |\n",
      "+-------------------+----------------------------------------------+\n",
      "====================================================================================================\n",
      "Arguments being used:\n",
      "\n",
      "+----------------+-------+\n",
      "|   Parameter    | Value |\n",
      "+================+=======+\n",
      "| An             | True  |\n",
      "+----------------+-------+\n",
      "| Cl             | False |\n",
      "+----------------+-------+\n",
      "| Dataset        | cora  |\n",
      "+----------------+-------+\n",
      "| Device number  | 0     |\n",
      "+----------------+-------+\n",
      "| Experiment     | 1     |\n",
      "+----------------+-------+\n",
      "| Gnn            | GNAE  |\n",
      "+----------------+-------+\n",
      "| Gpu            | True  |\n",
      "+----------------+-------+\n",
      "| Is pruned      | False |\n",
      "+----------------+-------+\n",
      "| Is reg         | False |\n",
      "+----------------+-------+\n",
      "| L2 reg         | 0     |\n",
      "+----------------+-------+\n",
      "| Model at epoch | None  |\n",
      "+----------------+-------+\n",
      "| Seed           | 57    |\n",
      "+----------------+-------+\n",
      "| Sparsity       | 0     |\n",
      "+----------------+-------+\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Summarize config and arguments on the screen as a sanity check\n",
    "print_config_summary(config, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc5a52d",
   "metadata": {},
   "source": [
    "### NESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e4aea",
   "metadata": {},
   "source": [
    "#### NESS class, the framework used to learn node embeddings from static subgraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbacfeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### It is not realll\n",
    "import csv\n",
    "import gc\n",
    "import itertools\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.seed import seed_everything as th_seed\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Union, Tuple, Optional\n",
    "\n",
    "from utils.loss_functions import JointLoss\n",
    "from utils.model_plot import save_auc_plot, save_loss_plot\n",
    "from utils.model_utils import GAEWrapper\n",
    "from utils.utils import set_dirs, set_seed\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "class NESS:\n",
    "    \"\"\"\n",
    "    Model: Trains a Graph Autoencoder with a Projection network, using NESS framework.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict):\n",
    "        \"\"\"Initializes the NESS class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config : dict\n",
    "            Configuration dictionary with parameters for training a Graph Autoencoder \n",
    "            using NESS framework.\n",
    "        \"\"\"\n",
    "        # Get config\n",
    "        self.config = config\n",
    "        # Set L2-Regularization Parameters******************\n",
    "        self.is_reg=config[\"is_reg\"]\n",
    "        self.l2_reg=config[\"l2_reg\"]\n",
    "    \n",
    "        if self.l2_reg!= 0:\n",
    "            self.is_reg=True\n",
    "         \n",
    "        print(f\" Is the approach is regularization:{self.is_reg}\")\n",
    "        print(f\"Regrularization rate is:{self.l2_reg}\")\n",
    "        \n",
    "        \n",
    "        # Define which device to use: GPU, or CPU\n",
    "        self.device = config[\"device\"]\n",
    "        # Create empty lists and dictionary\n",
    "        self.model_dict, self.summary = {}, {}\n",
    "        # Set random seed\n",
    "        set_seed(self.config)\n",
    "        # Set paths for results and initialize some arrays to collect data during training\n",
    "        self._set_paths()\n",
    "        # Set directories i.e. create ones that are missing.\n",
    "        set_dirs(self.config)\n",
    "        # ------Network---------\n",
    "        # Instantiate networks\n",
    "        print(\"Building the models for training and evaluation in NESS framework...\")\n",
    "        # Set Autoencoders i.e. setting loss, optimizer, and device assignment (GPU, or CPU)\n",
    "        self.set_autoencoder()\n",
    "        # Set scheduler (its use is optional)\n",
    "        self._set_scheduler()\n",
    "        # Print out model architecture\n",
    "        self.print_model_summary()\n",
    "        \n",
    "\n",
    "        \n",
    "    def set_autoencoder(self) -> None:\n",
    "        \"\"\"Sets up the autoencoder model, optimizer, and loss.\n",
    "        \n",
    "        This function is responsible for initializing the Graph Autoencoder, setting up \n",
    "        the optimizer, and defining the joint loss.\n",
    "        \"\"\"   \n",
    "        # Instantiate the model for the text Autoencoder\n",
    "        self.autoencoder = GAEWrapper(self.config)\n",
    "        # Add the model and its name to a list to save, and load in the future\n",
    "        self.model_dict.update({\"autoencoder\": self.autoencoder})\n",
    "        # Assign autoencoder to a device\n",
    "        for _, model in self.model_dict.items():\n",
    "            model.to(self.device)\n",
    "        # Get model parameters\n",
    "        parameters = [model.parameters() for _, model in self.model_dict.items()]\n",
    "        # Joint loss including contrastive, reconstruction and distance losses\n",
    "        \n",
    "        #for param in   parameters:\n",
    "            #l2_reg += torch.norm(param)\n",
    "    \n",
    "        # Combine the loss function with L2 regularization\n",
    "        #loss += (l2 * l2_reg)\n",
    "    \n",
    "        \n",
    "        self.joint_loss = None if self.config[\"dataset\"][:4] == \"ogbl\" else JointLoss(self.config)\n",
    "        # Set optimizer for autoencoder\n",
    "        \n",
    "        # Set optimizer for autoencoder with L2 regularization\n",
    "        self.optimizer_ae = self._adam(parameters, lr=self.config[\"learning_rate\"], weight_decay=0)\n",
    "        ##  self.optimizer_ae = self._adam(parameters, lr=self.config[\"learning_rate\"])\n",
    "        \n",
    "        # Add items to summary to be used for reporting later\n",
    "        self.summary.update({\"recon_loss\": []})\n",
    "\n",
    "    def set_parallelism(self, model) -> None:\n",
    "        \"\"\"Sets up parallelism in training if multiple GPUs are available.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : torch.nn.Module\n",
    "            The model for which the parallelism is to be set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model : torch.nn.Module\n",
    "            The input model wrapped in DataParallel if multiple GPUs are available.\n",
    "        \"\"\"\n",
    "        # If we are using GPU, and if there are multiple GPUs, parallelize training\n",
    "        if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "            print(torch.cuda.device_count(), \" GPUs will be used!\")\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        return model\n",
    "\n",
    "    def fit(self, data_loader: DataLoader) -> None:\n",
    "        \"\"\"Fits model to the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_loader : DataLoader\n",
    "            The DataLoader object that provides the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get data loaders\n",
    "        train_data = data_loader.train_data\n",
    "        validation_data = data_loader.validation_data\n",
    "        test_data = data_loader.test_data\n",
    "\n",
    "        # Placeholders to record losses per batch\n",
    "        self.metrics = {\"tloss_e\": [], \"vloss_e\": [], \"rloss_e\": [], \"zloss_e\": [], \"val_auc\": [], \"tr_auc\": []}\n",
    "        self.val_auc = \"NA\"\n",
    "        self.tr_auc = \"NA\"\n",
    "\n",
    "        # Turn on training mode for the model.\n",
    "        self.set_mode(mode=\"training\")\n",
    "\n",
    "        # Reset best test auc\n",
    "        self.best_val_auc = 0\n",
    "        self.best_epoch = 0\n",
    "        self.patient = 0\n",
    "\n",
    "        \n",
    "        # Start joint training of Autoencoder with Projection network\n",
    "        for epoch in range(1,config['epochs']):\n",
    "            \n",
    "            # Keep a record of epoch\n",
    "            self.epoch = epoch\n",
    "            \n",
    "            # 0 - Update Autoencoder\n",
    "            self.update_autoencoder(train_data)\n",
    "            \n",
    "            # 1 - Update log message using epoch and batch numbers\n",
    "           \n",
    "            if epoch % 40 == 0:\n",
    "                   self.update_log(epoch)\n",
    "            \n",
    "            # 2 - Clean-up for efficient memory usage.\n",
    "            gc.collect()                \n",
    "                \n",
    "            # 3 - Run Validation\n",
    "            self.run_validation(train_data, validation_data)\n",
    "\n",
    "            # 4 - Change learning rate if scheduler==True\n",
    "            _ = self.scheduler.step() if self.config[\"scheduler\"] else None\n",
    "            \n",
    "            # 5 - Stop training if we run out of patience\n",
    "            if self.patient == self.config[\"patience\"]:\n",
    "                break\n",
    "            \n",
    "        # Get the test performance and computing inference time \n",
    "       \n",
    "        start = time.time()\n",
    "        self.test_auc, self.test_ap = self.autoencoder.single_test(train_data, test_data)\n",
    "        end = time.time()\n",
    "        self.t_inference= end-start\n",
    "\n",
    "        # Save plots of training loss and validation auc\n",
    "        save_loss_plot(self.metrics, self._plots_path)\n",
    "        save_auc_plot(self.metrics, self._plots_path)\n",
    "        \n",
    "        # Convert loss dictionary to a dataframe\n",
    "        loss_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in self.metrics.items()]))\n",
    "        \n",
    "        # Save loss dataframe as csv file for later use\n",
    "        loss_df.to_csv(self._loss_path + \"/losses.csv\")\n",
    "        \n",
    "            \n",
    "    def run_validation(self, train_data: Union[List, torch.Tensor], validation_data: Union[List, torch.Tensor]) -> None:\n",
    "        \"\"\"Runs validation on the trained model and save weights if validation AUC improves.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_data : List or torch.Tensor\n",
    "            The training dataset.\n",
    "\n",
    "        validation_data : List or torch.Tensor\n",
    "            The validation dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set the evaluation mode\n",
    "        self.set_mode(mode=\"evaluation\")\n",
    "        \n",
    "        # Validate every nth epoch. n=1 by default, but it can be changed in the config file\n",
    "        if self.config[\"validate\"]:\n",
    "\n",
    "            # Compute validation AUCs\n",
    "            self.val_auc, _ = self.autoencoder.single_test(train_data, validation_data)\n",
    "                \n",
    "            # Append auc's to the list to use for plots\n",
    "            self.metrics[\"val_auc\"].append(self.val_auc)\n",
    "                \n",
    "        # Save intermediate model on regular intervals\n",
    "        if self.epoch >=self.config[\"nth_epoch\"] and self.epoch % self.config[\"nth_epoch\"] == 0:\n",
    "            \n",
    "            # Check the test auc at this epoch\n",
    "            self.config[\"model_at_epoch\"] = self.epoch\n",
    "            val_auc, _ = self.autoencoder.single_test(train_data, validation_data)\n",
    "\n",
    "            # Update the metrics.\n",
    "            if val_auc > self.best_val_auc:\n",
    "                self.best_val_auc = val_auc\n",
    "                self.best_epoch = self.epoch \n",
    "                self.save_weights()\n",
    "                self.patient = 0\n",
    "            else:\n",
    "                self.patient += 1\n",
    "                    \n",
    "        # Set training mode back\n",
    "        self.set_mode(mode=\"training\")\n",
    "    \n",
    "\n",
    "    def update_autoencoder(self, subgraphs: List[Data]) -> None:\n",
    "        \"\"\"Updates autoencoder model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        subgraphs : list of Data\n",
    "            A list that contains subgraphs + original training graph.\n",
    "        \"\"\"\n",
    "        total_loss, contrastive_loss, recon_loss, zrecon_loss = [], [], [], []\n",
    "\n",
    "        # Last element of the list is the original whole training graph\n",
    "        graph = subgraphs[-1]\n",
    "\n",
    "        # If len(subgraphs) > 1, it means that we sampled subgraphs from the graph. Else, we have a standard GAE\n",
    "        subgraphs = subgraphs[:-1] if len(subgraphs) > 1 else subgraphs\n",
    "\n",
    "        # A list to hold list of latents --- will be used to compute contrastive loss\n",
    "        z_list = []\n",
    "\n",
    "        # Initialize total loss\n",
    "        tloss = None\n",
    "\n",
    "        # pass subgraphs through model to reconstruct the original graph from subgraphs\n",
    "        for sg in subgraphs:\n",
    "\n",
    "            # Reference graph\n",
    "            ref_graph = graph if self.config[\"full_graph\"] else sg\n",
    "\n",
    "            # Drop edges if True\n",
    "            if self.config[\"add_noise\"]:\n",
    "                sg.edge_index, sg.edge_attr = dropout_adj(sg.edge_index, edge_attr= sg.edge_attr, p=self.config[\"p_noise\"])\n",
    "\n",
    "            # Forwards pass\n",
    "            z, latent = self.autoencoder(sg.x, sg.edge_index)           \n",
    "\n",
    "            # Reconstruction loss by using GAE's native function\n",
    "            rloss = self.autoencoder.gae.recon_loss(latent, ref_graph.pos_edge_label_index)\n",
    "\n",
    "            # If the model is a variational model\n",
    "            if self.autoencoder.variational:\n",
    "                rloss = rloss + (1 / ref_graph.num_nodes) * self.autoencoder.gae.kl_loss()\n",
    "\n",
    "            # Store z to the list\n",
    "            z_list.append(z)\n",
    "\n",
    "            # total loss\n",
    "            tloss = tloss + rloss if tloss  is not None else rloss\n",
    "\n",
    "            # Accumulate losses\n",
    "            total_loss.append(tloss)\n",
    "            recon_loss.append(rloss)\n",
    "\n",
    "        # Compute the losses\n",
    "        n = len(total_loss)\n",
    "        total_loss = sum(total_loss) / n\n",
    "        recon_loss = sum(recon_loss) / n\n",
    "\n",
    "        # If the graph is large such as pubmed, push the losses to cpu.\n",
    "        if self.config[\"dataset\"] == \"pubmed\":\n",
    "            total_loss = total_loss.cpu()\n",
    "            recon_loss = recon_loss.cpu()\n",
    "\n",
    "        # Initialize contrastive loss\n",
    "        closs = None\n",
    "        zloss = None\n",
    "\n",
    "        if self.config[\"contrastive_loss\"] and len(subgraphs)>1:\n",
    "\n",
    "            # Generate combinations of z's to compute contrastive loss\n",
    "            z_combinations = self.get_combinations_of_subgraphs(z_list)\n",
    "\n",
    "            # Compute the contrastive loss for each pair of latent vectors\n",
    "            for z in z_combinations:\n",
    "                # Contrastive loss\n",
    "                zloss = self.joint_loss(z)\n",
    "\n",
    "                # Total contrastive loss\n",
    "                closs = closs + zloss if closs is not None else zloss\n",
    "\n",
    "            # Mean constrative loss\n",
    "            closs = closs/len(z_combinations)\n",
    "\n",
    "        # Update total loss\n",
    "        total_loss = total_loss + closs if closs is not None else total_loss\n",
    "\n",
    "        \n",
    "        if self.is_reg :\n",
    "            l2_reg_sum = torch.tensor(0., requires_grad=True).to(self.device)\n",
    "            for param in self.autoencoder.parameters():\n",
    "                l2_reg_sum = l2_reg_sum + torch.norm(param, 2)**2 \n",
    "            total_loss = total_loss + self.config[\"l2_reg\"] * l2_reg_sum\n",
    "        # Add L2 regularization without in-place operation\n",
    "        \n",
    "\n",
    "        # Record losses\n",
    "        self.metrics[\"tloss_e\"].append(total_loss.item())\n",
    "        self.metrics[\"rloss_e\"].append(recon_loss.item())\n",
    "        self.metrics[\"zloss_e\"].append(closs.item() if closs is not None else 0)\n",
    "\n",
    "        # Update Autoencoder params\n",
    "        self._update_model(total_loss, self.optimizer_ae, retain_graph=True)\n",
    "\n",
    "        # Delete loss and associated graph for efficient memory usage\n",
    "        del total_loss, recon_loss, closs, zloss\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    def get_combinations_of_subgraphs(self, z_list: List[Data]) -> List[Tuple[Data, Data]]:\n",
    "        \"\"\"Generates a list of combinations of subgraphs from the list of subgraphs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z_list : list of Data\n",
    "            List of subgraphs e.g. [z1, z2, z3, ...]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of tuple\n",
    "            A list of combinations of subgraphs e.g. [(z1, z2), (z1, z3), ...]\n",
    "        \"\"\"                            \n",
    "        # Compute combinations of subgraphs [(z1, z2), (z1, z3)...]\n",
    "        subgraph_combinations = list(itertools.combinations(z_list, 2))\n",
    "        # List to store the concatenated subgraphs\n",
    "        concatenated_subgraphs_list = []\n",
    "        \n",
    "        # Go through combinations\n",
    "        for (zi, zj) in subgraph_combinations:\n",
    "            # Concatenate xi, and xj, and turn it into a tensor\n",
    "            z = torch.cat((zi, zj), dim=0)\n",
    "            \n",
    "            # Add it to the list\n",
    "            concatenated_subgraphs_list.append(z)\n",
    "        \n",
    "        # Return the list of combination of subgraphs\n",
    "        return concatenated_subgraphs_list\n",
    "    \n",
    "    def clean_up_memory(self, losses: List) -> None:\n",
    "        \"\"\"Deletes losses with attached graph, and cleans up memory.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        losses : list\n",
    "            List of loss values to be deleted.\n",
    "        \"\"\"\n",
    "        for loss in losses: del loss\n",
    "        gc.collect()\n",
    "\n",
    "    def update_log(self, epoch: int) -> None:\n",
    "        \"\"\"Updates the messages displayed during training and evaluation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch : int\n",
    "            The current epoch number.\n",
    "        \"\"\"\n",
    "        # For the first epoch, add losses for batches since we still don't have loss for the epoch\n",
    "        if epoch < 1:\n",
    "            description = f\"Epoch:[{epoch - 1}], Total loss:{self.metrics['tloss_e'][-1]:.4f}\"\n",
    "            description += f\", X recon loss:{self.metrics['rloss_e'][-1]:.4f}\"\n",
    "            if self.config[\"contrastive_loss\"]:\n",
    "                description += f\", contrastive loss:{self.metrics['zloss_e'][-1]:.6f}\"\n",
    "            description += f\", val auc:{self.val_auc}\"\n",
    "\n",
    "        # For sub-sequent epochs, display only epoch losses.\n",
    "        else:\n",
    "            description = f\"Epoch:[{epoch - 1}] training loss:{self.metrics['tloss_e'][-1]:.4f}\"\n",
    "            description += f\", X recon loss:{self.metrics['rloss_e'][-1]:.4f}\"\n",
    "            if self.config[\"contrastive_loss\"]:\n",
    "                description += f\", contrastive loss:{self.metrics['zloss_e'][-1]:.6f}\"\n",
    "            # Add validation auc\n",
    "            description += f\", val auc:{self.val_auc}\"\n",
    "\n",
    "        # Update the displayed message\n",
    "        print(description)\n",
    "\n",
    "    def set_mode(self, mode: str = \"training\") -> None:\n",
    "        \"\"\"Sets the mode of the models, either as .train(), or .eval().\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mode : str, optional\n",
    "            Mode in which to set the models, by default \"training\".\n",
    "        \"\"\"\n",
    "        for _, model in self.model_dict.items():\n",
    "            model.train() if mode == \"training\" else model.eval()\n",
    "\n",
    "    \n",
    "    \n",
    "    def save_weights(self,is_pruned:bool=False, with_epoch: bool = False) -> None:\n",
    "        \"\"\"Saves weights of the models.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        with_epoch : bool, optional\n",
    "            If True, includes the epoch number in the filename, by default False.\n",
    "        \"\"\"\n",
    "        for model_name in self.model_dict:\n",
    "            \n",
    "            # Check if we want to save the model at a specific epoch\n",
    "            file_name = model_name + \"_\" + str(self.epoch) if with_epoch else model_name\n",
    "            \n",
    "            # Save the model\n",
    "            if self.is_reg==False:\n",
    "            \n",
    "                dill.dump(self.model_dict[model_name], open(self._model_path + \"/\" + file_name + \".pt\", 'wb'))\n",
    "            if self.is_reg==True:  \n",
    "                dill.dump(self.model_dict[model_name], open(self._model_path + \"/\" + file_name + \"reg.pt\", 'wb'))\n",
    "        \n",
    "        print(\"Done with saving models.\")\n",
    "\n",
    "   \n",
    "\n",
    "    def load_models(self, epoch: Optional[int] = None) -> None:\n",
    "        \"\"\"Loads weights saved at the end of the training.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch : int, optional\n",
    "            If provided, loads the weights saved at the specified epoch, by default None.\n",
    "        \"\"\"\n",
    "        for model_name in self.model_dict:\n",
    "            \n",
    "            # Check if we want to load the model saved at a specific epoch\n",
    "            file_name = model_name + \"_\" + str(epoch) if epoch is not None else model_name\n",
    "\n",
    "            # Load the model\n",
    "            if self.is_reg==False:\n",
    "                model = dill.load(open(self._model_path + \"/\" + file_name + \".pt\", 'rb'))\n",
    "            if self.is_reg==True:\n",
    "                \n",
    "                model = dill.load(open(self._model_path + \"/\" + file_name + \"reg.pt\"), 'rb')\n",
    "            # Register model to the class\n",
    "            setattr(self, model_name, model.eval())\n",
    "            print(f\"--{model_name} is loaded\")\n",
    "        \n",
    "        print(\"Done with loading models.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    def print_model_summary(self) -> None:\n",
    "        \"\"\"Displays model architectures as a sanity check to see if the models are constructed correctly.\"\"\"\n",
    "        # Summary of the model\n",
    "        description = f\"{40 * '-'}Summary of the models:{40 * '-'}\\n\"\n",
    "        description += f\"{34 * '='} NESS Architecture {34 * '='}\\n\"\n",
    "        description += f\"{self.autoencoder}\\n\"\n",
    "        # Print model architecture\n",
    "        print(description)\n",
    "\n",
    "    def _update_model(self, loss: torch.Tensor, optimizer: torch.optim.Optimizer, retain_graph: bool = True) -> None:\n",
    "        \"\"\"Does backpropagation and updates the model parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        loss : torch.Tensor\n",
    "            Loss containing computational graph.\n",
    "\n",
    "        optimizer : torch.optim.Optimizer\n",
    "            Optimizer used during training.\n",
    "\n",
    "        retain_graph : bool, optional\n",
    "            If True, retains the computational graph after backpropagation, by default True.\n",
    "        \"\"\"\n",
    "        # Reset optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # Backward propagation to compute gradients\n",
    "        loss.backward(retain_graph=retain_graph)\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    def _set_scheduler(self) -> None:\n",
    "        \"\"\"Sets a scheduler for the learning rate of the autoencoder.\"\"\"\n",
    "        # Set scheduler (Its use will be optional)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer_ae, step_size=1, gamma=0.99)\n",
    "\n",
    "    def _set_paths(self) -> None:\n",
    "        \"\"\"Sets paths to be used for saving results at the end of the training.\"\"\"\n",
    "        # Top results directory\n",
    "        self._results_path = os.path.join(self.config[\"paths\"][\"results\"], self.config[\"experiment\"])\n",
    "        # Directory to save model\n",
    "        self._model_path = os.path.join(self._results_path, \"training\", \"model\")\n",
    "        # Directory to save plots as png files\n",
    "        self._plots_path = os.path.join(self._results_path, \"training\", \"plots\")\n",
    "        # Directory to save losses as csv file\n",
    "        self._loss_path = os.path.join(self._results_path, \"training\", \"loss\")\n",
    "\n",
    "    def _adam(self, params: Union[List, Tuple], lr: float = 1e-4,weight_decay: float = 0) -> torch.optim.AdamW:\n",
    "        \"\"\"Sets up AdamW optimizer using model parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : list or tuple\n",
    "            Parameters of the models to optimize.\n",
    "\n",
    "        lr : float, optional\n",
    "            Learning rate, by default 1e-4.\n",
    "\n",
    "        weight_decay: float, optional\n",
    "            L2 regularization coefficient, by default 1e-5.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.optim.AdamW\n",
    "            AdamW optimizer.\n",
    "         \"\"\"\n",
    "        return torch.optim.AdamW(itertools.chain(*params), lr=lr, betas=(0.9, 0.999), eps=1e-07, weight_decay= 0)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf1adba",
   "metadata": {},
   "source": [
    "### Function for training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f86388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need these two function for training dataset\n",
    "\n",
    "def train(config: Dict, data_loader: IterableDataset, save_weights: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Trains the model using provided configuration and data loader.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict\n",
    "        Dictionary containing options.\n",
    "\n",
    "    data_loader : IterableDataset\n",
    "        Pytorch data loader used for training the model.\n",
    "\n",
    "    save_weights : bool, optional\n",
    "        If True, the trained model is saved. By default, it's True.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate model\n",
    "    model = NESS(config)\n",
    "    # Start the clock to measure the training time\n",
    "    start = time.process_time()\n",
    "    # Fit the model to the data\n",
    "    model.fit(data_loader)\n",
    "    # Total time spent on training\n",
    "    training_time = time.process_time() - start\n",
    "    # Report the training time\n",
    "    print(\"Done with training...\")\n",
    "    print(f\"Training time:  {training_time//60} minutes, {training_time%60} seconds\")\n",
    "    \n",
    "\n",
    "    \n",
    "    # Return the best Test set AUC\n",
    "    return model.test_auc, model.test_ap, model.t_inference, model.val_auc, model\n",
    "\n",
    "\n",
    "def main(config: Dict) -> None:\n",
    "    \"\"\"\n",
    "    The main function that starts the execution of the program. Takes the \n",
    "    configuration dictionary as input.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict\n",
    "        Dictionary containing options.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Ser directories (or create if they don't exist)\n",
    "    set_dirs(config)\n",
    "    # Get data loader for first dataset.\n",
    "    ds_loader = GraphLoader(config, dataset_name=config[\"dataset\"])\n",
    "    # Add the number of features in a dataset as the first dimension of the model\n",
    "    config = update_config_with_model_dims(ds_loader, config)\n",
    "    # Start training and save model weights at the end\n",
    "    test_auc, test_ap,t_inference,val_auc, model= train(config, ds_loader, save_weights=True)\n",
    "    # Return best test auc\n",
    "    return test_auc, test_ap, t_inference, val_auc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181450fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0224fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01c2262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea04777c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22918223",
   "metadata": {},
   "source": [
    "##  Measurement "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe1a0a",
   "metadata": {},
   "source": [
    "#### Setting Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a234c910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used is cpu\n"
     ]
    }
   ],
   "source": [
    "# Get parser / command line arguments\n",
    "args = get_arguments()\n",
    "# Get configuration file\n",
    "config = get_config(args)\n",
    "\n",
    "\n",
    "# By default, we are using the name of the dataset. This can be customized.\n",
    "config[\"experiment\"] = config[\"dataset\"]\n",
    "\n",
    "# File name to use when saving results as csv. This can be customized\n",
    "config[\"file_name\"] = config[\"experiment\"] + \"_sub\" + str(config[\"n_subgraphs\"]) + '_seed' + str(config[\"seed\"])\n",
    "\n",
    "# The model path\n",
    "results_path = os.path.join(config[\"paths\"][\"results\"], config[\"experiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75dbd56",
   "metadata": {},
   "source": [
    "#### Setting Regularization rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf12af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Regularization Rate\n",
    "l2_lambda = 10000\n",
    "\n",
    "\n",
    "#The number of epochs \n",
    "config[\"epochs\"]=100\n",
    "# The number of iteration\n",
    "num_iterations=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12da9b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is a dictionary to save all measurements. Aftre measuring, we can compute mean and std of each item.\n",
    "Eva_final=dict()\n",
    "\n",
    "# The following are all list of criteria for measurements. \n",
    "# We collect all desired datas of each list across iterations. \n",
    "# Then, we compute average and std of each list.\n",
    "\n",
    "#Base model\n",
    "Base_model_accuracy=[]\n",
    "T_base_model=[]\n",
    "Num_parm_base_model=[]\n",
    "Base_model_size=[]\n",
    "Base_Energy_Consumption=[]\n",
    "Base_Cpu_Usage=[]\n",
    "Base_Memory_Usage=[]\n",
    "\n",
    "#regularized model\n",
    "Reg_model_accuracy=[]\n",
    "T_Reg_model=[]\n",
    "Num_parm_Reg_model=[]\n",
    "Reg_model_size=[]\n",
    "Reg_Energy_Consumption=[]\n",
    "Reg_Cpu_Usage=[]\n",
    "Reg_Memory_Usage=[]\n",
    "\n",
    "# Here is the dictionary to record the list of all measurements\n",
    "Eva_measure={'base model accuracy':Base_model_accuracy,\n",
    "            'time inference of base model':T_base_model,\n",
    "            'number parmameters of base model':Num_parm_base_model,\n",
    "            'base model size':Base_model_size,\n",
    "            'energy consumption of base model':Base_Energy_Consumption,\n",
    "            'cpu usage of base model':Base_Cpu_Usage,\n",
    "            'memory usage of base model':Base_Memory_Usage,\n",
    "            'regularized model accuracy': Reg_model_accuracy,\n",
    "            'time inference of regularized model':T_Reg_model,\n",
    "            'number parmameters of regularized model':Num_parm_Reg_model,\n",
    "            'regularized model size':Reg_model_size,\n",
    "            'energy consumption of regularized model':Reg_Energy_Consumption,\n",
    "            'cpu usage of regularized model':Reg_Cpu_Usage,\n",
    "            'memory usage of regularized model':Reg_Memory_Usage\n",
    "            }\n",
    "\n",
    "             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5f0198",
   "metadata": {},
   "source": [
    "### Trainig and regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1cb275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________\n",
      "************************************************\n",
      "This is iteration :1\n",
      "Training and evaluation before regularization \n",
      "Starting training...\n",
      "Directories are set.\n",
      " Is the approach is regularization:False\n",
      "Regrularization rate is:0\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3815, X recon loss:0.9655, val auc:0.8971360002313173\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2165, X recon loss:0.8979, val auc:0.9167257008197313\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  1.0 minutes, 55.09375 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=193562.00 bytes\n",
      "The time inference of base model is =2.1206897999509238\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 51.427\n",
      "total memory usage of base model':20417 \n",
      "cpu usage of base model':29.200 %\n",
      "________*******************************_____________\n",
      "Regularized Model\n",
      "Directories are set.\n",
      " Is the approach is regularization:True\n",
      "Regrularization rate is:10000\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:607.7520, X recon loss:2.9259, val auc:0.6393977070653039\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  1.0 minutes, 19.90625 seconds\n",
      "****************Result of regularized model ******************\n",
      "10000 regularized model has accuracy on test set=0.64%\n",
      "10000 regularized model has size=187980.00 bytes\n",
      "The time inference of 10000 regularized model is =2.0917868000105955\n",
      "The number of parametrs of 10000 regularized model is:45888\n",
      "Energy Consumption of 10000 regularized model: 20.918\n",
      "total memory usage of 10000 regularized model':15945 \n",
      "cpu usage of 10000 regularized model':13.400 %\n",
      "________________________________________________\n",
      "************************************************\n",
      "This is iteration :2\n",
      "Training and evaluation before regularization \n",
      "Starting training...\n",
      "Directories are set.\n",
      " Is the approach is regularization:False\n",
      "Regrularization rate is:0\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3815, X recon loss:0.9655, val auc:0.8971360002313173\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2165, X recon loss:0.8979, val auc:0.9167257008197313\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  1.0 minutes, 40.890625 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=187979.00 bytes\n",
      "The time inference of base model is =2.1077358999755234\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 23.923\n",
      "total memory usage of base model':15972 \n",
      "cpu usage of base model':15.300 %\n",
      "________*******************************_____________\n",
      "Regularized Model\n",
      "Directories are set.\n",
      " Is the approach is regularization:True\n",
      "Regrularization rate is:10000\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:607.7520, X recon loss:2.9259, val auc:0.6393977070653039\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  1.0 minutes, 2.296875 seconds\n",
      "****************Result of regularized model ******************\n",
      "10000 regularized model has accuracy on test set=0.64%\n",
      "10000 regularized model has size=187980.00 bytes\n",
      "The time inference of 10000 regularized model is =2.1068930000183173\n",
      "The number of parametrs of 10000 regularized model is:45888\n",
      "Energy Consumption of 10000 regularized model: 79.219\n",
      "total memory usage of 10000 regularized model':16076 \n",
      "cpu usage of 10000 regularized model':15.900 %\n",
      "________________________________________________\n",
      "************************************************\n",
      "This is iteration :3\n",
      "Training and evaluation before regularization \n",
      "Starting training...\n",
      "Directories are set.\n",
      " Is the approach is regularization:False\n",
      "Regrularization rate is:0\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3815, X recon loss:0.9655, val auc:0.8971360002313173\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2165, X recon loss:0.8979, val auc:0.9167257008197313\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  1.0 minutes, 30.125 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=187979.00 bytes\n",
      "The time inference of base model is =2.0770134999766015\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 21.186\n",
      "total memory usage of base model':16076 \n",
      "cpu usage of base model':0.400 %\n",
      "________*******************************_____________\n",
      "Regularized Model\n",
      "Directories are set.\n",
      " Is the approach is regularization:True\n",
      "Regrularization rate is:10000\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:607.7520, X recon loss:2.9259, val auc:0.6393977070653039\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  1.0 minutes, 3.203125 seconds\n",
      "****************Result of regularized model ******************\n",
      "10000 regularized model has accuracy on test set=0.64%\n",
      "10000 regularized model has size=187980.00 bytes\n",
      "The time inference of 10000 regularized model is =2.0845793999615125\n",
      "The number of parametrs of 10000 regularized model is:45888\n",
      "Energy Consumption of 10000 regularized model: 25.223\n",
      "total memory usage of 10000 regularized model':16135 \n",
      "cpu usage of 10000 regularized model':3.900 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________\n",
      "************************************************\n",
      "This is iteration :4\n",
      "Training and evaluation before regularization \n",
      "Starting training...\n",
      "Directories are set.\n",
      " Is the approach is regularization:False\n",
      "Regrularization rate is:0\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3815, X recon loss:0.9655, val auc:0.8971360002313173\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2165, X recon loss:0.8979, val auc:0.9167257008197313\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  1.0 minutes, 28.328125 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=187979.00 bytes\n",
      "The time inference of base model is =2.0769875000114553\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 20.770\n",
      "total memory usage of base model':16135 \n",
      "cpu usage of base model':0.800 %\n",
      "________*******************************_____________\n",
      "Regularized Model\n",
      "Directories are set.\n",
      " Is the approach is regularization:True\n",
      "Regrularization rate is:10000\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:607.7520, X recon loss:2.9259, val auc:0.6393977070653039\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  1.0 minutes, 3.984375 seconds\n",
      "****************Result of regularized model ******************\n",
      "10000 regularized model has accuracy on test set=0.64%\n",
      "10000 regularized model has size=187980.00 bytes\n",
      "The time inference of 10000 regularized model is =2.0921128999907523\n",
      "The number of parametrs of 10000 regularized model is:45888\n",
      "Energy Consumption of 10000 regularized model: 20.921\n",
      "total memory usage of 10000 regularized model':16024 \n",
      "cpu usage of 10000 regularized model':1.500 %\n",
      "________________________________________________\n",
      "************************************************\n",
      "This is iteration :5\n",
      "Training and evaluation before regularization \n",
      "Starting training...\n",
      "Directories are set.\n",
      " Is the approach is regularization:False\n",
      "Regrularization rate is:0\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3815, X recon loss:0.9655, val auc:0.8971360002313173\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2165, X recon loss:0.8979, val auc:0.9167257008197313\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  1.0 minutes, 25.796875 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=187979.00 bytes\n",
      "The time inference of base model is =2.069315600034315\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 22.245\n",
      "total memory usage of base model':16017 \n",
      "cpu usage of base model':1.600 %\n",
      "________*******************************_____________\n",
      "Regularized Model\n",
      "Directories are set.\n",
      " Is the approach is regularization:True\n",
      "Regrularization rate is:10000\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:607.7520, X recon loss:2.9259, val auc:0.6393977070653039\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  1.0 minutes, 1.1875 seconds\n",
      "****************Result of regularized model ******************\n",
      "10000 regularized model has accuracy on test set=0.64%\n",
      "10000 regularized model has size=187980.00 bytes\n",
      "The time inference of 10000 regularized model is =2.0907394000096247\n",
      "The number of parametrs of 10000 regularized model is:45888\n",
      "Energy Consumption of 10000 regularized model: 26.552\n",
      "total memory usage of 10000 regularized model':16017 \n",
      "cpu usage of 10000 regularized model':20.600 %\n",
      "________________________________________________\n",
      "************************************************\n",
      "This is iteration :6\n",
      "Training and evaluation before regularization \n",
      "Starting training...\n",
      "Directories are set.\n",
      " Is the approach is regularization:False\n",
      "Regrularization rate is:0\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[39] training loss:2.3815, X recon loss:0.9655, val auc:0.8971360002313173\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Epoch:[79] training loss:2.2165, X recon loss:0.8979, val auc:0.9167257008197313\n",
      "Done with saving models.\n",
      "Done with saving models.\n",
      "Done with training...\n",
      "Training time:  1.0 minutes, 24.765625 seconds\n",
      "*****Results of base model*********\n",
      "base model has accuracy on test set=0.94%\n",
      "base model has size=187979.00 bytes\n",
      "The time inference of base model is =2.092202800035011\n",
      "The number of parametrs of base model is:45888\n",
      "Energy Consumption : 21.340\n",
      "total memory usage of base model':16135 \n",
      "cpu usage of base model':0.000 %\n",
      "________*******************************_____________\n",
      "Regularized Model\n",
      "Directories are set.\n",
      " Is the approach is regularization:True\n",
      "Regrularization rate is:10000\n",
      "Directories are set.\n",
      "Building the models for training and evaluation in NESS framework...\n",
      "----------------------------------------Summary of the models:----------------------------------------\n",
      "================================== NESS Architecture ==================================\n",
      "GAEWrapper(\n",
      "  (gae): GAE(\n",
      "    (encoder): GNAEEncoder(\n",
      "      (linear1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (propagate): APPNP(K=1, alpha=0)\n",
      "    )\n",
      "    (decoder): InnerProductDecoder()\n",
      "  )\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds_loader = GraphLoader(config, dataset_name=config[\"dataset\"])\n",
    "train_data = ds_loader.train_data\n",
    "validation_data = ds_loader.validation_data\n",
    "test_data = ds_loader.test_data\n",
    "\n",
    "\n",
    "\n",
    "for i in range(num_iterations):\n",
    "        print('________________________________________________')\n",
    "        print('************************************************')\n",
    "        print(f\"This is iteration :{i+1}\")\n",
    "        \n",
    "        print(f'Training and evaluation before regularization ')\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "\n",
    "        Eva=dict() \n",
    "        config[\"is_reg\"]=False\n",
    "        config[\"l2_reg\"]=0\n",
    "        _, _, _,_, model = main(config)\n",
    "\n",
    "        best_checkpoint = dict()\n",
    "        best_checkpoint['state_dict'] = copy.deepcopy(model.autoencoder.state_dict())\n",
    "        model.autoencoder.load_state_dict(best_checkpoint['state_dict'])\n",
    "        recover_model = lambda: model.autoencoder.load_state_dict(best_checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "\n",
    "        base_model_path = os.path.join(results_path, \"training\", \"model\", \"autoencoder.pt\")\n",
    "\n",
    "\n",
    "        # Start monitoring CPU and memory usage, model size, number of parametes, time inference and  power consumption\n",
    "        gc.collect()\n",
    "        time.sleep(5)  # Add a 5-second delay to stabilize the initial state\n",
    "        tracemalloc.start()  # Start tracking memory allocations\n",
    "        snapshot_before = tracemalloc.take_snapshot()#take a snapshot of the current memory state before starting the measurement.\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        initial_cpu_usage = get_cpu_usage()\n",
    "        power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "\n",
    "        base_model_accuracy, test_ap=model.autoencoder.single_test(train_data, test_data)\n",
    "\n",
    "\n",
    "        base_cpu_usage = get_cpu_usage()\n",
    "        t1 = time.perf_counter()\n",
    "        t_base_model=t1-t0\n",
    "\n",
    "        snapshot_after = tracemalloc.take_snapshot()\n",
    "        tracemalloc.stop()\n",
    "        top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "        base_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "\n",
    "        base_energy_consumption = power_usage * t_base_model\n",
    "        # model size\n",
    "        base_model_size = os.path.getsize(base_model_path)\n",
    "        # number of parameters\n",
    "        num_parm_base_model=get_num_parameters(model, count_nonzero_only=True)\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5) \n",
    "\n",
    "        print(f'*****Results of base model*********')\n",
    "\n",
    "        print(f\"base model has accuracy on test set={base_model_accuracy:.2f}%\")\n",
    "        print(f\"base model has size={base_model_size:.2f} bytes\")\n",
    "        print(f\"The time inference of base model is ={t_base_model}\") \n",
    "        print(f\"The number of parametrs of base model is:{num_parm_base_model}\") \n",
    "\n",
    "        print(f\"Energy Consumption : {base_energy_consumption:.3f}\")\n",
    "        print(f\"total memory usage of base model':{base_total_memory_diff} \")\n",
    "        print(f\"cpu usage of base model':{base_cpu_usage:.3f} %\")\n",
    "\n",
    "\n",
    "        #Update Eva dictionary\n",
    "        Eva.update({'base model accuracy': base_model_accuracy,\n",
    "                    'time inference of base model': t_base_model,\n",
    "                    'number parmameters of base model': num_parm_base_model,\n",
    "                    'size of base model': base_model_size, \n",
    "                    'energy consumption of base model':base_energy_consumption,\n",
    "                    'total memory usage of base model':base_total_memory_diff,\n",
    "                    'cpu usage of base model':base_cpu_usage\n",
    "                   })\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5)  \n",
    "\n",
    "        #### Regularization of the Model\n",
    "        gc.collect()\n",
    "        time.sleep(5)   \n",
    "\n",
    "\n",
    "\n",
    "        print('________*******************************_____________')\n",
    "        print(f'Regularized Model')\n",
    "\n",
    "        config[\"is_reg\"]=True\n",
    "        config[\"l2_reg\"]= l2_lambda\n",
    "        _, _, _,_, model=main(config)\n",
    "\n",
    "        #### load the best regularized model\n",
    "        reg_model_path =  os.path.join(results_path, \"training\", \"model\",\"autoencoder\" + \"reg.pt\")\n",
    "        best_checkpoint['state_dict'] = copy.deepcopy(model.autoencoder.state_dict())\n",
    "        model.autoencoder.load_state_dict(best_checkpoint['state_dict'])\n",
    "        recover_model = lambda: model.autoencoder.load_state_dict(best_checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "\n",
    "        # Result of regularization\n",
    "\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5)  \n",
    "        tracemalloc.start() \n",
    "        snapshot_before = tracemalloc.take_snapshot()\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        initial_cpu_usage = get_cpu_usage()\n",
    "        power_usage = estimate_power_usage(initial_cpu_usage)\n",
    "\n",
    "        regularized_model_accuracy, regularized_test_ap=model.autoencoder.single_test(train_data, test_data)\n",
    "\n",
    "\n",
    "        regularized_cpu_usage = get_cpu_usage()\n",
    "        t1 = time.perf_counter()\n",
    "        t_regularized_model=t1-t0\n",
    "\n",
    "        snapshot_after = tracemalloc.take_snapshot()\n",
    "        tracemalloc.stop()\n",
    "        top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "        regularized_total_memory_diff = sum([stat.size_diff for stat in top_stats])\n",
    "        regularized_energy_consumption = power_usage * t_regularized_model\n",
    "        regularized_model_size = os.path.getsize( reg_model_path )\n",
    "        num_parm_regularized_model=get_num_parameters(model, count_nonzero_only=True)\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5)  # Add a 5-second delay to stabilize the initial state    \n",
    "\n",
    "\n",
    "\n",
    "        print('****************Result of regularized model ******************')\n",
    "\n",
    "\n",
    "        print(f\"{l2_lambda} regularized model has accuracy on test set={regularized_model_accuracy:.2f}%\")\n",
    "        print(f\"{l2_lambda} regularized model has size={regularized_model_size:.2f} bytes\")\n",
    "        print(f\"The time inference of {l2_lambda} regularized model is ={t_regularized_model}\") \n",
    "        print(f\"The number of parametrs of {l2_lambda} regularized model is:{num_parm_regularized_model}\") \n",
    "\n",
    "        print(f\"Energy Consumption of {l2_lambda} regularized model: {regularized_energy_consumption:.3f}\")\n",
    "        print(f\"total memory usage of {l2_lambda} regularized model':{regularized_total_memory_diff} \")\n",
    "        print(f\"cpu usage of {l2_lambda} regularized model':{regularized_cpu_usage:.3f} %\")\n",
    "\n",
    "\n",
    "        #Update Eva dictionary\n",
    "        Eva.update({'regularized model accuracy': regularized_model_accuracy,\n",
    "                    'time inference of regularized model': t_regularized_model,\n",
    "                    'number parmameters of regularized model': num_parm_regularized_model,\n",
    "                    'size of regularized model': regularized_model_size, \n",
    "                    'energy consumption of regularized model':regularized_energy_consumption,\n",
    "                    'total memory usage of regularized model':regularized_total_memory_diff,\n",
    "                    'cpu usage of regularized model':regularized_cpu_usage\n",
    "                   })\n",
    "\n",
    "        gc.collect()\n",
    "        time.sleep(5)   \n",
    "\n",
    "\n",
    "\n",
    "        Base_model_accuracy.append(Eva['base model accuracy'])\n",
    "        T_base_model.append(Eva['time inference of base model'])\n",
    "        Num_parm_base_model.append(int(Eva['number parmameters of base model']))\n",
    "        Base_model_size.append(int(Eva['size of base model']))\n",
    "        Base_Energy_Consumption.append(Eva['energy consumption of base model'])\n",
    "        Base_Cpu_Usage.append(Eva['cpu usage of base model'])\n",
    "        Base_Memory_Usage.append(Eva['total memory usage of base model'])\n",
    "\n",
    "        Reg_model_accuracy.append(Eva['regularized model accuracy'])\n",
    "        T_Reg_model.append(Eva['time inference of regularized model'])\n",
    "        Num_parm_Reg_model.append(int(Eva['number parmameters of regularized model']))\n",
    "        Reg_model_size.append(int(Eva['size of regularized model']))\n",
    "        Reg_Energy_Consumption.append(Eva['energy consumption of regularized model'])\n",
    "        Reg_Cpu_Usage.append(Eva['cpu usage of regularized model'])\n",
    "        Reg_Memory_Usage.append(Eva['total memory usage of regularized model'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d149e4e7",
   "metadata": {},
   "source": [
    "### Computing Mean and Std of Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cba9c12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model accuracy is:0.909  0.016\n",
      "Time inference of Base model :2.098  0.018\n",
      "Time number of parameters of Base model :45888.000  0.000\n",
      "The size of Base model :187684 bytes\n",
      "The energy consumption of Base model :69.834  30.260 \n",
      "The CPU usage of Base model :45.560  30.101 \n",
      "The memory usage of Base model :16643.000  1392.893 \n",
      "====================================================================================================\n",
      "Regularized model accuracy is:0.800  0.119\n",
      "Time inference of Regularized model :2.135  0.110\n",
      "Time number of parameters of Regularized model :45888.000  0.000\n",
      "The size of Regularized model :187688.600  3.098 bytes\n",
      "The energy consumption of Regularized model :72.549  40.796 \n",
      "The CPU usage of Regularized model :47.850  34.898 \n",
      "The memory usage of Regularized model :16166.300  68.767 \n",
      "====================================================================================================\n",
      "All measurement about regularization process of rate:10000 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Ave of base model accuracy': 0.909,\n",
       " 'Std of base model accuracy': 0.016,\n",
       " 'Ave of time inference of base model': 2.098,\n",
       " 'Std of time inference of base model': 0.018,\n",
       " 'Ave of number parmameters of base model': 45888,\n",
       " 'Std of number parmameters of base model': 0.0,\n",
       " 'Ave of base model size': 187686.8,\n",
       " 'Std of base model size': 3.6147844564602556,\n",
       " 'Ave of energy consumption of base model': 69.83412391841993,\n",
       " 'Std of energy consumption of base model': 30.26036674807619,\n",
       " 'Ave of cpu usage of base model': 45.56,\n",
       " 'Std of cpu usage of base model': 30.10069028370538,\n",
       " 'Ave of memory usage of base model': 16643,\n",
       " 'Std of memory usage of base model': 1392.892833079575,\n",
       " 'Ave of regularized model accuracy': 0.8,\n",
       " 'Std of regularized model accuracy': 0.119,\n",
       " 'Ave of time inference of regularized model': 2.135,\n",
       " 'Std of time inference of regularized model': 0.11,\n",
       " 'Ave of number parmameters of regularized model': 45888,\n",
       " 'Std of number parmameters of regularized model': 0.0,\n",
       " 'Ave of regularized model size': 187688.6,\n",
       " 'Std of regularized model size': 3.0983866769659336,\n",
       " 'Ave of energy consumption of regularized model': 72.54857461257721,\n",
       " 'Std of energy consumption of regularized model': 40.796403325305846,\n",
       " 'Ave of cpu usage of regularized model': 47.85,\n",
       " 'Std of cpu usage of regularized model': 34.89750866625168,\n",
       " 'Ave of memory usage of regularized model': 16166.3,\n",
       " 'Std of memory usage of regularized model': 68.76699789870138}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eva_final=dict()\n",
    "base_model_accuracy_mean = stat.mean(Base_model_accuracy)\n",
    "base_model_accuracy_std =  stat.stdev(Base_model_accuracy)\n",
    "Eva_final.update({'Ave of base model accuracy':float(format(base_model_accuracy_mean, '.3f'))})\n",
    "Eva_final.update({'Std of base model accuracy':float(format(base_model_accuracy_std, '.3f'))})\n",
    "base_model_accuracy = \"{:.3f}  {:.3f}\".format(base_model_accuracy_mean ,base_model_accuracy_std)\n",
    "print(f\"Base model accuracy is:{base_model_accuracy}\")\n",
    "\n",
    "                 \n",
    "t_base_model_mean =stat.mean(T_base_model)\n",
    "t_base_model_std =stat.stdev(T_base_model)  \n",
    "Eva_final.update({'Ave of time inference of base model':float(format(t_base_model_mean, '.3f'))})\n",
    "Eva_final.update({'Std of time inference of base model':float(format(t_base_model_std, '.3f'))})\n",
    "t_base_model = \"{:.3f}  {:.3f}\".format(t_base_model_mean ,t_base_model_std)\n",
    "print(f\"Time inference of Base model :{t_base_model}\")\n",
    "\n",
    "\n",
    "num_parm_base_model_mean = stat.mean(Num_parm_base_model)\n",
    "num_parm_base_model_std = stat.stdev(Num_parm_base_model)\n",
    "Eva_final.update({'Ave of number parmameters of base model':num_parm_base_model_mean})\n",
    "Eva_final.update({'Std of number parmameters of base model':num_parm_base_model_std})\n",
    "num_parm_base_model = \"{:.3f}  {:.3f}\".format(num_parm_base_model_mean ,num_parm_base_model_std)\n",
    "print(f\"Time number of parameters of Base model :{num_parm_base_model}\")\n",
    "\n",
    "base_model_size_mean = stat.mean(Base_model_size)\n",
    "base_model_size_std = stat.stdev(Base_model_size)\n",
    "Eva_final.update({'Ave of base model size':base_model_size_mean})\n",
    "Eva_final.update({'Std of base model size':base_model_size_std})\n",
    "base_model_size_model = \"{:.3f}  {:.3f}\".format(base_model_size_mean ,base_model_size_std)\n",
    "print(f\"The size of Base model :{base_model_size} bytes\")\n",
    "\n",
    "\n",
    "base_energy_consumption_mean = stat.mean(Base_Energy_Consumption)\n",
    "base_energy_consumption_std = stat.stdev(Base_Energy_Consumption)\n",
    "Eva_final.update({'Ave of energy consumption of base model':base_energy_consumption_mean })\n",
    "Eva_final.update({'Std of energy consumption of base model':base_energy_consumption_std})\n",
    "base_energy_consumption = \"{:.3f}  {:.3f}\".format(base_energy_consumption_mean ,base_energy_consumption_std)\n",
    "print(f\"The energy consumption of Base model :{base_energy_consumption} \")\n",
    "\n",
    "\n",
    "base_cpu_usage_mean = stat.mean(Base_Cpu_Usage)\n",
    "base_cpu_usage_std = stat.stdev(Base_Cpu_Usage)\n",
    "Eva_final.update({'Ave of cpu usage of base model':base_cpu_usage_mean})\n",
    "Eva_final.update({'Std of cpu usage of base model':base_cpu_usage_std})\n",
    "base_cpu_usage = \"{:.3f}  {:.3f}\".format(base_cpu_usage_mean ,base_cpu_usage_std)\n",
    "print(f\"The CPU usage of Base model :{base_cpu_usage} \")\n",
    "\n",
    "\n",
    "base_memory_usage_mean = stat.mean(Base_Memory_Usage)\n",
    "base_memory_usage_std = stat.stdev(Base_Memory_Usage)\n",
    "Eva_final.update({'Ave of memory usage of base model':base_memory_usage_mean})\n",
    "Eva_final.update({'Std of memory usage of base model':base_memory_usage_std})\n",
    "base_memory_usage = \"{:.3f}  {:.3f}\".format(base_memory_usage_mean ,base_memory_usage_std)\n",
    "print(f\"The memory usage of Base model :{base_memory_usage} \")\n",
    "\n",
    "print(100 * \"=\")\n",
    "####################################################\n",
    "\n",
    "reg_model_accuracy_mean =stat.mean(Reg_model_accuracy)\n",
    "reg_model_accuracy_std = stat.stdev(Reg_model_accuracy)\n",
    "Eva_final.update({'Ave of regularized model accuracy':float(format(reg_model_accuracy_mean, '.3f'))})\n",
    "Eva_final.update({'Std of regularized model accuracy':float(format(reg_model_accuracy_std, '.3f'))})\n",
    "reg_model_accuracy = \"{:.3f}  {:.3f}\".format(reg_model_accuracy_mean ,reg_model_accuracy_std)\n",
    "print(f\"Regularized model accuracy is:{reg_model_accuracy}\")\n",
    "                 \n",
    "\n",
    "t_reg_model_mean = stat.mean(T_Reg_model)\n",
    "t_reg_model_std =stat.stdev(T_Reg_model)\n",
    "Eva_final.update({'Ave of time inference of regularized model':float(format(t_reg_model_mean, '.3f'))})\n",
    "Eva_final.update({'Std of time inference of regularized model':float(format(t_reg_model_std, '.3f'))})\n",
    "t_reg_model = \"{:.3f}  {:.3f}\".format(t_reg_model_mean ,t_reg_model_std)\n",
    "print(f\"Time inference of Regularized model :{t_reg_model}\")\n",
    "\n",
    "num_parm_reg_model_mean = stat.mean(Num_parm_Reg_model)\n",
    "num_parm_reg_model_std = stat.stdev(Num_parm_Reg_model)\n",
    "Eva_final.update({'Ave of number parmameters of regularized model':num_parm_reg_model_mean})\n",
    "Eva_final.update({'Std of number parmameters of regularized model':num_parm_reg_model_std})\n",
    "num_parm_reg_model = \"{:.3f}  {:.3f}\".format(num_parm_reg_model_mean ,num_parm_reg_model_std)\n",
    "print(f\"Time number of parameters of Regularized model :{num_parm_reg_model}\")\n",
    "\n",
    "reg_model_size_mean =stat.mean( Reg_model_size)\n",
    "reg_model_size_std = stat.stdev(Reg_model_size)\n",
    "Eva_final.update({'Ave of regularized model size':reg_model_size_mean})\n",
    "Eva_final.update({'Std of regularized model size':reg_model_size_std })\n",
    "reg_model_size = \"{:.3f}  {:.3f}\".format(reg_model_size_mean ,reg_model_size_std)\n",
    "print(f\"The size of Regularized model :{reg_model_size} bytes\")\n",
    "\n",
    "reg_energy_consumption_mean = stat.mean(Reg_Energy_Consumption)\n",
    "reg_energy_consumption_std = stat.stdev(Reg_Energy_Consumption)\n",
    "Eva_final.update({'Ave of energy consumption of regularized model':reg_energy_consumption_mean })\n",
    "Eva_final.update({'Std of energy consumption of regularized model':reg_energy_consumption_std})\n",
    "reg_energy_consumption = \"{:.3f}  {:.3f}\".format(reg_energy_consumption_mean ,reg_energy_consumption_std)\n",
    "print(f\"The energy consumption of Regularized model :{reg_energy_consumption} \")\n",
    "\n",
    "\n",
    "reg_cpu_usage_mean = stat.mean(Reg_Cpu_Usage)\n",
    "reg_cpu_usage_std = stat.stdev(Reg_Cpu_Usage)\n",
    "Eva_final.update({'Ave of cpu usage of regularized model':reg_cpu_usage_mean})\n",
    "Eva_final.update({'Std of cpu usage of regularized model':reg_cpu_usage_std})\n",
    "reg_cpu_usage = \"{:.3f}  {:.3f}\".format(reg_cpu_usage_mean ,reg_cpu_usage_std)\n",
    "print(f\"The CPU usage of Regularized model :{reg_cpu_usage} \")\n",
    "\n",
    "\n",
    "reg_memory_usage_mean = stat.mean(Reg_Memory_Usage)\n",
    "reg_memory_usage_std = stat.stdev(Reg_Memory_Usage)\n",
    "#desc = \"{:.3f}  {:.3f}\".format(acc_mean,acc_std)\n",
    "Eva_final.update({'Ave of memory usage of regularized model':reg_memory_usage_mean})\n",
    "Eva_final.update({'Std of memory usage of regularized model':reg_memory_usage_std})\n",
    "reg_memory_usage = \"{:.3f}  {:.3f}\".format(reg_memory_usage_mean ,reg_memory_usage_std)\n",
    "print(f\"The memory usage of Regularized model :{reg_memory_usage} \")\n",
    "\n",
    "\n",
    "\n",
    "#################################\n",
    "\n",
    "print(100 * \"=\")\n",
    "print(f\"All measurement about regularization process of rate:{l2_lambda} \")   \n",
    "Eva_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af946953",
   "metadata": {},
   "source": [
    "### Recording results in txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d9a9d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_name = 'Cora-Link'\n",
    "Pruning_Method='Regularization'\n",
    "max_epoch = 100\n",
    "resume = True\n",
    "result_folder ='pathresult/'\n",
    "if not os.path.exists(result_folder):\n",
    "    os.makedirs(result_folder)\n",
    "\n",
    "\n",
    "\n",
    "file_name = result_folder+Pruning_Method+'_'+'with rate of regularization of'+'_'+str(l2_lambda)+'_on_'+dataset_name+'_'+str(max_epoch)+'.txt'\n",
    "with open(file_name, 'w') as f:\n",
    "        f.write('%s:%s\\n'%('dataset_name', 'Cora-Link'))\n",
    "        f.write('%s:%s\\n'%('max_epoch', max_epoch))\n",
    "        f.write('%s:%s\\n'%('sparsity', l2_lambda))\n",
    "        \n",
    "        for key, value in Eva_final.items():\n",
    "            f.write('%s:%s\\n'%(key, value))\n",
    "  \n",
    "        for key, value in Eva_measure.items():\n",
    "            f.write('%s:%s\\n' % (key, ','.join(map(str, value)))) \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6222d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb0eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc243d57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
